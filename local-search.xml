<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/p/4a17b156/"/>
    <url>/p/4a17b156/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br>hexo n -p <span class="hljs-built_in">dir</span>/title.md    会在<span class="hljs-built_in">source</span>/_posts下的<span class="hljs-built_in">dir</span>目录创建title.md<br><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>nn.Parameter</title>
    <link href="/p/a7eff039/"/>
    <url>/p/a7eff039/</url>
    
    <content type="html"><![CDATA[<h1 id="nn-Parameter"><a href="#nn-Parameter" class="headerlink" title="nn.Parameter"></a>nn.Parameter</h1><p><a href="https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html">Pytorch  torch.nn.Parameter</a><br>一种张量，可以认为是模块参数，神经网络参数。<br>参数是Tensor子类，当与Modules一起使用时具有非常特殊的属性-当它们被分配为Module属性时，它们会自动添加到其参数列表中，并且会出现在例如parameters（）迭代器中。而张量则没有这种效应。这是因为人们可能想在模型中缓存一些临时状态，比如RNN的最后一个隐藏状态。如果没有Parameter这样的类，这些临时变量也会被注册。</p><p><code>classtorch.nn.parameter.Parameter(data=None, requires_grad=True)</code><br>Parameters:</p><ul><li>data (Tensor) – 参数张量。</li><li>requires_grad (bool, optional) – 如果参数需要梯度。请注意，torch.no_grad() 上下文不会影响创建参数的默认行为–在 no_grad 模式下，参数的 requires_grad&#x3D;True 值仍为 True。更多详情，请参阅本地禁用梯度计算。默认值: true</li></ul><p>以上都是Pytorch的文档中的内容，其实说白了，torch.nn.Parameter就是一个张量，在神经网络的时候，它可以作为作为神经网络的训练的参数。虽然普通的tensor也可以通过设置<code>requires_grad=True</code>来得到梯度，但是它不能自动包括到model.Parameters()中，而nn.Parameter可以。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br>a=torch.tensor([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>],dtype=torch.float32)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;a&#x27;</span>, a)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;nn.Parameter(a):&#x27;</span>, nn.Parameter(a))<br><span class="hljs-comment"># a tensor([1., 2.])</span><br><span class="hljs-comment"># nn.Parameter(a): Parameter containing:</span><br><span class="hljs-comment"># tensor([1., 2.], requires_grad=True)</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>训练集、验证集和测试集的区别</title>
    <link href="/p/828b3633/"/>
    <url>/p/828b3633/</url>
    
    <content type="html"><![CDATA[<h3 id="写这篇文章的原因"><a href="#写这篇文章的原因" class="headerlink" title="写这篇文章的原因"></a>写这篇文章的原因</h3><p>之前搞机器学习很久了，也总能看到验证集这三个字，而我自己的数据集一直都是由训练集和测试集组成。然后我的数据集划分成这样两个部分之后，在每一个epoch的时候，我都会用训练集训练，测试集做测试，同时我也一直认为验证集和测试集只是名字不同，但感觉实际上是一个东西。</p><p>直到最近，我感觉自己写 for 循环写每个 epochs 的过程的时候我总觉得自己的代码经常需要在这里改许多东西，于是就想把代码能不能封装一下，不要浪费时间在这上面，于是我搜了搜，发现了pytorch-lighting。但是在写刚开始写的过程中，我就发现在lighting中，需要你自己定义好<code>training_step(self, batch, batch_idx)</code>、<code>validation_step(self, batch, batch_idx)</code>和<code>test_step(self, batch, batch_idx)</code>三个函数，然后在看这三个函数的时候才发现在每一个epoch会执行的是<code>training_step</code>和<code>validation_step</code>，然后在训练完成之后，才会执行<code>test_step</code>,这时候我才恍然大悟，原来自己之前所谓的测试集其实是验证集。自己之前一直把验证集当测试集用，然后没有真正的测试集。</p><h2 id="1-训练集"><a href="#1-训练集" class="headerlink" title="1.训练集"></a>1.训练集</h2><p>训练集的主要作用是用于拟合数据样本，用于训练模型，训练神经网络中的参数权重。<br>这是你提供给模型学习算法的主要数据。在训练过程中，模型将使用训练集中的数据进行学习和预测，并不断调整其内部参数以优化预测结果。</p><h2 id="2-验证集"><a href="#2-验证集" class="headerlink" title="2.验证集"></a>2.验证集</h2><p>验证集的主要作用是在训练过程中对模型进行一个初步的评估，用于调整模型的超参数。同时也不断的监控训练过程是否正常。<br>验证集的主要用途是防止过拟合，并且帮助我们找到最优的模型参数。在训练过程中，模型会根据验证集的表现来调整超参数，以优化模型的性能。</p><h2 id="3-测试集"><a href="#3-测试集" class="headerlink" title="3.测试集"></a>3.测试集</h2><p>测试集的主要作用是用于评估模型的性能，在训练过程中是不参与任何计算的，它是在模型训练完成之后，用来对模型进行一个全面的评估。<br>这是你用来评估模型在未见过的数据上的性能如何的数据集。在模型训练完成后，你可以使用测试集来测试模型的预测能力，并得到模型在未知数据上的性能评估结果。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>nn.init</title>
    <link href="/p/edda2ce1/"/>
    <url>/p/edda2ce1/</url>
    
    <content type="html"><![CDATA[<p>nn.init 模块是 PyTorch 中用于初始化神经网络模型参数的模块。通过 nn.init 模块，可以对模型的权重、偏置等参数进行初始化，以帮助模型更快地收敛并获得更好的性能。<br><code>torch.nn.init</code> 此模块中的所有函数都用于初始化神经网络参数，因此它们都在 <code>torch.no_grad()</code> 模式下运行，<code>autograd</code> 不会考虑它们。   </p><h2 id="torch-nn-init-calculate-gain-nonlinearity-param-None"><a href="#torch-nn-init-calculate-gain-nonlinearity-param-None" class="headerlink" title="torch.nn.init.calculate_gain(nonlinearity, param&#x3D;None)"></a>torch.nn.init.calculate_gain(nonlinearity, param&#x3D;None)</h2><p>为了实现自归一化神经网络，你应该使用nonlinear &#x3D;’linear’而不是nonlinearity &#x3D;’selu’。这使得初始权重的方差为1&#x2F;N，这对于在前向传递中引入稳定的固定点是必要的。相比之下，SELU的默认增益牺牲了归一化效果，以获得矩形层中更稳定的梯度流。</p><h4 id="Parameters"><a href="#Parameters" class="headerlink" title="Parameters:"></a>Parameters:</h4><ul><li><code>nonlinearity</code>(strzm)：[nn.functional] 要计算增益的非线性函数的名称。例如，<code>relu</code>、<code>tanh</code>、<code>sigmoid</code> 等。   </li><li><code>param</code>(float or None, optional)：非线性函数的参数。例如，对于 <code>LeakyReLU</code>，<code>param</code> 是负斜率。</li></ul><h4 id="Returns"><a href="#Returns" class="headerlink" title="Returns:"></a>Returns:</h4><p><code>gain</code>(float)：一个浮点数，表示给定非线性函数的增益。</p><h4 id="Examples"><a href="#Examples" class="headerlink" title="Examples:"></a>Examples:</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">gain = nn.init.calculate_gain(<span class="hljs-string">&#x27;leaky_relu&#x27;</span>, <span class="hljs-number">0.2</span>)   <span class="hljs-comment"># leaky_relu with negative_slope=0.2</span><br></code></pre></td></tr></table></figure><h2 id="torch-nn-init-uniform-tensor-a-0-0-b-1-0-generator-None"><a href="#torch-nn-init-uniform-tensor-a-0-0-b-1-0-generator-None" class="headerlink" title="torch.nn.init.uniform_(tensor, a&#x3D;0.0, b&#x3D;1.0, generator&#x3D;None)"></a>torch.nn.init.uniform_(tensor, a&#x3D;0.0, b&#x3D;1.0, generator&#x3D;None)</h2><p>用从均匀分布中提取的值填充输入张量。</p><h4 id="Parameters-1"><a href="#Parameters-1" class="headerlink" title="Parameters:"></a>Parameters:</h4><ul><li><code>tensor</code>(Tensor):一个 N 维张量torch.Tensor</li><li><code>a</code>(float):均匀分布的下限</li><li><code>b</code>(float):均匀分布的上界</li></ul><h4 id="Returns-1"><a href="#Returns-1" class="headerlink" title="Returns:"></a>Returns:</h4><ul><li><code>Tensor</code> :返回一个张量，同时参数也会自动更新</li></ul><h4 id="Examples-1"><a href="#Examples-1" class="headerlink" title="Examples:"></a>Examples:</h4><p>nn.init.uniform_(w)后自动修改w的值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><br>w = torch.empty(<span class="hljs-number">3</span>, <span class="hljs-number">5</span>)<br><span class="hljs-built_in">print</span>(w)<br>res = nn.init.uniform_(w)<br><span class="hljs-built_in">print</span>(w)<br><span class="hljs-built_in">print</span>(res)<br><span class="hljs-comment"># tensor([[1.0194e-38, 1.0469e-38, 1.0010e-38, 6.4286e-39, 9.9184e-39],</span><br><span class="hljs-comment">#         [8.4490e-39, 1.0102e-38, 9.0919e-39, 1.0102e-38, 8.9082e-39],</span><br><span class="hljs-comment">#         [8.4489e-39, 1.0102e-38, 1.0561e-38, 9.4592e-39, 1.0102e-38]])</span><br><span class="hljs-comment"># tensor([[0.2565, 0.3630, 0.4908, 0.5529, 0.4225],</span><br><span class="hljs-comment">#         [0.9944, 0.0088, 0.4067, 0.4673, 0.5027],</span><br><span class="hljs-comment">#         [0.0211, 0.9172, 0.5605, 0.1267, 0.0560]])</span><br><span class="hljs-comment"># tensor([[0.2565, 0.3630, 0.4908, 0.5529, 0.4225],</span><br><span class="hljs-comment">#         [0.9944, 0.0088, 0.4067, 0.4673, 0.5027],</span><br><span class="hljs-comment">#         [0.0211, 0.9172, 0.5605, 0.1267, 0.0560]])</span><br></code></pre></td></tr></table></figure><h2 id="torch-nn-init-normal-tensor-mean-0-0-std-1-0-generator-None"><a href="#torch-nn-init-normal-tensor-mean-0-0-std-1-0-generator-None" class="headerlink" title="torch.nn.init.normal_(tensor, mean&#x3D;0.0, std&#x3D;1.0, generator&#x3D;None)"></a>torch.nn.init.normal_(tensor, mean&#x3D;0.0, std&#x3D;1.0, generator&#x3D;None)</h2><p>使用从正态分布中提取的值填充输入张量。</p><h4 id="Parameters-2"><a href="#Parameters-2" class="headerlink" title="Parameters:"></a>Parameters:</h4><ul><li><code>tensor</code>(Tensor):一个 N 维张量torch.Tensor</li><li><code>mean</code>(float):正态分布的均值</li><li><code>std</code>(float):正态分布的标准差</li></ul><h4 id="Returns-2"><a href="#Returns-2" class="headerlink" title="Returns:"></a>Returns:</h4><ul><li><code>Tensor</code> :返回一个张量，同时参数也会自动更新</li></ul><h4 id="Examples-2"><a href="#Examples-2" class="headerlink" title="Examples:"></a>Examples:</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python">w = torch.empty(<span class="hljs-number">3</span>, <span class="hljs-number">5</span>)<br>nn.init.normal_(w)<br>```     <br><br><span class="hljs-comment">## torch.nn.init.constant_(tensor, val)</span><br>用标量值val填充输入张量。<br><br><span class="hljs-comment">#### Parameters:  </span><br>- `tensor`(Tensor): 一个 N 维张量torch.Tensor<br>- `val`(<span class="hljs-built_in">float</span>): 填充值<br><br><span class="hljs-comment">#### Returns:  </span><br>-  `Tensor` :返回一个张量，同时参数也会自动更新<br><br><span class="hljs-comment">#### Examples:</span><br>```python<br>w = torch.empty(<span class="hljs-number">3</span>, <span class="hljs-number">5</span>)<br>nn.init.constant_(w, <span class="hljs-number">0.5</span>)<br></code></pre></td></tr></table></figure><h2 id="torch-nn-init-ones-tensor"><a href="#torch-nn-init-ones-tensor" class="headerlink" title="torch.nn.init.ones_(tensor)"></a>torch.nn.init.ones_(tensor)</h2><p>用标量值1填充输入张量。</p><h4 id="Parameters-3"><a href="#Parameters-3" class="headerlink" title="Parameters:"></a>Parameters:</h4><ul><li><code>tensor</code>(Tensor):一个 N 维张量torch.Tensor</li></ul><h4 id="Returns-3"><a href="#Returns-3" class="headerlink" title="Returns:"></a>Returns:</h4><ul><li><code>Tensor</code> :返回一个张量，同时参数也会自动更新</li></ul><h4 id="Examples-3"><a href="#Examples-3" class="headerlink" title="Examples:"></a>Examples:</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">w = torch.empty(<span class="hljs-number">3</span>, <span class="hljs-number">5</span>)<br>nn.init.ones_(w)<br></code></pre></td></tr></table></figure><h2 id="torch-nn-init-zeros-tensor"><a href="#torch-nn-init-zeros-tensor" class="headerlink" title="torch.nn.init.zeros_(tensor)"></a>torch.nn.init.zeros_(tensor)</h2><p>用标量值0填充输入张量。</p><h4 id="Parameters-4"><a href="#Parameters-4" class="headerlink" title="Parameters:"></a>Parameters:</h4><ul><li><code>tensor</code>(Tensor):一个 N 维张量torch.Tensor</li></ul><h4 id="Returns-4"><a href="#Returns-4" class="headerlink" title="Returns:"></a>Returns:</h4><ul><li><code>Tensor</code> :返回一个张量，同时参数也会自动更新</li></ul><h4 id="Examples-4"><a href="#Examples-4" class="headerlink" title="Examples:"></a>Examples:</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">w = torch.empty(<span class="hljs-number">3</span>, <span class="hljs-number">5</span>)<br>nn.init.zeros_(w)<br></code></pre></td></tr></table></figure><h2 id="torch-nn-init-eye-tensor-out-None"><a href="#torch-nn-init-eye-tensor-out-None" class="headerlink" title="torch.nn.init.eye_(tensor, out&#x3D;None)"></a>torch.nn.init.eye_(tensor, out&#x3D;None)</h2><p>用单位矩阵填充二维输入张量。<br>保留线性层中输入的标识，其中保留尽可能多的输入。</p><h4 id="Parameters-5"><a href="#Parameters-5" class="headerlink" title="Parameters:"></a>Parameters:</h4><ul><li><code>tensor</code>(Tensor):一个二维张量torch.Tensor</li></ul><h4 id="Returns-5"><a href="#Returns-5" class="headerlink" title="Returns:"></a>Returns:</h4><ul><li><code>Tensor</code> :返回一个二维张量，同时参数也会自动更新</li></ul><h4 id="Examples-5"><a href="#Examples-5" class="headerlink" title="Examples:"></a>Examples:</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">w = torch.empty(<span class="hljs-number">3</span>, <span class="hljs-number">5</span>)<br>nn.init.eye_(w)<br></code></pre></td></tr></table></figure><h2 id="torch-nn-init-dirac-tensor-groups-1"><a href="#torch-nn-init-dirac-tensor-groups-1" class="headerlink" title="torch.nn.init.dirac_(tensor, groups&#x3D;1)"></a>torch.nn.init.dirac_(tensor, groups&#x3D;1)</h2><p>用狄拉克函数填充{3，4，5}维输入张量。<br>保留卷积层中输入的标识，其中保留尽可能多的输入通道。在组&gt;1的情况下，每个通道组保持同一性</p><h4 id="Parameters-6"><a href="#Parameters-6" class="headerlink" title="Parameters:"></a>Parameters:</h4><ul><li><code>tensor</code>(Tensor):一个 {3,4,5} 维张量torch.Tensor</li><li><code>groups</code>(int):每个输入通道的组数</li></ul><h4 id="Returns-6"><a href="#Returns-6" class="headerlink" title="Returns:"></a>Returns:</h4><ul><li><code>Tensor</code> :返回一个张量，同时参数也会自动更新</li></ul><h4 id="Examples-6"><a href="#Examples-6" class="headerlink" title="Examples:"></a>Examples:</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">w = torch.empty(<span class="hljs-number">3</span>, <span class="hljs-number">16</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>)<br>nn.init.dirac_(w)<br>w = torch.empty(<span class="hljs-number">3</span>, <span class="hljs-number">24</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>)<br>nn.init.dirac_(w, <span class="hljs-number">3</span>)<br></code></pre></td></tr></table></figure><h2 id="torch-nn-init-xavier-uniform-tensor-gain-1-0-generator-None"><a href="#torch-nn-init-xavier-uniform-tensor-gain-1-0-generator-None" class="headerlink" title="torch.nn.init.xavier_uniform_(tensor, gain&#x3D;1.0, generator&#x3D;None)"></a>torch.nn.init.xavier_uniform_(tensor, gain&#x3D;1.0, generator&#x3D;None)</h2><p>使用Xavier均匀分布填充输入张量。<br>根据Glorot, X.和Bengio, Y.在《Understanding the difficulty of training deep feedforward neural networks》中描述的方法，用一个均匀分布生成值，填充输入的张量或变量。结果张量中的值采样自 $U(−a, a)$，其中：$$a&#x3D;\text{gain}\times\sqrt{\frac6{\text{fan}_\text{in}+\text{fan}_\text{out}}}$$   </p><h4 id="Parameters-7"><a href="#Parameters-7" class="headerlink" title="Parameters:"></a>Parameters:</h4><ul><li><code>tensor</code>(Tensor):一个 N 维张量torch.Tensor</li><li><code>gain</code>(float, optional):可缩放因子</li></ul><h4 id="Returns-7"><a href="#Returns-7" class="headerlink" title="Returns:"></a>Returns:</h4><ul><li><code>Tensor</code> :返回一个张量，同时参数也会自动更新</li></ul><h4 id="Examples-7"><a href="#Examples-7" class="headerlink" title="Examples:"></a>Examples:</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">w = torch.empty(<span class="hljs-number">3</span>, <span class="hljs-number">5</span>)<br>nn.init.xavier_uniform_(w, gain=nn.init.calculate_gain(<span class="hljs-string">&#x27;relu&#x27;</span>))<br></code></pre></td></tr></table></figure><h2 id="torch-nn-init-xavier-normal-tensor-gain-1-0-generator-None"><a href="#torch-nn-init-xavier-normal-tensor-gain-1-0-generator-None" class="headerlink" title="torch.nn.init.xavier_normal_(tensor, gain&#x3D;1.0, generator&#x3D;None)"></a>torch.nn.init.xavier_normal_(tensor, gain&#x3D;1.0, generator&#x3D;None)</h2><p>使用Xavier正态分布填充输入张量。<br>根据Glorot, X.和Bengio, Y.在《Understanding the difficulty of training deep feedforward neural networks》中描述的方法，用一个正态分布生成值，填充输入的张量或变量。结果张量中的值采样自 $N(0,\text{std}^2)$ 的正态分布，其中标准差：$$\mathrm{std}&#x3D;\mathrm{gain}\times\sqrt{\frac2{\mathrm{fan}_\mathrm{in}+\mathrm{fan}_\mathrm{out}}}$$</p><h4 id="Parameters-8"><a href="#Parameters-8" class="headerlink" title="Parameters:"></a>Parameters:</h4><ul><li><code>tensor</code>(Tensor):一个 N 维张量torch.Tensor</li><li><code>gain</code>(float, optional):可缩放因子</li></ul><h4 id="Returns-8"><a href="#Returns-8" class="headerlink" title="Returns:"></a>Returns:</h4><ul><li><code>Tensor</code> :返回一个张量，同时参数也会自动更新</li></ul><h4 id="Examples-8"><a href="#Examples-8" class="headerlink" title="Examples:"></a>Examples:</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">w = torch.empty(<span class="hljs-number">3</span>, <span class="hljs-number">5</span>)<br>nn.init.xavier_normal_(w)<br></code></pre></td></tr></table></figure><h2 id="torch-nn-init-kaiming-uniform-tensor-a-0-mode-’fan-in’-nonlinearity-’leaky-relu’-generator-None"><a href="#torch-nn-init-kaiming-uniform-tensor-a-0-mode-’fan-in’-nonlinearity-’leaky-relu’-generator-None" class="headerlink" title="torch.nn.init.kaiming_uniform_(tensor, a&#x3D;0, mode&#x3D;’fan_in’, nonlinearity&#x3D;’leaky_relu’, generator&#x3D;None)"></a>torch.nn.init.kaiming_uniform_(tensor, a&#x3D;0, mode&#x3D;’fan_in’, nonlinearity&#x3D;’leaky_relu’, generator&#x3D;None)</h2><p>使用Kaiming均匀分布填充输入张量。<br>根据He, K等人于2015年在《Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification》中描述的方法，用一个均匀分布生成值，填充输入的张量或变量。结果张量中的值采样自 $U(−bound,bound)$，其中：$$\mathrm{bound}&#x3D;\mathrm{gain}\times\sqrt{\frac{3}{\mathrm{fan_mode}}}$$</p><h4 id="Parameters-9"><a href="#Parameters-9" class="headerlink" title="Parameters:"></a>Parameters:</h4><ul><li><code>tensor</code>(Tensor):一个 N 维张量torch.Tensor</li><li><code>a</code>(float, optional):用于计算负斜率的Leaky ReLU参数,（仅与’leaky_relu’一起使用）</li><li><code>mode</code>(str, optional): 可以为fan_in或fan_out。若为fan_in则保留前向传播时权值方差的量级，若为fan_out则保留反向传播时的量级，默认值为fan_in。</li><li><code>nonlinearity</code>(str, optional): 一个非线性函数，即一个nn.functional的名称，推荐使用relu或者leaky_relu，默认值为leaky_relu。</li></ul><h4 id="Returns-9"><a href="#Returns-9" class="headerlink" title="Returns:"></a>Returns:</h4><ul><li><code>Tensor</code> :返回一个张量，同时参数也会自动更新</li></ul><h4 id="Examples-9"><a href="#Examples-9" class="headerlink" title="Examples:"></a>Examples:</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">w = torch.empty(<span class="hljs-number">3</span>, <span class="hljs-number">5</span>)<br>nn.init.kaiming_uniform_(w, mode=<span class="hljs-string">&#x27;fan_in&#x27;</span>, nonlinearity=<span class="hljs-string">&#x27;relu&#x27;</span>)<br></code></pre></td></tr></table></figure><h2 id="torch-nn-init-kaiming-normal-tensor-a-0-mode-’fan-in’-nonlinearity-’leaky-relu’-generator-None"><a href="#torch-nn-init-kaiming-normal-tensor-a-0-mode-’fan-in’-nonlinearity-’leaky-relu’-generator-None" class="headerlink" title="torch.nn.init.kaiming_normal_(tensor, a&#x3D;0, mode&#x3D;’fan_in’, nonlinearity&#x3D;’leaky_relu’, generator&#x3D;None)"></a>torch.nn.init.kaiming_normal_(tensor, a&#x3D;0, mode&#x3D;’fan_in’, nonlinearity&#x3D;’leaky_relu’, generator&#x3D;None)</h2><p>使用Kaiming正态分布填充输入张量。<br>结果张量中的值采样自 $N(0,\text{std}^2)$，其中:$$\mathrm{std}&#x3D;\frac{\mathrm{gain}}{\sqrt{\mathrm{fan_mode}}}$$<br>也称为He初始化。</p><h4 id="Parameters-10"><a href="#Parameters-10" class="headerlink" title="Parameters:"></a>Parameters:</h4><ul><li><code>tensor</code>(Tensor):一个 N 维张量torch.Tensor</li><li><code>a</code>(float, optional):用于计算负斜率的Leaky ReLU参数,（仅与’leaky_relu’一起使用）</li><li><code>mode</code>(str, optional): 可以为fan_in或fan_out。若为fan_in则保留前向传播时权值方差的量级，若为fan_out则保留反向传播时的量级，默认值为fan_in。</li><li><code>nonlinearity</code>(str, optional): 一个非线性函数，即一个nn.functional的名称，推荐使用relu或者leaky_relu，默认值为leaky_relu。</li></ul><h4 id="Returns-10"><a href="#Returns-10" class="headerlink" title="Returns:"></a>Returns:</h4><ul><li><code>Tensor</code> :返回一个张量，同时参数也会自动更新</li></ul><h4 id="Examples-10"><a href="#Examples-10" class="headerlink" title="Examples:"></a>Examples:</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">w = torch.empty(<span class="hljs-number">3</span>, <span class="hljs-number">5</span>)<br>nn.init.kaiming_normal_(w, mode=<span class="hljs-string">&#x27;fan_out&#x27;</span>, nonlinearity=<span class="hljs-string">&#x27;relu&#x27;</span>)<br></code></pre></td></tr></table></figure><h2 id="torch-nn-init-trunc-normal-tensor-mean-0-0-std-1-0-a-2-0-b-2-0-generator-None"><a href="#torch-nn-init-trunc-normal-tensor-mean-0-0-std-1-0-a-2-0-b-2-0-generator-None" class="headerlink" title="torch.nn.init.trunc_normal_(tensor, mean&#x3D;0.0, std&#x3D;1.0, a&#x3D;-2.0, b&#x3D;2.0, generator&#x3D;None)"></a>torch.nn.init.trunc_normal_(tensor, mean&#x3D;0.0, std&#x3D;1.0, a&#x3D;-2.0, b&#x3D;2.0, generator&#x3D;None)</h2><p>使用从截断正态分布中提取的值填充输入张量。<br>该函数用截断正态分布中的值填充输入张量。这些值实际上是从正态分布 $N(\text{mean}, \text{std}^2)$ 中得出的，其中[a, b] 之外的值被重新绘制，直到它们在边界内。用于生成随机值的方法在 ${a ≤ mean ≤ b}$  情况下效果最佳。</p><h4 id="Parameters-11"><a href="#Parameters-11" class="headerlink" title="Parameters:"></a>Parameters:</h4><ul><li><code>tensor</code>(Tensor):一个 N 维张量torch.Tensor</li><li><code>mean</code>(float, optional): 正态分布的平均值</li><li><code>std</code>(float, optional): 正态分布的标准差</li><li><code>a</code>(float, optional): 最小截止值</li><li><code>b</code>(float, optional): 最大截止值</li></ul><h4 id="Returns-11"><a href="#Returns-11" class="headerlink" title="Returns:"></a>Returns:</h4><ul><li><code>Tensor</code> :返回一个张量，同时参数也会自动更新</li></ul><h4 id="Examples-11"><a href="#Examples-11" class="headerlink" title="Examples:"></a>Examples:</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">w = torch.empty(<span class="hljs-number">3</span>, <span class="hljs-number">5</span>)<br>nn.init.trunc_normal_(w)<br></code></pre></td></tr></table></figure><h2 id="torch-nn-init-orthogonal-tensor-gain-1-generator-None"><a href="#torch-nn-init-orthogonal-tensor-gain-1-generator-None" class="headerlink" title="torch.nn.init.orthogonal_(tensor, gain&#x3D;1, generator&#x3D;None)"></a>torch.nn.init.orthogonal_(tensor, gain&#x3D;1, generator&#x3D;None)</h2><p>用（半）正交矩阵填充输入张量。<br>输入张量必须至少有2个维度，对于超过2个维度的张量，尾部维度将被展平。</p><h4 id="Parameters-12"><a href="#Parameters-12" class="headerlink" title="Parameters:"></a>Parameters:</h4><ul><li><code>tensor</code>(Tensor):一个 N 维张量torch.Tensor, 其中 n&gt;&#x3D;2</li><li><code>gain</code>(float, optional): 用于缩放的因子</li></ul><h4 id="Returns-12"><a href="#Returns-12" class="headerlink" title="Returns:"></a>Returns:</h4><ul><li><code>Tensor</code> :返回一个张量，同时参数也会自动更新</li></ul><h4 id="Examples-12"><a href="#Examples-12" class="headerlink" title="Examples:"></a>Examples:</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">w = torch.empty(<span class="hljs-number">3</span>, <span class="hljs-number">5</span>)<br>nn.init.orthogonal_(w)<br></code></pre></td></tr></table></figure><h2 id="torch-nn-init-sparse-tensor-sparsity-std-0-01-generator-None"><a href="#torch-nn-init-sparse-tensor-sparsity-std-0-01-generator-None" class="headerlink" title="torch.nn.init.sparse_(tensor, sparsity, std&#x3D;0.01, generator&#x3D;None)"></a>torch.nn.init.sparse_(tensor, sparsity, std&#x3D;0.01, generator&#x3D;None)</h2><p>将2D输入张量填充为稀疏矩阵。<br>非零元素将从正态分布中提取 $N(0,0.01)$</p><h4 id="Parameters-13"><a href="#Parameters-13" class="headerlink" title="Parameters:"></a>Parameters:</h4><ul><li><code>tensor</code>(Tensor):一个 N 维张量torch.Tensor</li><li><code>sparsity</code>(float): 每列中元素被设置为零的比例</li><li><code>std</code>(float, optional): 用于生成非零值的正态分布的标准差</li></ul><h4 id="Returns-13"><a href="#Returns-13" class="headerlink" title="Returns:"></a>Returns:</h4><ul><li><code>Tensor</code> :返回一个张量，同时参数也会自动更新</li></ul><h4 id="Examples-13"><a href="#Examples-13" class="headerlink" title="Examples:"></a>Examples:</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">w = torch.empty(<span class="hljs-number">3</span>, <span class="hljs-number">5</span>)<br>nn.init.sparse_(w, sparsity=<span class="hljs-number">0.1</span>)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>BiasedMHA</title>
    <link href="/p/dd118e70/"/>
    <url>/p/dd118e70/</url>
    
    <content type="html"><![CDATA[<h2 id="class-dgl-nn-pytorch-gt-BiasedMHA-feat-size-num-heads-bias-True-attn-bias-type-add-attn-drop-0-1"><a href="#class-dgl-nn-pytorch-gt-BiasedMHA-feat-size-num-heads-bias-True-attn-bias-type-add-attn-drop-0-1" class="headerlink" title="class dgl.nn.pytorch.gt.BiasedMHA(feat_size, num_heads, bias=True, attn_bias_type=&#39;add&#39;, attn_drop=0.1)"></a><code>class dgl.nn.pytorch.gt.BiasedMHA(feat_size, num_heads, bias=True, attn_bias_type=&#39;add&#39;, attn_drop=0.1)</code></h2><p>具有图形注意偏差的密集多头注意模型。<br><a href="https://proceedings.neurips.cc/paper/2021/file/f1c1592588411002af340cbaedd6fc33-Paper.pdf">Do Transformers Really Perform Bad for Graph Representation</a>中介绍的使用从图结构中获得的注意力偏差计算节点之间的注意力。<br>$$\text{Attn}&#x3D;\text{softmax}(\dfrac{QK^T}{\sqrt{d}} \circ b)$$<br><code>Q</code>和<code>K</code>是节点的特征表示。<code>d</code>是对应的feat_size。<code>b</code>是注意力偏差，$\circ$ 运算符可以是加，也可以是乘。<br> ，可以是加法或乘法。</p><h3 id="Parameters"><a href="#Parameters" class="headerlink" title="Parameters"></a>Parameters</h3><ul><li>feat_size (int) - 特征尺寸。</li><li>num_heads (int) – 注意力头的数量，feat_size可被其整除。</li><li>bias (bool, optional) – 如果为True，则使用bias进行线性投影。默认值：True。</li><li>attn_bias_type (str, optional) – 注意力偏差的类型，用于修正注意力。从“add”或“mul”中选择。默认值：’add’。</li><li>attn_drop (float, optional) – 注意力权重的丢弃概率。Defalt：0.1。</li></ul><h3 id="forward-ndata-attn-bias-None-attn-mask-None"><a href="#forward-ndata-attn-bias-None-attn-mask-None" class="headerlink" title="forward(ndata, attn_bias=None, attn_mask=None)"></a><code>forward(ndata, attn_bias=None, attn_mask=None)</code></h3><p>Parameters<br>    - ndata (torch.Tensor) – 3D输入张量。Shape:(batch_size，N，feat_size)，其中N是节点的最大数量。<br>    - attn_bias (torch.Tensor, optional) – 用于注意力修改的注意力偏差。形状：(batch_size，N，N，num_heads)。<br>    -attn_mask (torch.Tensor, optional) – 用于避免计算无效位置的注意掩码，其中无效位置由True值指示。形状：（batch_size，N，N）。注意：对于与不存在的节点对应的行，请确保至少有一个条目设置为False，以防止使用softmax获取NaN。<br>Returns<br>    y – 输出张量。形状：（batch_size，N，feat_size）</p><p>Return type<br>    torch.Tensor</p><h2 id="源代码"><a href="#源代码" class="headerlink" title="源代码"></a>源代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> th<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">BiasedMHA</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        feat_size,</span><br><span class="hljs-params">        num_heads,</span><br><span class="hljs-params">        bias=<span class="hljs-literal">True</span>,</span><br><span class="hljs-params">        attn_bias_type=<span class="hljs-string">&quot;add&quot;</span>,</span><br><span class="hljs-params">        attn_drop=<span class="hljs-number">0.1</span>,</span><br><span class="hljs-params">    </span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.feat_size = feat_size<br>        <span class="hljs-variable language_">self</span>.num_heads = num_heads<br>        <span class="hljs-variable language_">self</span>.head_dim = feat_size // num_heads<br>        <span class="hljs-keyword">assert</span> (<br>            <span class="hljs-variable language_">self</span>.head_dim * num_heads == feat_size<br>        ), <span class="hljs-string">&quot;feat_size must be divisible by num_heads&quot;</span><br>        <span class="hljs-variable language_">self</span>.scaling = <span class="hljs-variable language_">self</span>.head_dim**-<span class="hljs-number">0.5</span><br>        <span class="hljs-variable language_">self</span>.attn_bias_type = attn_bias_type<br><br>        <span class="hljs-variable language_">self</span>.q_proj = nn.Linear(feat_size, feat_size, bias=bias)<br>        <span class="hljs-variable language_">self</span>.k_proj = nn.Linear(feat_size, feat_size, bias=bias)<br>        <span class="hljs-variable language_">self</span>.v_proj = nn.Linear(feat_size, feat_size, bias=bias)<br>        <span class="hljs-variable language_">self</span>.out_proj = nn.Linear(feat_size, feat_size, bias=bias)<br><br>        <span class="hljs-variable language_">self</span>.dropout = nn.Dropout(p=attn_drop)<br><br>        <span class="hljs-variable language_">self</span>.reset_parameters()<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">reset_parameters</span>(<span class="hljs-params">self</span>):<br>        nn.init.xavier_uniform_(<span class="hljs-variable language_">self</span>.q_proj.weight, gain=<span class="hljs-number">2</span>**-<span class="hljs-number">0.5</span>)<br>        nn.init.xavier_uniform_(<span class="hljs-variable language_">self</span>.k_proj.weight, gain=<span class="hljs-number">2</span>**-<span class="hljs-number">0.5</span>)<br>        nn.init.xavier_uniform_(<span class="hljs-variable language_">self</span>.v_proj.weight, gain=<span class="hljs-number">2</span>**-<span class="hljs-number">0.5</span>)<br><br>        nn.init.xavier_uniform_(<span class="hljs-variable language_">self</span>.out_proj.weight)<br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.out_proj.bias <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            nn.init.constant_(<span class="hljs-variable language_">self</span>.out_proj.bias, <span class="hljs-number">0.0</span>)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, ndata, attn_bias=<span class="hljs-literal">None</span>, attn_mask=<span class="hljs-literal">None</span></span>):<br>        q_h = <span class="hljs-variable language_">self</span>.q_proj(ndata).transpose(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>)<br>        k_h = <span class="hljs-variable language_">self</span>.k_proj(ndata).transpose(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>)<br>        v_h = <span class="hljs-variable language_">self</span>.v_proj(ndata).transpose(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>)<br>        bsz, N, _ = ndata.shape<br>        q_h = (<br>            q_h.reshape(N, bsz * <span class="hljs-variable language_">self</span>.num_heads, <span class="hljs-variable language_">self</span>.head_dim).transpose(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>)<br>            * <span class="hljs-variable language_">self</span>.scaling<br>        )<br>        k_h = k_h.reshape(N, bsz * <span class="hljs-variable language_">self</span>.num_heads, <span class="hljs-variable language_">self</span>.head_dim).permute(<br>            <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span><br>        )<br>        v_h = v_h.reshape(N, bsz * <span class="hljs-variable language_">self</span>.num_heads, <span class="hljs-variable language_">self</span>.head_dim).transpose(<br>            <span class="hljs-number">0</span>, <span class="hljs-number">1</span><br>        )<br><br>        attn_weights = (<br>            th.bmm(q_h, k_h)<br>            .transpose(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>)<br>            .reshape(N, N, bsz, <span class="hljs-variable language_">self</span>.num_heads)<br>            .transpose(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>)<br>        )<br><br>        <span class="hljs-keyword">if</span> attn_bias <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.attn_bias_type == <span class="hljs-string">&quot;add&quot;</span>:<br>                attn_weights += attn_bias<br>            <span class="hljs-keyword">else</span>:<br>                attn_weights *= attn_bias<br>        <span class="hljs-keyword">if</span> attn_mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            attn_weights[attn_mask.to(th.<span class="hljs-built_in">bool</span>)] = <span class="hljs-built_in">float</span>(<span class="hljs-string">&quot;-inf&quot;</span>)<br>        attn_weights = F.softmax(<br>            attn_weights.transpose(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>)<br>            .reshape(N, N, bsz * <span class="hljs-variable language_">self</span>.num_heads)<br>            .transpose(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>),<br>            dim=<span class="hljs-number">2</span>,<br>        )<br><br>        attn_weights = <span class="hljs-variable language_">self</span>.dropout(attn_weights)<br><br>        attn = th.bmm(attn_weights, v_h).transpose(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>)<br><br>        attn = <span class="hljs-variable language_">self</span>.out_proj(<br>            attn.reshape(N, bsz, <span class="hljs-variable language_">self</span>.feat_size).transpose(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>)<br>        )<br><br>        <span class="hljs-keyword">return</span> attn<br></code></pre></td></tr></table></figure><h2 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h2><p>Example1:   </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> th<br><span class="hljs-keyword">from</span> dgl.nn <span class="hljs-keyword">import</span> BiasedMHA<br>ndata = th.rand(<span class="hljs-number">16</span>, <span class="hljs-number">100</span>, <span class="hljs-number">512</span>)<br>bias = th.rand(<span class="hljs-number">16</span>, <span class="hljs-number">100</span>, <span class="hljs-number">100</span>, <span class="hljs-number">8</span>)<br>net = BiasedMHA(feat_size=<span class="hljs-number">512</span>, num_heads=<span class="hljs-number">8</span>)<br>out = net(ndata, bias)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>GraphormerLayer</title>
    <link href="/p/591bc275/"/>
    <url>/p/591bc275/</url>
    
    <content type="html"><![CDATA[<h2 id="class-dgl-nn-pytorch-gt-GraphormerLayer-feat-size-hidden-size-num-heads-attn-bias-type-add-norm-first-False-dropout-0-1-attn-dropout-0-1-activation-ReLU"><a href="#class-dgl-nn-pytorch-gt-GraphormerLayer-feat-size-hidden-size-num-heads-attn-bias-type-add-norm-first-False-dropout-0-1-attn-dropout-0-1-activation-ReLU" class="headerlink" title="class dgl.nn.pytorch.gt.GraphormerLayer(feat_size, hidden_size, num_heads, attn_bias_type=&#39;add&#39;, norm_first=False, dropout=0.1, attn_dropout=0.1, activation=ReLU())"></a><code>class dgl.nn.pytorch.gt.GraphormerLayer(feat_size, hidden_size, num_heads, attn_bias_type=&#39;add&#39;, norm_first=False, dropout=0.1, attn_dropout=0.1, activation=ReLU())</code></h2><p><a href="https://proceedings.neurips.cc/paper/2021/file/f1c1592588411002af340cbaedd6fc33-Paper.pdf">Do Transformers Really Perform Bad for Graph Representation</a>中介绍的Graphormer层</p><h3 id="Parameters"><a href="#Parameters" class="headerlink" title="Parameters"></a>Parameters</h3><ul><li>feat_size (int) – 特征的维度。</li><li>hidden_size (int) – 前馈层的隐藏维度。</li><li>num_heads (int) – 注意力头的数量。<code>feat_size</code>可被其整除。</li><li>attn_bias_type (str) – 注意力偏差的类型，用于修正注意力。可以是“add”或“mul”。默认“add”。</li><li>norm_first (bool, optional) – 如果为True，则在注意力和前馈操作之前执行层规范化。否则，它会在之后应用层规范化。默认值：False。</li><li>dropout (float, optional) – Dropout概率。默认0.1。</li><li>attn_dropout (float, optional) – 注意力Dropout概率。默认0.1。</li><li>activation (callable activation layer, optional) – 激活函数。默认<code>nn.ReLU()</code>。</li></ul><h3 id="forward-nfeat-attn-bias-None-attn-mask-None"><a href="#forward-nfeat-attn-bias-None-attn-mask-None" class="headerlink" title="forward(nfeat, attn_bias=None, attn_mask=None)"></a><code>forward(nfeat, attn_bias=None, attn_mask=None)</code></h3><p>Parameters<br>    - nfeat (torch.Tensor) – -3D输入张量。形状为：<code>(batch_size，N，feat_size)</code>，其中<code>N</code>是节点的最大数量。<br>    - attn_bias (torch.Tensor, optional) – 用于注意力修改的注意力偏差。形状：<code>(batch_size，N，N，num_heads)</code>。<br>    - attn_mask (torch.Tensor, optional) – 用于避免计算无效位置的注意掩码，其中无效位置由True值指示。形状：<code>(batch_size，N，N)</code>。注意：对于与不存在的节点对应的行，请确保至少有一个条目设置为False，以防止使用softmax获取NaN。<br>Returns<br>    y - 输出张量。形状：<code>(batch_size，N，feat_size)</code></p><p>Return type<br>    torch.Tensor</p><h2 id="源代码"><a href="#源代码" class="headerlink" title="源代码"></a>源代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">from</span> .biased_mha <span class="hljs-keyword">import</span> BiasedMHA<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">GraphormerLayer</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        feat_size,</span><br><span class="hljs-params">        hidden_size,</span><br><span class="hljs-params">        num_heads,</span><br><span class="hljs-params">        attn_bias_type=<span class="hljs-string">&quot;add&quot;</span>,</span><br><span class="hljs-params">        norm_first=<span class="hljs-literal">False</span>,</span><br><span class="hljs-params">        dropout=<span class="hljs-number">0.1</span>,</span><br><span class="hljs-params">        attn_dropout=<span class="hljs-number">0.1</span>,</span><br><span class="hljs-params">        activation=nn.ReLU(<span class="hljs-params"></span>),</span><br><span class="hljs-params">    </span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br><br>        <span class="hljs-variable language_">self</span>.norm_first = norm_first<br><br>        <span class="hljs-variable language_">self</span>.attn = BiasedMHA(<br>            feat_size=feat_size,<br>            num_heads=num_heads,<br>            attn_bias_type=attn_bias_type,<br>            attn_drop=attn_dropout,<br>        )<br>        <span class="hljs-variable language_">self</span>.ffn = nn.Sequential(<br>            nn.Linear(feat_size, hidden_size),<br>            activation,<br>            nn.Dropout(p=dropout),<br>            nn.Linear(hidden_size, feat_size),<br>            nn.Dropout(p=dropout),<br>        )<br><br>        <span class="hljs-variable language_">self</span>.dropout = nn.Dropout(p=dropout)<br>        <span class="hljs-variable language_">self</span>.attn_layer_norm = nn.LayerNorm(feat_size)<br>        <span class="hljs-variable language_">self</span>.ffn_layer_norm = nn.LayerNorm(feat_size)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, nfeat, attn_bias=<span class="hljs-literal">None</span>, attn_mask=<span class="hljs-literal">None</span></span>):<br>        residual = nfeat<br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.norm_first:<br>            nfeat = <span class="hljs-variable language_">self</span>.attn_layer_norm(nfeat)<br>        nfeat = <span class="hljs-variable language_">self</span>.attn(nfeat, attn_bias, attn_mask)<br>        nfeat = <span class="hljs-variable language_">self</span>.dropout(nfeat)<br>        nfeat = residual + nfeat<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-variable language_">self</span>.norm_first:<br>            nfeat = <span class="hljs-variable language_">self</span>.attn_layer_norm(nfeat)<br>        residual = nfeat<br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.norm_first:<br>            nfeat = <span class="hljs-variable language_">self</span>.ffn_layer_norm(nfeat)<br>        nfeat = <span class="hljs-variable language_">self</span>.ffn(nfeat)<br>        nfeat = residual + nfeat<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-variable language_">self</span>.norm_first:<br>            nfeat = <span class="hljs-variable language_">self</span>.ffn_layer_norm(nfeat)<br>        <span class="hljs-keyword">return</span> nfeat<br></code></pre></td></tr></table></figure><h2 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h2><p>Example1:   </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> th<br><span class="hljs-keyword">from</span> dgl.nn <span class="hljs-keyword">import</span> GraphormerLayer<br><br>batch_size = <span class="hljs-number">16</span><br>num_nodes = <span class="hljs-number">100</span><br>feat_size = <span class="hljs-number">512</span><br>num_heads = <span class="hljs-number">8</span><br>nfeat = th.rand(batch_size, num_nodes, feat_size)<br>bias = th.rand(batch_size, num_nodes, num_nodes, num_heads)<br>net = GraphormerLayer(feat_size=feat_size, hidden_size=<span class="hljs-number">2048</span>, num_heads=num_heads)<br>out = net(nfeat, bias)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>LapPosEncoder</title>
    <link href="/p/dc2703a/"/>
    <url>/p/dc2703a/</url>
    
    <content type="html"><![CDATA[<h2 id="class-dgl-nn-pytorch-gt-LapPosEncoder-model-type-num-layer-k-dim-n-head-1-batch-norm-False-num-post-layer-0"><a href="#class-dgl-nn-pytorch-gt-LapPosEncoder-model-type-num-layer-k-dim-n-head-1-batch-norm-False-num-post-layer-0" class="headerlink" title="class dgl.nn.pytorch.gt.LapPosEncoder(model_type, num_layer, k, dim, n_head=1, batch_norm=False, num_post_layer=0)"></a><code>class dgl.nn.pytorch.gt.LapPosEncoder(model_type, num_layer, k, dim, n_head=1, batch_norm=False, num_post_layer=0)</code></h2><p><a href="https://proceedings.neurips.cc/paper/2021/file/f1c1592588411002af340cbaedd6fc33-Paper.pdf">Do Transformers Really Perform Bad for Graph Representation</a>中介绍的Laplacian Positional Encoder（LPE）<br>该模块是使用Transformer或DeepSet的学习拉普拉斯位置编码模块。</p><h3 id="Parameters"><a href="#Parameters" class="headerlink" title="Parameters"></a>Parameters</h3><ul><li>model_type(str) - LPE的编码器模型类型，只能是“Transformer”或“DeepSet”。</li><li>num_layer(int) - Transformer&#x2F;DeepSet编码器中的层数。</li><li>k(int) - 最小非平凡特征向量的个数。</li><li>dim(int) - 最终拉普拉斯编码的输出大小。</li><li>n_head(int,optional) - Transformer编码器中的头数。预设值：1。</li><li>batch_norm (bool, optional) - 如果为True，则对原始拉普拉斯位置编码应用批量归一化。默认值：False。</li><li>num_post_layer (int, optional) - 如果num_post_layer &gt; 0，则在池化之后应用num_post_layer层的MLP。默认值：0。</li></ul><h3 id="forward-eigvals-eigvecs"><a href="#forward-eigvals-eigvecs" class="headerlink" title="forward(eigvals, eigvecs)"></a><code>forward(eigvals, eigvecs)</code></h3><p>Parameters<br>    - eigvals（Tensor）-形状为<code>(N,k)</code>，k 不同的拉普拉斯特征值 特征值重复N次，可以通过使用LaplacianPE获得。<br>    - eigvecs (Tensor) – 形状为<code>(N,k)</code>的拉普拉斯特征向量，可以通过以下方式获得： 使用LaplacianPE。</p><p>Returns<br>    返回形状<code>(N,d)</code>的拉普拉斯位置编码， 其中<code>N</code>是输入图中的节点数，<code>d</code>是<code>dim</code>。</p><p>Return type<br>    torch.Tensor</p><h2 id="源代码"><a href="#源代码" class="headerlink" title="源代码"></a>源代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">LapPosEncoder</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        model_type,</span><br><span class="hljs-params">        num_layer,</span><br><span class="hljs-params">        k,</span><br><span class="hljs-params">        dim,</span><br><span class="hljs-params">        n_head=<span class="hljs-number">1</span>,</span><br><span class="hljs-params">        batch_norm=<span class="hljs-literal">False</span>,</span><br><span class="hljs-params">        num_post_layer=<span class="hljs-number">0</span>,</span><br><span class="hljs-params">    </span>):<br>        <span class="hljs-built_in">super</span>(LapPosEncoder, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.model_type = model_type<br>        <span class="hljs-variable language_">self</span>.linear = nn.Linear(<span class="hljs-number">2</span>, dim)<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.model_type == <span class="hljs-string">&quot;Transformer&quot;</span>:<br>            encoder_layer = nn.TransformerEncoderLayer(<br>                d_model=dim, nhead=n_head, batch_first=<span class="hljs-literal">True</span><br>            )<br>            <span class="hljs-variable language_">self</span>.pe_encoder = nn.TransformerEncoder(<br>                encoder_layer, num_layers=num_layer<br>            )<br>        <span class="hljs-keyword">elif</span> <span class="hljs-variable language_">self</span>.model_type == <span class="hljs-string">&quot;DeepSet&quot;</span>:<br>            layers = []<br>            <span class="hljs-keyword">if</span> num_layer == <span class="hljs-number">1</span>:<br>                layers.append(nn.ReLU())<br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-variable language_">self</span>.linear = nn.Linear(<span class="hljs-number">2</span>, <span class="hljs-number">2</span> * dim)<br>                layers.append(nn.ReLU())<br>                <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_layer - <span class="hljs-number">2</span>):<br>                    layers.append(nn.Linear(<span class="hljs-number">2</span> * dim, <span class="hljs-number">2</span> * dim))<br>                    layers.append(nn.ReLU())<br>                layers.append(nn.Linear(<span class="hljs-number">2</span> * dim, dim))<br>                layers.append(nn.ReLU())<br>            <span class="hljs-variable language_">self</span>.pe_encoder = nn.Sequential(*layers)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">raise</span> ValueError(<br>                <span class="hljs-string">f&quot;model_type &#x27;<span class="hljs-subst">&#123;model_type&#125;</span>&#x27; is not allowed, must be &quot;</span><br>                <span class="hljs-string">&quot;&#x27;Transformer&#x27; or &#x27;DeepSet&#x27;.&quot;</span><br>            )<br><br>        <span class="hljs-keyword">if</span> batch_norm:<br>            <span class="hljs-variable language_">self</span>.raw_norm = nn.BatchNorm1d(k)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-variable language_">self</span>.raw_norm = <span class="hljs-literal">None</span><br><br>        <span class="hljs-keyword">if</span> num_post_layer &gt; <span class="hljs-number">0</span>:<br>            layers = []<br>            <span class="hljs-keyword">if</span> num_post_layer == <span class="hljs-number">1</span>:<br>                layers.append(nn.Linear(dim, dim))<br>                layers.append(nn.ReLU())<br>            <span class="hljs-keyword">else</span>:<br>                layers.append(nn.Linear(dim, <span class="hljs-number">2</span> * dim))<br>                layers.append(nn.ReLU())<br>                <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_post_layer - <span class="hljs-number">2</span>):<br>                    layers.append(nn.Linear(<span class="hljs-number">2</span> * dim, <span class="hljs-number">2</span> * dim))<br>                    layers.append(nn.ReLU())<br>                layers.append(nn.Linear(<span class="hljs-number">2</span> * dim, dim))<br>                layers.append(nn.ReLU())<br>            <span class="hljs-variable language_">self</span>.post_mlp = nn.Sequential(*layers)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-variable language_">self</span>.post_mlp = <span class="hljs-literal">None</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, eigvals, eigvecs</span>):<br>        pos_encoding = th.cat((eigvecs.unsqueeze(<span class="hljs-number">2</span>), eigvals.unsqueeze(<span class="hljs-number">2</span>)), dim=<span class="hljs-number">2</span>).<span class="hljs-built_in">float</span>()<br>        empty_mask = th.isnan(pos_encoding)<br><br>        pos_encoding[empty_mask] = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.raw_norm:<br>            pos_encoding = <span class="hljs-variable language_">self</span>.raw_norm(pos_encoding)<br>        pos_encoding = <span class="hljs-variable language_">self</span>.linear(pos_encoding)<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.model_type == <span class="hljs-string">&quot;Transformer&quot;</span>:<br>            pos_encoding = <span class="hljs-variable language_">self</span>.pe_encoder(src=pos_encoding, src_key_padding_mask=empty_mask[:, :, <span class="hljs-number">1</span>])<br>        <span class="hljs-keyword">else</span>:<br>            pos_encoding = <span class="hljs-variable language_">self</span>.pe_encoder(pos_encoding)<br><br>        <span class="hljs-comment"># Remove masked sequences.</span><br>        pos_encoding[empty_mask[:, :, <span class="hljs-number">1</span>]] = <span class="hljs-number">0</span><br><br>        <span class="hljs-comment"># Sum pooling.</span><br>        pos_encoding = th.<span class="hljs-built_in">sum</span>(pos_encoding, <span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">False</span>)<br><br>        <span class="hljs-comment"># MLP post pooling.</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.post_mlp:<br>            pos_encoding = <span class="hljs-variable language_">self</span>.post_mlp(pos_encoding)<br><br>        <span class="hljs-keyword">return</span> pos_encoding<br></code></pre></td></tr></table></figure><h2 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h2><p>Example1:   </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> dgl<br><span class="hljs-keyword">from</span> dgl <span class="hljs-keyword">import</span> LapPE<br><span class="hljs-keyword">from</span> dgl.nn <span class="hljs-keyword">import</span> LapPosEncoder<br>transform = LapPE(k=<span class="hljs-number">5</span>, feat_name=<span class="hljs-string">&#x27;eigvec&#x27;</span>, eigval_name=<span class="hljs-string">&#x27;eigval&#x27;</span>, padding=<span class="hljs-literal">True</span>)<br>g = dgl.graph(([<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">4</span>,<span class="hljs-number">0</span>], [<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">4</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>]))<br>g = transform(g)<br>eigvals, eigvecs = g.ndata[<span class="hljs-string">&#x27;eigval&#x27;</span>], g.ndata[<span class="hljs-string">&#x27;eigvec&#x27;</span>]<br>transformer_encoder = LapPosEncoder(model_type=<span class="hljs-string">&quot;Transformer&quot;</span>, num_layer=<span class="hljs-number">3</span>, k=<span class="hljs-number">5</span>, dim=<span class="hljs-number">16</span>, n_head=<span class="hljs-number">4</span>)<br>pos_encoding = transformer_encoder(eigvals, eigvecs)<br>deepset_encoder = LapPosEncoder(model_type=<span class="hljs-string">&quot;DeepSet&quot;</span>, num_layer=<span class="hljs-number">3</span>, k=<span class="hljs-number">5</span>, dim=<span class="hljs-number">16</span>, num_post_layer=<span class="hljs-number">2</span>)<br>pos_encoding = deepset_encoder(eigvals, eigvecs)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>DegreeEncoder</title>
    <link href="/p/b75828bf/"/>
    <url>/p/b75828bf/</url>
    
    <content type="html"><![CDATA[<h2 id="class-dgl-nn-pytorch-gt-DegreeEncoder-max-degree-embedding-dim-direction-both"><a href="#class-dgl-nn-pytorch-gt-DegreeEncoder-max-degree-embedding-dim-direction-both" class="headerlink" title="class dgl.nn.pytorch.gt.DegreeEncoder(max_degree, embedding_dim, direction=&#39;both&#39;)"></a><code>class dgl.nn.pytorch.gt.DegreeEncoder(max_degree, embedding_dim, direction=&#39;both&#39;)</code></h2><p><a href="https://proceedings.neurips.cc/paper/2021/file/f1c1592588411002af340cbaedd6fc33-Paper.pdf">Do Transformers Really Perform Bad for Graph Representation</a>中介绍的度编码器（Degree Encoder）<br>这个模块是一个可学习的度嵌入模块。</p><h3 id="Parameters"><a href="#Parameters" class="headerlink" title="Parameters"></a>Parameters</h3><ul><li>max_degree(int) - 要编码的度数的上限。每个度数都将被限制在范围[0，max_degree]内。</li><li>embedding_dim(int) - 嵌入向量的输出维数。</li><li>direction(str, optional) - 要编码的度数方向。选项包括<code>in</code>，<code>out</code>和<code>both</code>。<code>both</code>都对两个方向的度进行编码并输出它们的相加。默认值：<code>both</code>。</li></ul><h3 id="forward（degrees）"><a href="#forward（degrees）" class="headerlink" title="forward（degrees）"></a><code>forward（degrees）</code></h3><p>Parameters<br>    degrees(Tensor) - -如果<code>direction</code>是<code>both</code>，则应该以具有零填充的批处理图的入度和出度进行堆叠，形状为<code>(2,B,N)</code>的张量。 否则，它应该在批处理的度数或度数外填充零。图，一个形状为<code>(B,N)</code>的张量，其中<code>B</code>是批量大小，<code>N</code>是最大节点数。</p><p>Returns<br>    返回形状为<code>(B,N,d)</code>的度嵌入向量， 其中<code>d</code>是<code>embedding_dim</code>。</p><p>Return type<br>    torch.Tensor</p><h2 id="源代码"><a href="#源代码" class="headerlink" title="源代码"></a>源代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">DegreeEncoder</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, max_degree, embedding_dim, direction=<span class="hljs-string">&quot;both&quot;</span></span>):<br>        <span class="hljs-built_in">super</span>(DegreeEncoder, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.direction = direction<br>        <span class="hljs-keyword">if</span> direction == <span class="hljs-string">&quot;both&quot;</span>:<br>            <span class="hljs-variable language_">self</span>.encoder1 = nn.Embedding(max_degree + <span class="hljs-number">1</span>, embedding_dim, padding_idx=<span class="hljs-number">0</span>)<br>            <span class="hljs-variable language_">self</span>.encoder2 = nn.Embedding(max_degree + <span class="hljs-number">1</span>, embedding_dim, padding_idx=<span class="hljs-number">0</span>)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-variable language_">self</span>.encoder = nn.Embedding(max_degree + <span class="hljs-number">1</span>, embedding_dim, padding_idx=<span class="hljs-number">0</span>)<br>        <span class="hljs-variable language_">self</span>.max_degree = max_degree<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, degrees</span>):<br>        degrees = th.clamp(degrees, <span class="hljs-built_in">min</span>=<span class="hljs-number">0</span>, <span class="hljs-built_in">max</span>=<span class="hljs-variable language_">self</span>.max_degree)<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.direction == <span class="hljs-string">&quot;in&quot;</span>:<br>            <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(degrees.shape) == <span class="hljs-number">2</span><br>            degree_embedding = <span class="hljs-variable language_">self</span>.encoder(degrees)<br>        <span class="hljs-keyword">elif</span> <span class="hljs-variable language_">self</span>.direction == <span class="hljs-string">&quot;out&quot;</span>:<br>            <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(degrees.shape) == <span class="hljs-number">2</span><br>            degree_embedding = <span class="hljs-variable language_">self</span>.encoder(degrees)<br>        <span class="hljs-keyword">elif</span> <span class="hljs-variable language_">self</span>.direction == <span class="hljs-string">&quot;both&quot;</span>:<br>            <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(degrees.shape) == <span class="hljs-number">3</span> <span class="hljs-keyword">and</span> degrees.shape[<span class="hljs-number">0</span>] == <span class="hljs-number">2</span><br>            degree_embedding = <span class="hljs-variable language_">self</span>.encoder1(degrees[<span class="hljs-number">0</span>]) + <span class="hljs-variable language_">self</span>.encoder2(degrees[<span class="hljs-number">1</span>])<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">raise</span> ValueError(<br>                <span class="hljs-string">f&#x27;Supported direction options: &quot;in&quot;, &quot;out&quot; and &quot;both&quot;, &#x27;</span><br>                <span class="hljs-string">f&quot;but got <span class="hljs-subst">&#123;self.direction&#125;</span>&quot;</span><br>            )<br>        <span class="hljs-keyword">return</span> degree_embedding<br></code></pre></td></tr></table></figure><h2 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h2><p>Example1:   </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> dgl<br><span class="hljs-keyword">from</span> dgl.nn <span class="hljs-keyword">import</span> DegreeEncoder<br><span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> th<br><span class="hljs-keyword">from</span> torch.nn.utils.rnn <span class="hljs-keyword">import</span> pad_sequence<br><br>g1 = dgl.graph(([<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">3</span>], [<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">0</span>,<span class="hljs-number">3</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]))<br>g2 = dgl.graph(([<span class="hljs-number">0</span>,<span class="hljs-number">1</span>], [<span class="hljs-number">1</span>,<span class="hljs-number">0</span>]))<br>in_degree = pad_sequence([g1.in_degrees(), g2.in_degrees()], batch_first=<span class="hljs-literal">True</span>)<br>out_degree = pad_sequence([g1.out_degrees(), g2.out_degrees()], batch_first=<span class="hljs-literal">True</span>)<br><span class="hljs-built_in">print</span>(in_degree.shape)<br>degree_encoder = DegreeEncoder(<span class="hljs-number">5</span>, <span class="hljs-number">16</span>)<br>degree_embedding = degree_encoder(th.stack((in_degree, out_degree)))<br><span class="hljs-built_in">print</span>(degree_embedding.shape)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>PathEncoder</title>
    <link href="/p/f3181863/"/>
    <url>/p/f3181863/</url>
    
    <content type="html"><![CDATA[<h2 id="class-dgl-nn-pytorch-gt-PathEncoder-max-len-feat-dim-num-heads-1"><a href="#class-dgl-nn-pytorch-gt-PathEncoder-max-len-feat-dim-num-heads-1" class="headerlink" title="class dgl.nn.pytorch.gt.PathEncoder(max_len, feat_dim, num_heads=1)"></a><code>class dgl.nn.pytorch.gt.PathEncoder(max_len, feat_dim, num_heads=1)</code></h2><p><a href="https://proceedings.neurips.cc/paper/2021/file/f1c1592588411002af340cbaedd6fc33-Paper.pdf">Do Transformers Really Perform Bad for Graph Representation</a>中介绍的路径编码器（Path Encoder）<br>该模块是一个可学习的路径嵌入模块，并将每对节点之间的最短路径编码为注意力偏置。</p><h3 id="Parameters"><a href="#Parameters" class="headerlink" title="Parameters"></a>Parameters</h3><ul><li>max_len(int) 每个路径中要编码的最大边数。每条路径的超出部分将被截断，即截断序列号不小于max_len的边。</li><li>feat_dim(int) - 输入图中边特征的维数。</li><li>num_heads(int, optional) 如果应用多头注意机制，则注意头的数量。预设值：1。</li></ul><h3 id="forward-dist，path-data"><a href="#forward-dist，path-data" class="headerlink" title="forward(dist，path_data)"></a><code>forward(dist，path_data)</code></h3><p>Parameters<br>    - dist(Tensor) - 具有零填充的批处理图的最短路径距离矩阵，形状为<code>(B, N, N)</code> ，其中<code>B</code>是批处理图，<code>N</code>是节点的最大数量。<br>    - path_data(Tensor) - 沿最短路径（零填充）的边特征，形状为<code>(B,N,N,L,d)</code>，其中<code>L</code>是最短路径的最大值，<code>d</code>是边特征的维数。<br>Returns<br>    返回注意偏置作为路径编码，形状为<code>(B,N,N,H)</code> ，其中<code>B</code>是输入图的批次，<code>N</code>是节点的最大数量，以及<code>H</code>是num_heads。</p><p>Return type<br>    torch.Tensor</p><h2 id="源代码"><a href="#源代码" class="headerlink" title="源代码"></a>源代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">PathEncoder</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, max_len, feat_dim, num_heads=<span class="hljs-number">1</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.max_len = max_len<br>        <span class="hljs-variable language_">self</span>.feat_dim = feat_dim<br>        <span class="hljs-variable language_">self</span>.num_heads = num_heads<br>        <span class="hljs-variable language_">self</span>.embedding_table = nn.Embedding(max_len * num_heads, feat_dim)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, dist, path_data</span>):<br>        shortest_distance = th.clamp(dist, <span class="hljs-built_in">min</span>=<span class="hljs-number">1</span>, <span class="hljs-built_in">max</span>=<span class="hljs-variable language_">self</span>.max_len)<br>        edge_embedding = <span class="hljs-variable language_">self</span>.embedding_table.weight.reshape(<br>            <span class="hljs-variable language_">self</span>.max_len, <span class="hljs-variable language_">self</span>.num_heads, -<span class="hljs-number">1</span><br>        )<br>        path_encoding = th.div(<br>            th.einsum(<span class="hljs-string">&quot;bxyld,lhd-&gt;bxyh&quot;</span>, path_data, edge_embedding).permute(<br>                <span class="hljs-number">3</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span><br>            ),<br>            shortest_distance,<br>        ).permute(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">0</span>)<br>        <span class="hljs-keyword">return</span> path_encoding<br></code></pre></td></tr></table></figure><h2 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h2><p>Example1:   </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> th<br><span class="hljs-keyword">import</span> dgl<br><span class="hljs-keyword">from</span> dgl.nn <span class="hljs-keyword">import</span> PathEncoder<br><span class="hljs-keyword">from</span> dgl <span class="hljs-keyword">import</span> shortest_dist<br><br>g = dgl.graph(([<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">3</span>], [<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">0</span>,<span class="hljs-number">3</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]))<br>edata = th.rand(<span class="hljs-number">8</span>, <span class="hljs-number">16</span>)<br><span class="hljs-comment"># Since shortest_dist returns -1 for unreachable node pairs,</span><br><span class="hljs-comment"># edata[-1] should be filled with zero padding.</span><br>edata = th.cat((edata, th.zeros(<span class="hljs-number">1</span>, <span class="hljs-number">16</span>)), dim=<span class="hljs-number">0</span>)<br>dist, path = shortest_dist(g, root=<span class="hljs-literal">None</span>, return_paths=<span class="hljs-literal">True</span>)<br>path_data = edata[path[:, :, :<span class="hljs-number">2</span>]]<br>path_encoder = PathEncoder(<span class="hljs-number">2</span>, <span class="hljs-number">16</span>, num_heads=<span class="hljs-number">8</span>)<br>out = path_encoder(dist.unsqueeze(<span class="hljs-number">0</span>), path_data.unsqueeze(<span class="hljs-number">0</span>))<br><span class="hljs-built_in">print</span>(out.shape)<br></code></pre></td></tr></table></figure><h2 id="shortest-dist"><a href="#shortest-dist" class="headerlink" title="shortest_dist"></a>shortest_dist</h2><p><code>dgl.shortest_dist(g, root=None, return_paths=False, dist=None, method=&#39;dijkstra&#39;)</code><br>计算图中每对节点之间的最短路径距离。如果<code>root</code>为<code>None</code>，则返回所有节点对之间的最短路径距离。如果<code>root</code>不为<code>None</code>，则返回从<code>root</code>到所有其他节点的最短路径距离。</p><h3 id="Parameters-1"><a href="#Parameters-1" class="headerlink" title="Parameters"></a>Parameters</h3><ul><li>g(DGLGraph) - 输入图。</li><li>root(int, optional) - 根节点。如果为<code>None</code>，则返回所有节点对之间的最短路径距离。预设值：<code>None</code>。</li><li>return_paths(bool, optional) - 如果为<code>True</code>，则返回每对节点之间的最短路径。预设值：<code>False</code>。</li></ul><h3 id="Returns"><a href="#Returns" class="headerlink" title="Returns"></a>Returns</h3><ul><li>dist(Tensor) - 如果<code>root</code>是节点ID，返回最短距离，形状是<code>(N,)</code>。如果<code>root</code>是None,则返回最短路径距离矩阵，形状为<code>(N, N)</code>，其中<code>N</code>是节点的最大数量。</li><li>path(Tensor) - 如果<code>root</code>是节点ID，返回最短路径，形状是<code>(N,L)</code>。如果<code>root</code>是None，返回最短路径矩阵，形状为<code>(N, N, L)</code>，其中<code>L</code>是最短路径的最大值。</li></ul><h3 id="Example-1"><a href="#Example-1" class="headerlink" title="Example"></a>Example</h3><p>可以看dgl官网给出的这个例子，这里有四个顶点0，1，2，3，然后有四条边，eid:0 -&gt;(0,2), eid:1 -&gt;(1,0), eid:2 -&gt;(1,3), eid:3 -&gt;(2,3)<br>返回的paths矩阵的内容就对应着这里的eid</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python">g = dgl.graph(([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>]))<br>dgl.shortest_dist(g, root=<span class="hljs-number">0</span>)<br>dist, paths = dgl.shortest_dist(g, root=<span class="hljs-literal">None</span>, return_paths=<span class="hljs-literal">True</span>)<br><span class="hljs-built_in">print</span>(dist)<br><span class="hljs-comment"># tensor([[ 0, -1,  1,  2],</span><br><span class="hljs-comment">#         [ 1,  0,  2,  1],</span><br><span class="hljs-comment">#         [-1, -1,  0,  1],</span><br><span class="hljs-comment">#         [-1, -1, -1,  0]])</span><br><span class="hljs-built_in">print</span>(paths)<br><span class="hljs-comment"># tensor([[[-1, -1],</span><br><span class="hljs-comment">#          [-1, -1],</span><br><span class="hljs-comment">#          [ 0, -1],</span><br><span class="hljs-comment">#          [ 0,  3]],</span><br><br><span class="hljs-comment">#         [[ 1, -1],</span><br><span class="hljs-comment">#          [-1, -1],</span><br><span class="hljs-comment">#          [ 1,  0],</span><br><span class="hljs-comment">#          [ 2, -1]],</span><br><br><span class="hljs-comment">#         [[-1, -1],</span><br><span class="hljs-comment">#          [-1, -1],</span><br><span class="hljs-comment">#          [-1, -1],</span><br><span class="hljs-comment">#          [ 3, -1]],</span><br><br><span class="hljs-comment">#         [[-1, -1],</span><br><span class="hljs-comment">#          [-1, -1],</span><br><span class="hljs-comment">#          [-1, -1],</span><br><span class="hljs-comment">#          [-1, -1]]])</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>SpatialEncoder</title>
    <link href="/p/5cc2d788/"/>
    <url>/p/5cc2d788/</url>
    
    <content type="html"><![CDATA[<h2 id="class-dgl-nn-pytorch-gt-SpatialEncoder-max-dist-num-heads-1"><a href="#class-dgl-nn-pytorch-gt-SpatialEncoder-max-dist-num-heads-1" class="headerlink" title="class dgl.nn.pytorch.gt.SpatialEncoder(max_dist, num_heads=1)"></a><code>class dgl.nn.pytorch.gt.SpatialEncoder(max_dist, num_heads=1)</code></h2><p><a href="https://proceedings.neurips.cc/paper/2021/file/f1c1592588411002af340cbaedd6fc33-Paper.pdf">Do Transformers Really Perform Bad for Graph Representation</a>中介绍的空间编码模块.<br>这个模块是一个可学习的空间嵌入模块，它对每个节点对之间的最短距离进行编码，以获得注意力偏差。</p><h3 id="Parameters"><a href="#Parameters" class="headerlink" title="Parameters"></a>Parameters</h3><ul><li>max_dist(int) 要编码的每个节点对之间的最短路径距离的上限。所有距离将被限制在范围[0，max_dist]内。</li><li>num_heads(int, optional) 如果应用多头注意机制，则注意头的数量。预设值：1。</li></ul><h3 id="forward-dist"><a href="#forward-dist" class="headerlink" title="forward(dist)"></a><code>forward(dist)</code></h3><p>Parameters<br>    dist(Tensor) - 具有-1填充的批处理图的最短路径距离，形状为<code>(B, N, N)</code> ，其中<code>B</code>是批处理图，<code>N</code>是节点的最大数量。</p><p>Returns<br>    返回注意偏置作为空间编码，形状为<code>(B,N,N,H)</code>。</p><p>Return type<br>    torch.Tensor</p><h2 id="源代码"><a href="#源代码" class="headerlink" title="源代码"></a>源代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">SpatialEncoder</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, max_dist, num_heads=<span class="hljs-number">1</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.max_dist = max_dist<br>        <span class="hljs-variable language_">self</span>.num_heads = num_heads<br>        <span class="hljs-comment"># deactivate node pair between which the distance is -1</span><br>        <span class="hljs-variable language_">self</span>.embedding_table = nn.Embedding(<br>            max_dist + <span class="hljs-number">2</span>, num_heads, padding_idx=<span class="hljs-number">0</span><br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, dist</span>):<br>        spatial_encoding = <span class="hljs-variable language_">self</span>.embedding_table(<br>            th.clamp(<br>                dist,<br>                <span class="hljs-built_in">min</span>=-<span class="hljs-number">1</span>,<br>                <span class="hljs-built_in">max</span>=<span class="hljs-variable language_">self</span>.max_dist,<br>            )<br>            + <span class="hljs-number">1</span><br>        )<br>        <span class="hljs-keyword">return</span> spatial_encoding<br></code></pre></td></tr></table></figure><h2 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h2><p>实际上就是把距离值一个数字，embedding成一个长度为num_heads的向量<br>Example1:   </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">from</span> dgl.nn <span class="hljs-keyword">import</span> SpatialEncoder<br><br><span class="hljs-comment"># 创建一个 SpatialEncoder 实例</span><br>spatial_encoder = SpatialEncoder(max_dist=<span class="hljs-number">3</span>, num_heads=<span class="hljs-number">2</span>)<br><span class="hljs-comment"># 输入一些距离值</span><br>distances = torch.tensor([[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [-<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>]])<br><span class="hljs-comment"># 对距离值进行空间编码处理</span><br>output = spatial_encoder(distances)<br><span class="hljs-built_in">print</span>(output)<br><br><span class="hljs-comment"># tensor([[[ 0.1907, -0.5513],</span><br><span class="hljs-comment">#          [ 1.0202,  0.1709],</span><br><span class="hljs-comment">#          [ 1.7177,  0.9194]],</span><br><br><span class="hljs-comment">#         [[ 0.0000,  0.0000],</span><br><span class="hljs-comment">#          [ 0.1681,  0.6919],</span><br><span class="hljs-comment">#          [ 0.1681,  0.6919]]], grad_fn=&lt;EmbeddingBackward0&gt;)</span><br></code></pre></td></tr></table></figure><p>Example2:<br>实际上就是将最短路径矩阵中的每一个数字都embedding成了一个向量，然后当前空间编码   </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> th<br><span class="hljs-keyword">import</span> dgl<br><span class="hljs-keyword">from</span> dgl.nn <span class="hljs-keyword">import</span> SpatialEncoder<br><span class="hljs-keyword">from</span> dgl <span class="hljs-keyword">import</span> shortest_dist<br><br>g1 = dgl.graph(([<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">3</span>], [<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">0</span>,<span class="hljs-number">3</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]))<br>g2 = dgl.graph(([<span class="hljs-number">0</span>,<span class="hljs-number">1</span>], [<span class="hljs-number">1</span>,<span class="hljs-number">0</span>]))<br>n1, n2 = g1.num_nodes(), g2.num_nodes()<br><span class="hljs-comment"># use -1 padding since shortest_dist returns -1 for unreachable node pairs</span><br>dist = -th.ones((<span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>), dtype=th.long)<br>dist[<span class="hljs-number">0</span>, :n1, :n1] = shortest_dist(g1, root=<span class="hljs-literal">None</span>, return_paths=<span class="hljs-literal">False</span>)<br>dist[<span class="hljs-number">1</span>, :n2, :n2] = shortest_dist(g2, root=<span class="hljs-literal">None</span>, return_paths=<span class="hljs-literal">False</span>)<br>spatial_encoder = SpatialEncoder(max_dist=<span class="hljs-number">2</span>, num_heads=<span class="hljs-number">8</span>)<br>out = spatial_encoder(dist)<br><span class="hljs-built_in">print</span>(dist)<br><span class="hljs-comment"># tensor([[[ 0,  1,  1,  1],</span><br><span class="hljs-comment">#          [ 1,  0,  2,  1],</span><br><span class="hljs-comment">#          [ 1,  2,  0,  2],</span><br><span class="hljs-comment">#          [ 1,  1,  2,  0]],</span><br><br><span class="hljs-comment">#         [[ 0,  1, -1, -1],</span><br><span class="hljs-comment">#          [ 1,  0, -1, -1],</span><br><span class="hljs-comment">#          [-1, -1, -1, -1],</span><br><span class="hljs-comment">#          [-1, -1, -1, -1]]])</span><br><span class="hljs-built_in">print</span>(out.shape)<br><span class="hljs-comment"># torch.Size([2, 4, 4, 8])</span><br><span class="hljs-built_in">print</span>(out)<br><span class="hljs-comment"># tensor([[[[-0.1012, -0.0641, -1.3648,  0.5305,  1.6427,  0.9673, -1.4536,</span><br><span class="hljs-comment">#            -1.1487],</span><br><span class="hljs-comment">#           [-0.1723, -0.1548, -1.0919, -2.2756, -0.7477,  1.4145,  0.4393,</span><br><span class="hljs-comment">#            -0.3580],</span><br><span class="hljs-comment">#           [-0.1723, -0.1548, -1.0919, -2.2756, -0.7477,  1.4145,  0.4393,</span><br><span class="hljs-comment">#            -0.3580],</span><br><span class="hljs-comment">#           [-0.1723, -0.1548, -1.0919, -2.2756, -0.7477,  1.4145,  0.4393,</span><br><span class="hljs-comment">#            -0.3580]],</span><br><br><span class="hljs-comment">#          [[-0.1723, -0.1548, -1.0919, -2.2756, -0.7477,  1.4145,  0.4393,</span><br><span class="hljs-comment">#            -0.3580],</span><br><span class="hljs-comment">#           [-0.1012, -0.0641, -1.3648,  0.5305,  1.6427,  0.9673, -1.4536,</span><br><span class="hljs-comment">#            -1.1487],</span><br><span class="hljs-comment">#           [-1.3901, -0.3768,  0.6562,  0.4067, -0.7534,  1.0690, -1.0219,</span><br><span class="hljs-comment">#            -0.9923],</span><br><span class="hljs-comment">#           [-0.1723, -0.1548, -1.0919, -2.2756, -0.7477,  1.4145,  0.4393,</span><br><span class="hljs-comment">#            -0.3580]],</span><br><br><span class="hljs-comment">#          [[-0.1723, -0.1548, -1.0919, -2.2756, -0.7477,  1.4145,  0.4393,</span><br><span class="hljs-comment">#            -0.3580],</span><br><span class="hljs-comment">#           [-1.3901, -0.3768,  0.6562,  0.4067, -0.7534,  1.0690, -1.0219,</span><br><span class="hljs-comment">#            -0.9923],</span><br><span class="hljs-comment">#           [-0.1012, -0.0641, -1.3648,  0.5305,  1.6427,  0.9673, -1.4536,</span><br><span class="hljs-comment">#            -1.1487],</span><br><span class="hljs-comment">#           [-1.3901, -0.3768,  0.6562,  0.4067, -0.7534,  1.0690, -1.0219,</span><br><span class="hljs-comment">#            -0.9923]],</span><br><br><span class="hljs-comment">#          [[-0.1723, -0.1548, -1.0919, -2.2756, -0.7477,  1.4145,  0.4393,</span><br><span class="hljs-comment">#            -0.3580],</span><br><span class="hljs-comment">#           [-0.1723, -0.1548, -1.0919, -2.2756, -0.7477,  1.4145,  0.4393,</span><br><span class="hljs-comment">#            -0.3580],</span><br><span class="hljs-comment">#           [-1.3901, -0.3768,  0.6562,  0.4067, -0.7534,  1.0690, -1.0219,</span><br><span class="hljs-comment">#            -0.9923],</span><br><span class="hljs-comment">#           [-0.1012, -0.0641, -1.3648,  0.5305,  1.6427,  0.9673, -1.4536,</span><br><span class="hljs-comment">#            -1.1487]]],</span><br><br><br><span class="hljs-comment">#         [[[-0.1012, -0.0641, -1.3648,  0.5305,  1.6427,  0.9673, -1.4536,</span><br><span class="hljs-comment">#            -1.1487],</span><br><span class="hljs-comment">#           [-0.1723, -0.1548, -1.0919, -2.2756, -0.7477,  1.4145,  0.4393,</span><br><span class="hljs-comment">#            -0.3580],</span><br><span class="hljs-comment">#           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,</span><br><span class="hljs-comment">#             0.0000],</span><br><span class="hljs-comment">#           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,</span><br><span class="hljs-comment">#             0.0000]],</span><br><br><span class="hljs-comment">#          [[-0.1723, -0.1548, -1.0919, -2.2756, -0.7477,  1.4145,  0.4393,</span><br><span class="hljs-comment">#            -0.3580],</span><br><span class="hljs-comment">#           [-0.1012, -0.0641, -1.3648,  0.5305,  1.6427,  0.9673, -1.4536,</span><br><span class="hljs-comment">#            -1.1487],</span><br><span class="hljs-comment">#           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,</span><br><span class="hljs-comment">#             0.0000],</span><br><span class="hljs-comment">#           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,</span><br><span class="hljs-comment">#             0.0000]],</span><br><span class="hljs-comment">#          [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,</span><br><span class="hljs-comment">#             0.0000],</span><br><span class="hljs-comment">#           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,</span><br><span class="hljs-comment">#             0.0000],</span><br><span class="hljs-comment">#           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,</span><br><span class="hljs-comment">#             0.0000],</span><br><span class="hljs-comment">#           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,</span><br><span class="hljs-comment">#             0.0000]],</span><br><br><span class="hljs-comment">#          [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,</span><br><span class="hljs-comment">#             0.0000],</span><br><span class="hljs-comment">#           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,</span><br><span class="hljs-comment">#             0.0000],</span><br><span class="hljs-comment">#           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,</span><br><span class="hljs-comment">#             0.0000],</span><br><span class="hljs-comment">#           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,</span><br><span class="hljs-comment">#             0.0000]]]], grad_fn=&lt;EmbeddingBackward0&gt;)</span><br></code></pre></td></tr></table></figure><h2 id="def-shortest-dist-g-root-None-return-paths-False"><a href="#def-shortest-dist-g-root-None-return-paths-False" class="headerlink" title="def shortest_dist(g, root=None, return_paths=False)"></a><code>def shortest_dist(g, root=None, return_paths=False)</code></h2><p>dist[0, :n1, :n1] &#x3D; shortest_dist(g1, root&#x3D;None, return_paths&#x3D;False)</p><p>这个函数shortest_dist的作用是计算给定图中的最短距离和路径。以下是这个函数的主要功能和用法介绍：</p><p>功能:   </p><ul><li>该函数用于计算图中节点之间的最短距离和路径。</li><li>它支持无权图的情况。</li><li>只考虑有向路径（其中所有边都朝着同一个方向）。</li></ul><p>参数:</p><ul><li>g : 输入图，必须是同质图。</li><li>root : 给定一个根节点ID，返回根节点与所有节点之间的最短距离和路径（可选）。如果为None，则返回所有节点对的结果。默认为None。</li><li>return_paths : 如果为True，则返回与最短距离对应的最短路径。默认为False。</li></ul><p>返回值:</p><ul><li>dist : 最短距离张量。<br>  如果root是一个节点ID，则形状为(N,)，其中N是节点数。dist[j]给出从root到节点j的最短距离。<br>  否则，形状为(N, N)。dist[i][j]给出从节点i到节点j的最短距离。<br>  无法到达的节点对的距离值填充为-1。</li><li>paths : 最短路径张量（可选）。<br>  仅在return_paths为True时返回。<br>  如果root是一个节点ID，则形状为(N, L)，其中L是最长路径的长度。path[j]是从root到节点j的最短路径。<br>  否则，形状为(N, N, L)。path[i][j]是从节点i到节点j的最短路径。<br>  每条路径是一个向量，由边ID构成，末尾填充为-1。<br>  节点与自身之间的最短路径是一个由-1填充的向量。</li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>nn.LayerNorm</title>
    <link href="/p/79f6ba2e/"/>
    <url>/p/79f6ba2e/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>nn.BatchNorm</title>
    <link href="/p/2cea6c42/"/>
    <url>/p/2cea6c42/</url>
    
    <content type="html"><![CDATA[<h2 id="Batch-Normalization的使用场景"><a href="#Batch-Normalization的使用场景" class="headerlink" title="Batch Normalization的使用场景"></a>Batch Normalization的使用场景</h2><p>深层神经网络：在深度神经网络中，梯度消失和梯度爆炸问题更加明显，Batch Normalization可以帮助缓解这些问题，加速模型的训练和收敛。<br>非线性激活函数：使用非线性激活函数（如ReLU）时，Batch Normalization可以减少梯度消失问题，提高模型性能。<br>大规模数据集：在大规模数据集上，Batch Normalization的效果通常更加显著，可以提高模型的泛化能力。<br>需要更高学习率：稳定的输入分布使得可以使用更高的学习率，加快模型收敛速度。</p><h2 id="Batch-Normalization的用法"><a href="#Batch-Normalization的用法" class="headerlink" title="Batch Normalization的用法"></a>Batch Normalization的用法</h2><p>在PyTorch中，使用nn.BatchNorm模块添加Batch Normalization层到神经网络中。以下是一些常见用法和注意事项：</p><p>添加Batch Normalization层：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyModel</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(MyModel, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.fc1 = nn.Linear(<span class="hljs-number">784</span>, <span class="hljs-number">256</span>)<br>        <span class="hljs-variable language_">self</span>.bn = nn.BatchNorm1d(<span class="hljs-number">256</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = <span class="hljs-variable language_">self</span>.fc1(x)<br>        x = <span class="hljs-variable language_">self</span>.bn(x)<br>        <span class="hljs-keyword">return</span> x<br><br>model = MyModel()<br></code></pre></td></tr></table></figure><p>训练时和推断时的不同：在训练时，Batch Normalization使用当前mini-batch的均值和方差进行标准化；在推断时，通常使用整个训练集的均值和方差进行标准化。</p><p>注意事项：<br>Batch Normalization在卷积层和全连接层中均可使用。<br>当网络较小时，可能没有必要使用Batch Normalization。<br>可以通过调整momentum参数控制均值和方差的移动平均更新速度。<br>总结<br>Batch Normalization是一种强大的技术，对于加速模型训练、提高性能和泛化能力至关重要。在深度神经网络、非线性激活函数、大规模数据集和需要更高学习率的场景下，使用Batch Normalization能够取得更好的效果。通过正确使用Batch Normalization，可以优化深度学习模型的训练过程，提高模型的性能和泛化能力。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>PyTorch中的nn.Dropout</title>
    <link href="/p/b928e7f1/"/>
    <url>/p/b928e7f1/</url>
    
    <content type="html"><![CDATA[<p>在深度学习中，过拟合是一个常见的问题，而 nn.Dropout 是一种常用的正则化技术，有助于减少过拟合现象。<br>nn.Dropout 是 PyTorch 中用于应用 Dropout 正则化的模块。</p><h2 id="作用"><a href="#作用" class="headerlink" title="作用"></a>作用</h2><p>在训练阶段，nn.Dropout 按照指定的概率随机丢弃输入张量中的部分元素，以减少神经网络的复杂性。这有助于提高模型的泛化能力，避免过拟合。</p><h2 id="nn-Dropout-的基本用法"><a href="#nn-Dropout-的基本用法" class="headerlink" title="nn.Dropout 的基本用法"></a>nn.Dropout 的基本用法</h2><p><code>torch.nn.Dropout(p=0.5, inplace=False)</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyModel</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(MyModel, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.fc1 = nn.Linear(<span class="hljs-number">10</span>, <span class="hljs-number">5</span>)<br>        <span class="hljs-variable language_">self</span>.dropout = nn.Dropout(p=<span class="hljs-number">0.5</span>)  <span class="hljs-comment"># 添加 Dropout 模块</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = <span class="hljs-variable language_">self</span>.fc1(x)<br>        x = <span class="hljs-variable language_">self</span>.dropout(x)  <span class="hljs-comment"># 在前向传播函数中应用 Dropout</span><br>        <span class="hljs-keyword">return</span> x<br><br><span class="hljs-comment"># 创建模型实例</span><br>model = MyModel()<br></code></pre></td></tr></table></figure><p>训练和测试阶段的使用<br>在训练和测试阶段，需要注意 nn.Dropout 的行为不同：</p><p>训练阶段：nn.Dropout 会按照指定的概率丢弃输入元素。<br>测试阶段：nn.Dropout 不会丢弃输入元素，而是将输入内容原样输出。</p><p>不需要用if语句来控制是否是训练阶段，只需在训练阶段设置<code>model.train()</code>，在测试阶段设置<code>model.eval()</code>即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">input_data = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">10</span>)<br><br><span class="hljs-comment"># 训练阶段</span><br>model.train()<br>output_train = model(input_data)<br><br><span class="hljs-comment"># 测试阶段</span><br>model.<span class="hljs-built_in">eval</span>()<br>output_test = model(input_data)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Output during training:&quot;</span>, output_train)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Output during testing:&quot;</span>, output_test)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>PyTorch中的nn.Embedding</title>
    <link href="/p/511f4ac6/"/>
    <url>/p/511f4ac6/</url>
    
    <content type="html"><![CDATA[<p>nn.Embedding 是PyTorch中的一个模块，用于将整数索引映射到连续的低维向量表示。这个操作在自然语言处理（NLP）和推荐系统等领域中非常常见，因为它可以有效地学习输入数据中的离散特征。</p><h2 id="作用："><a href="#作用：" class="headerlink" title="作用："></a>作用：</h2><p>将整数索引映射为低维向量表示：nn.Embedding 能够将离散的整数索引（如单词、类别等）映射为低维的实数向量。<br>学习数据中的结构信息：这种嵌入操作能够学习输入数据中的结构信息，使得神经网络可以更好地处理这些数据。<br>我个人感觉 nn.Embedding 就是把原本固定的特征便成了一个神经网络可学习的特征</p><h2 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h2><p>比如我有许多类别，包含M，V，Q，D，R，C我怎么把它们embed</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><br><span class="hljs-comment"># 定义类别到索引的映射</span><br>category_to_index = &#123;<span class="hljs-string">&quot;M&quot;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&quot;V&quot;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&quot;Q&quot;</span>: <span class="hljs-number">2</span>, <span class="hljs-string">&quot;D&quot;</span>: <span class="hljs-number">3</span>, <span class="hljs-string">&quot;R&quot;</span>: <span class="hljs-number">4</span>, <span class="hljs-string">&quot;C&quot;</span>: <span class="hljs-number">5</span>&#125;<br>num_categories = <span class="hljs-built_in">len</span>(category_to_index)<br><span class="hljs-comment"># 定义嵌入层，假设每个类别映射为一个3维向量</span><br>embedding_dim = <span class="hljs-number">3</span><br>embedding = nn.Embedding(num_categories, embedding_dim)<br><span class="hljs-comment"># 准备输入数据，将类别转换为对应的索引</span><br>input_categories = [<span class="hljs-string">&quot;M&quot;</span>, <span class="hljs-string">&quot;V&quot;</span>, <span class="hljs-string">&quot;Q&quot;</span>]<br>input_indices = torch.LongTensor([category_to_index[cat] <span class="hljs-keyword">for</span> cat <span class="hljs-keyword">in</span> input_categories])<br><span class="hljs-comment"># 应用嵌入层</span><br>embedded_vectors = embedding(input_indices)<br><span class="hljs-built_in">print</span>(embedded_vectors)<br><span class="hljs-comment"># tensor([[-0.5249, 1.0863, 1.6412], </span><br><span class="hljs-comment">#         [1.7894, -1.5931, -1.1443], </span><br><span class="hljs-comment">#         [1.5128, 1.8498, 0.5345]], grad_fn=&lt;EmbeddingBackward0&gt;)</span><br></code></pre></td></tr></table></figure><h2 id="Pytorch文档代码"><a href="#Pytorch文档代码" class="headerlink" title="Pytorch文档代码"></a>Pytorch文档代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># mypy: allow-untyped-defs</span><br><span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> <span class="hljs-type">Optional</span><br><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> Tensor<br><span class="hljs-keyword">from</span> torch.nn.parameter <span class="hljs-keyword">import</span> Parameter<br><br><span class="hljs-keyword">from</span> .module <span class="hljs-keyword">import</span> Module<br><span class="hljs-keyword">from</span> .. <span class="hljs-keyword">import</span> functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">from</span> .. <span class="hljs-keyword">import</span> init<br><br>__all__ = [<span class="hljs-string">&#x27;Embedding&#x27;</span>, <span class="hljs-string">&#x27;EmbeddingBag&#x27;</span>]<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Embedding</span>(<span class="hljs-title class_ inherited__">Module</span>):<br>    <span class="hljs-string">r&quot;&quot;&quot;一个简单的查找表，存储固定字典和大小的嵌入。</span><br><span class="hljs-string">    该模块通常用于存储词嵌入并使用索引检索它们。</span><br><span class="hljs-string">    该模块的输入是索引列表，输出是相应的词嵌入。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        num_embeddings (int): 嵌入词典的大小</span><br><span class="hljs-string">        embedding_dim (int): 每个嵌入向量的大小</span><br><span class="hljs-string">        padding_idx (int, optional): 如果指定，`padding_idx` 处的条目不会对梯度产生贡献；</span><br><span class="hljs-string">                                     因此，`padding_idx` 处的嵌入向量在训练期间不会更新，</span><br><span class="hljs-string">                                     即它仍为固定的“填充”。对于新构建的嵌入，</span><br><span class="hljs-string">                                     :attr:`padding_idx` 处的嵌入向量将默认为全零，</span><br><span class="hljs-string">                                     但可以更新为其他值以用作填充向量。</span><br><span class="hljs-string">        max_norm (float, optional): 如果给定，则每个范数大于`max_norm` 的嵌入向量将被重新规范化以具有范数`max_norm`。</span><br><span class="hljs-string">        norm_type (float, optional): 用于计算 `max_norm` 选项的 p 范数的 p。默认值为 ``2``。</span><br><span class="hljs-string">        scale_grad_by_freq (bool, optional): 如果指定，这将按小批量中单词频率的倒数缩放梯度。默认为“False”。</span><br><span class="hljs-string">        sparse (bool, optional): 如果为“True”，则相对于 “weight” 矩阵的梯度将为稀疏张量。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Attributes:</span><br><span class="hljs-string">        weight (Tensor): 形状为 (num_embeddings, embedding_dim) 的模块的可学习权重</span><br><span class="hljs-string">                         从 :math:`\mathcal&#123;N&#125;(0, 1)` 初始化</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Shape:</span><br><span class="hljs-string">        - Input: :math:`(*)`, 包含要提取的索引的任意形状的 IntTensor 或 LongTensor</span><br><span class="hljs-string">        - Output: :math:`(*, H)`, 其中 `*` 是输入形状，math:`H=\text&#123;embedding\_dim&#125;`</span><br><span class="hljs-string">    .. note::</span><br><span class="hljs-string">        请记住，只有有限数量的优化器支持稀疏梯度：目前是：class:`optim.SGD`（`CUDA` 和 `CPU`）、</span><br><span class="hljs-string">        ：class:`optim.SparseAdam`（`CUDA` 和 `CPU`）和：class:`optim.Adagrad`（`CPU`）</span><br><span class="hljs-string"></span><br><span class="hljs-string">    .. note::</span><br><span class="hljs-string">        当 `max_norm` 不为 ``None`` 时，:class:`Embedding` 的 forward 方法将就地修改 `weight` 张量。由于梯度计算所需的张量无法就地修改，因此在调用 :class:`Embedding` 的 forward 方法之前对 ``Embedding.weight`` 执行可微分运算需要在 `max_norm` 不为 ``None`` 时克隆 ``Embedding.weight``。</span><br><span class="hljs-string">        For example::</span><br><span class="hljs-string"></span><br><span class="hljs-string">            n, d, m = 3, 5, 7</span><br><span class="hljs-string">            embedding = nn.Embedding(n, d, max_norm=True)</span><br><span class="hljs-string">            W = torch.randn((m, d), requires_grad=True)</span><br><span class="hljs-string">            idx = torch.tensor([1, 2])</span><br><span class="hljs-string">            a = embedding.weight.clone() @ W.t()  # weight must be cloned for this to be differentiable</span><br><span class="hljs-string">            b = embedding(idx) @ W.t()  # modifies weight in-place</span><br><span class="hljs-string">            out = (a.unsqueeze(0) + b.unsqueeze(1))</span><br><span class="hljs-string">            loss = out.sigmoid().prod()</span><br><span class="hljs-string">            loss.backward()</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Examples::</span><br><span class="hljs-string"></span><br><span class="hljs-string">        &gt;&gt;&gt; # 一个包含 10 个大小为 3 的张量的嵌入模块</span><br><span class="hljs-string">        &gt;&gt;&gt; embedding = nn.Embedding(10, 3)</span><br><span class="hljs-string">        &gt;&gt;&gt; # 一批 2 个样本，每个样本包含 4 个索引</span><br><span class="hljs-string">        &gt;&gt;&gt; input = torch.LongTensor([[1, 2, 4, 5], [4, 3, 2, 9]])</span><br><span class="hljs-string">        &gt;&gt;&gt; # xdoctest: +IGNORE_WANT(&quot;non-deterministic&quot;)</span><br><span class="hljs-string">        &gt;&gt;&gt; embedding(input)</span><br><span class="hljs-string">        tensor([[[-0.0251, -1.6902,  0.7172],</span><br><span class="hljs-string">                 [-0.6431,  0.0748,  0.6969],</span><br><span class="hljs-string">                 [ 1.4970,  1.3448, -0.9685],</span><br><span class="hljs-string">                 [-0.3677, -2.7265, -0.1685]],</span><br><span class="hljs-string"></span><br><span class="hljs-string">                [[ 1.4970,  1.3448, -0.9685],</span><br><span class="hljs-string">                 [ 0.4362, -0.4004,  0.9400],</span><br><span class="hljs-string">                 [-0.6431,  0.0748,  0.6969],</span><br><span class="hljs-string">                 [ 0.9124, -2.3616,  1.1151]]])</span><br><span class="hljs-string"></span><br><span class="hljs-string"></span><br><span class="hljs-string">        &gt;&gt;&gt; # 带有 padding_idx 的示例</span><br><span class="hljs-string">        &gt;&gt;&gt; embedding = nn.Embedding(10, 3, padding_idx=0)</span><br><span class="hljs-string">        &gt;&gt;&gt; input = torch.LongTensor([[0, 2, 0, 5]])</span><br><span class="hljs-string">        &gt;&gt;&gt; embedding(input)</span><br><span class="hljs-string">        tensor([[[ 0.0000,  0.0000,  0.0000],</span><br><span class="hljs-string">                 [ 0.1535, -2.0309,  0.9315],</span><br><span class="hljs-string">                 [ 0.0000,  0.0000,  0.0000],</span><br><span class="hljs-string">                 [-0.1655,  0.9897,  0.0635]]])</span><br><span class="hljs-string"></span><br><span class="hljs-string">        &gt;&gt;&gt; # 改变 `pad` 向量的示例</span><br><span class="hljs-string">        &gt;&gt;&gt; padding_idx = 0</span><br><span class="hljs-string">        &gt;&gt;&gt; embedding = nn.Embedding(3, 3, padding_idx=padding_idx)</span><br><span class="hljs-string">        &gt;&gt;&gt; embedding.weight</span><br><span class="hljs-string">        Parameter containing:</span><br><span class="hljs-string">        tensor([[ 0.0000,  0.0000,  0.0000],</span><br><span class="hljs-string">                [-0.7895, -0.7089, -0.0364],</span><br><span class="hljs-string">                [ 0.6778,  0.5803,  0.2678]], requires_grad=True)</span><br><span class="hljs-string">        &gt;&gt;&gt; with torch.no_grad():</span><br><span class="hljs-string">        ...     embedding.weight[padding_idx] = torch.ones(3)</span><br><span class="hljs-string">        &gt;&gt;&gt; embedding.weight</span><br><span class="hljs-string">        Parameter containing:</span><br><span class="hljs-string">        tensor([[ 1.0000,  1.0000,  1.0000],</span><br><span class="hljs-string">                [-0.7895, -0.7089, -0.0364],</span><br><span class="hljs-string">                [ 0.6778,  0.5803,  0.2678]], requires_grad=True)</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    __constants__ = [<span class="hljs-string">&#x27;num_embeddings&#x27;</span>, <span class="hljs-string">&#x27;embedding_dim&#x27;</span>, <span class="hljs-string">&#x27;padding_idx&#x27;</span>, <span class="hljs-string">&#x27;max_norm&#x27;</span>,<br>                     <span class="hljs-string">&#x27;norm_type&#x27;</span>, <span class="hljs-string">&#x27;scale_grad_by_freq&#x27;</span>, <span class="hljs-string">&#x27;sparse&#x27;</span>]<br><br>    num_embeddings: <span class="hljs-built_in">int</span><br>    embedding_dim: <span class="hljs-built_in">int</span><br>    padding_idx: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>]<br>    max_norm: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">float</span>]<br>    norm_type: <span class="hljs-built_in">float</span><br>    scale_grad_by_freq: <span class="hljs-built_in">bool</span><br>    weight: Tensor<br>    freeze: <span class="hljs-built_in">bool</span><br>    sparse: <span class="hljs-built_in">bool</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, num_embeddings: <span class="hljs-built_in">int</span>, embedding_dim: <span class="hljs-built_in">int</span>, padding_idx: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">                 max_norm: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">float</span>] = <span class="hljs-literal">None</span>, norm_type: <span class="hljs-built_in">float</span> = <span class="hljs-number">2.</span>, scale_grad_by_freq: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">                 sparse: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span>, _weight: <span class="hljs-type">Optional</span>[Tensor] = <span class="hljs-literal">None</span>, _freeze: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">                 device=<span class="hljs-literal">None</span>, dtype=<span class="hljs-literal">None</span></span>) -&gt; <span class="hljs-literal">None</span>:<br>        factory_kwargs = &#123;<span class="hljs-string">&#x27;device&#x27;</span>: device, <span class="hljs-string">&#x27;dtype&#x27;</span>: dtype&#125;<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.num_embeddings = num_embeddings<br>        <span class="hljs-variable language_">self</span>.embedding_dim = embedding_dim<br>        <span class="hljs-keyword">if</span> padding_idx <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">if</span> padding_idx &gt; <span class="hljs-number">0</span>:<br>                <span class="hljs-keyword">assert</span> padding_idx &lt; <span class="hljs-variable language_">self</span>.num_embeddings, <span class="hljs-string">&#x27;Padding_idx must be within num_embeddings&#x27;</span><br>            <span class="hljs-keyword">elif</span> padding_idx &lt; <span class="hljs-number">0</span>:<br>                <span class="hljs-keyword">assert</span> padding_idx &gt;= -<span class="hljs-variable language_">self</span>.num_embeddings, <span class="hljs-string">&#x27;Padding_idx must be within num_embeddings&#x27;</span><br>                padding_idx = <span class="hljs-variable language_">self</span>.num_embeddings + padding_idx<br>        <span class="hljs-variable language_">self</span>.padding_idx = padding_idx<br>        <span class="hljs-variable language_">self</span>.max_norm = max_norm<br>        <span class="hljs-variable language_">self</span>.norm_type = norm_type<br>        <span class="hljs-variable language_">self</span>.scale_grad_by_freq = scale_grad_by_freq<br>        <span class="hljs-keyword">if</span> _weight <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-variable language_">self</span>.weight = Parameter(torch.empty((num_embeddings, embedding_dim), **factory_kwargs),<br>                                    requires_grad=<span class="hljs-keyword">not</span> _freeze)<br>            <span class="hljs-variable language_">self</span>.reset_parameters()<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">assert</span> <span class="hljs-built_in">list</span>(_weight.shape) == [num_embeddings, embedding_dim], \<br>                <span class="hljs-string">&#x27;Shape of weight does not match num_embeddings and embedding_dim&#x27;</span><br>            <span class="hljs-variable language_">self</span>.weight = Parameter(_weight, requires_grad=<span class="hljs-keyword">not</span> _freeze)<br><br>        <span class="hljs-variable language_">self</span>.sparse = sparse<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">reset_parameters</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-literal">None</span>:<br>        init.normal_(<span class="hljs-variable language_">self</span>.weight)<br>        <span class="hljs-variable language_">self</span>._fill_padding_idx_with_zero()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_fill_padding_idx_with_zero</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.padding_idx <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">with</span> torch.no_grad():<br>                <span class="hljs-variable language_">self</span>.weight[<span class="hljs-variable language_">self</span>.padding_idx].fill_(<span class="hljs-number">0</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, <span class="hljs-built_in">input</span>: Tensor</span>) -&gt; Tensor:<br>        <span class="hljs-keyword">return</span> F.embedding(<br>            <span class="hljs-built_in">input</span>, <span class="hljs-variable language_">self</span>.weight, <span class="hljs-variable language_">self</span>.padding_idx, <span class="hljs-variable language_">self</span>.max_norm,<br>            <span class="hljs-variable language_">self</span>.norm_type, <span class="hljs-variable language_">self</span>.scale_grad_by_freq, <span class="hljs-variable language_">self</span>.sparse)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">extra_repr</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-built_in">str</span>:<br>        s = <span class="hljs-string">&#x27;&#123;num_embeddings&#125;, &#123;embedding_dim&#125;&#x27;</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.padding_idx <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            s += <span class="hljs-string">&#x27;, padding_idx=&#123;padding_idx&#125;&#x27;</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.max_norm <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            s += <span class="hljs-string">&#x27;, max_norm=&#123;max_norm&#125;&#x27;</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.norm_type != <span class="hljs-number">2</span>:<br>            s += <span class="hljs-string">&#x27;, norm_type=&#123;norm_type&#125;&#x27;</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.scale_grad_by_freq <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">False</span>:<br>            s += <span class="hljs-string">&#x27;, scale_grad_by_freq=&#123;scale_grad_by_freq&#125;&#x27;</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.sparse <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">False</span>:<br>            s += <span class="hljs-string">&#x27;, sparse=True&#x27;</span><br>        <span class="hljs-keyword">return</span> s.<span class="hljs-built_in">format</span>(**<span class="hljs-variable language_">self</span>.__dict__)<br><br><br><span class="hljs-meta">    @classmethod</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">from_pretrained</span>(<span class="hljs-params">cls, embeddings, freeze=<span class="hljs-literal">True</span>, padding_idx=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">                        max_norm=<span class="hljs-literal">None</span>, norm_type=<span class="hljs-number">2.</span>, scale_grad_by_freq=<span class="hljs-literal">False</span>,</span><br><span class="hljs-params">                        sparse=<span class="hljs-literal">False</span></span>):<br>        <span class="hljs-string">r&quot;&quot;&quot;Create Embedding instance from given 2-dimensional FloatTensor.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            embeddings (Tensor): FloatTensor containing weights for the Embedding.</span><br><span class="hljs-string">                First dimension is being passed to Embedding as ``num_embeddings``, second as ``embedding_dim``.</span><br><span class="hljs-string">            freeze (bool, optional): If ``True``, the tensor does not get updated in the learning process.</span><br><span class="hljs-string">                Equivalent to ``embedding.weight.requires_grad = False``. Default: ``True``</span><br><span class="hljs-string">            padding_idx (int, optional): If specified, the entries at :attr:`padding_idx` do not contribute to the gradient;</span><br><span class="hljs-string">                                         therefore, the embedding vector at :attr:`padding_idx` is not updated during training,</span><br><span class="hljs-string">                                         i.e. it remains as a fixed &quot;pad&quot;.</span><br><span class="hljs-string">            max_norm (float, optional): See module initialization documentation.</span><br><span class="hljs-string">            norm_type (float, optional): See module initialization documentation. Default ``2``.</span><br><span class="hljs-string">            scale_grad_by_freq (bool, optional): See module initialization documentation. Default ``False``.</span><br><span class="hljs-string">            sparse (bool, optional): See module initialization documentation.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Examples::</span><br><span class="hljs-string"></span><br><span class="hljs-string">            &gt;&gt;&gt; # FloatTensor containing pretrained weights</span><br><span class="hljs-string">            &gt;&gt;&gt; weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]])</span><br><span class="hljs-string">            &gt;&gt;&gt; embedding = nn.Embedding.from_pretrained(weight)</span><br><span class="hljs-string">            &gt;&gt;&gt; # Get embeddings for index 1</span><br><span class="hljs-string">            &gt;&gt;&gt; input = torch.LongTensor([1])</span><br><span class="hljs-string">            &gt;&gt;&gt; # xdoctest: +IGNORE_WANT(&quot;non-deterministic&quot;)</span><br><span class="hljs-string">            &gt;&gt;&gt; embedding(input)</span><br><span class="hljs-string">            tensor([[ 4.0000,  5.1000,  6.3000]])</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">assert</span> embeddings.dim() == <span class="hljs-number">2</span>, \<br>            <span class="hljs-string">&#x27;Embeddings parameter is expected to be 2-dimensional&#x27;</span><br>        rows, cols = embeddings.shape<br>        embedding = cls(<br>            num_embeddings=rows,<br>            embedding_dim=cols,<br>            _weight=embeddings,<br>            _freeze=freeze,<br>            padding_idx=padding_idx,<br>            max_norm=max_norm,<br>            norm_type=norm_type,<br>            scale_grad_by_freq=scale_grad_by_freq,<br>            sparse=sparse)<br>        <span class="hljs-keyword">return</span> embedding<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>ML</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Python Re模块</title>
    <link href="/p/792e5600/"/>
    <url>/p/792e5600/</url>
    
    <content type="html"><![CDATA[<h2 id="使用Python的re模块进行正则表达式处理"><a href="#使用Python的re模块进行正则表达式处理" class="headerlink" title="使用Python的re模块进行正则表达式处理"></a>使用Python的<code>re</code>模块进行正则表达式处理</h2><p>Python的<code>re</code>模块提供了强大的正则表达式功能，用于字符串的查找、替换和拆分。本文将介绍<code>re</code>模块的基本用法，并通过示例代码展示其强大之处。</p><h3 id="什么是正则表达式？"><a href="#什么是正则表达式？" class="headerlink" title="什么是正则表达式？"></a>什么是正则表达式？</h3><p>正则表达式是一种用于匹配字符串的模式。它可以用来验证输入、查找特定的字符串模式、替换文本等。<code>re</code>模块是Python中处理正则表达式的标准库。</p><h3 id="导入re模块"><a href="#导入re模块" class="headerlink" title="导入re模块"></a>导入<code>re</code>模块</h3><p>在使用正则表达式之前，需要先导入<code>re</code>模块：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> re<br></code></pre></td></tr></table></figure><h3 id="基本函数介绍"><a href="#基本函数介绍" class="headerlink" title="基本函数介绍"></a>基本函数介绍</h3><p><code>re</code>模块提供了几个核心函数来处理正则表达式：</p><ol><li><code>re.match(pattern, string, flags=0)</code>：从字符串的起始位置匹配一个模式。</li><li><code>re.search(pattern, string, flags=0)</code>：在整个字符串中搜索模式。</li><li><code>re.findall(pattern, string, flags=0)</code>：返回字符串中所有匹配模式的子串。</li><li><code>re.finditer(pattern, string, flags=0)</code>：返回字符串中所有匹配模式的迭代器。</li><li><code>re.sub(pattern, repl, string, count=0, flags=0)</code>：替换字符串中匹配模式的子串。</li><li><code>re.split(pattern, string, maxsplit=0, flags=0)</code>：按照模式拆分字符串。</li><li><code>re.compile(pattern, flags=0)</code>：编译一个正则表达式模式，返回一个<code>Pattern</code>对象，用于提高多次使用该模式时的效率。</li></ol><h3 id="常用模式符号"><a href="#常用模式符号" class="headerlink" title="常用模式符号"></a>常用模式符号</h3><ul><li><code>.</code>：匹配任意单个字符（除换行符外）。</li><li><code>^</code>：匹配字符串的开头。</li><li><code>$</code>：匹配字符串的结尾。</li><li><code>*</code>：匹配前面的字符零次或多次。</li><li><code>+</code>：匹配前面的字符一次或多次。</li><li><code>?</code>：匹配前面的字符零次或一次。</li><li><code>\d</code>：匹配任何数字字符，相当于<code>[0-9]</code>。</li><li><code>\w</code>：匹配任何字母数字字符，相当于<code>[a-zA-Z0-9_]</code>。</li><li><code>\s</code>：匹配任何空白字符（空格、制表符、换行符等）。</li><li><code>[]</code>：匹配括号内的任意一个字符。</li><li><code>|</code>：表示“或”操作，匹配符号两边的任意一个模式。</li></ul><h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><h4 id="匹配模式"><a href="#匹配模式" class="headerlink" title="匹配模式"></a>匹配模式</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> re<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">match_example</span>():<br>    pattern = <span class="hljs-string">r&#x27;\d+&#x27;</span>  <span class="hljs-comment"># 匹配一个或多个数字</span><br>    string = <span class="hljs-string">&quot;There are 123 apples and 45 oranges.&quot;</span><br>    <span class="hljs-keyword">match</span> = re.<span class="hljs-keyword">match</span>(pattern, string)<br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">match</span>:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Match found:&quot;</span>, <span class="hljs-keyword">match</span>.group())<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;No match found&quot;</span>)<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    match_example()<br></code></pre></td></tr></table></figure><h4 id="搜索模式"><a href="#搜索模式" class="headerlink" title="搜索模式"></a>搜索模式</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> re<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">search_example</span>():<br>    pattern = <span class="hljs-string">r&#x27;\d+&#x27;</span>  <span class="hljs-comment"># 匹配一个或多个数字</span><br>    string = <span class="hljs-string">&quot;There are 123 apples and 45 oranges.&quot;</span><br>    search = re.search(pattern, string)<br>    <span class="hljs-keyword">if</span> search:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Search found:&quot;</span>, search.group())<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;No search found&quot;</span>)<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    search_example()<br></code></pre></td></tr></table></figure><h4 id="查找所有匹配模式"><a href="#查找所有匹配模式" class="headerlink" title="查找所有匹配模式"></a>查找所有匹配模式</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> re<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">findall_example</span>():<br>    pattern = <span class="hljs-string">r&#x27;\d+&#x27;</span>  <span class="hljs-comment"># 匹配一个或多个数字</span><br>    string = <span class="hljs-string">&quot;There are 123 apples and 45 oranges.&quot;</span><br>    matches = re.findall(pattern, string)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Findall matches:&quot;</span>, matches)<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    findall_example()<br></code></pre></td></tr></table></figure><h4 id="替换匹配模式"><a href="#替换匹配模式" class="headerlink" title="替换匹配模式"></a>替换匹配模式</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> re<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sub_example</span>():<br>    pattern = <span class="hljs-string">r&#x27;\d+&#x27;</span>  <span class="hljs-comment"># 匹配一个或多个数字</span><br>    string = <span class="hljs-string">&quot;There are 123 apples and 45 oranges.&quot;</span><br>    result = re.sub(pattern, <span class="hljs-string">&#x27;#&#x27;</span>, string)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Sub result:&quot;</span>, result)<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    test_sub_example()<br></code></pre></td></tr></table></figure><h4 id="拆分字符串"><a href="#拆分字符串" class="headerlink" title="拆分字符串"></a>拆分字符串</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> re<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">split_example</span>():<br>    pattern = <span class="hljs-string">r&#x27;\d+&#x27;</span>  <span class="hljs-comment"># 匹配一个或多个数字</span><br>    string = <span class="hljs-string">&quot;There are 123 apples and 45 oranges.&quot;</span><br>    result = re.split(pattern, string)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Split result:&quot;</span>, result)<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    split_example()<br></code></pre></td></tr></table></figure><h4 id="编译正则表达式"><a href="#编译正则表达式" class="headerlink" title="编译正则表达式"></a>编译正则表达式</h4><p>使用<code>re.compile</code>可以将正则表达式编译成一个模式对象，以提高多次使用该模式的效率。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> re<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">compile_example</span>():<br>    pattern = re.<span class="hljs-built_in">compile</span>(<span class="hljs-string">r&#x27;\d+&#x27;</span>)  <span class="hljs-comment"># 编译一个匹配一个或多个数字的正则表达式</span><br>    string = <span class="hljs-string">&quot;There are 123 apples and 45 oranges.&quot;</span><br>    <span class="hljs-keyword">match</span> = pattern.<span class="hljs-keyword">match</span>(string)<br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">match</span>:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Match found:&quot;</span>, <span class="hljs-keyword">match</span>.group())<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;No match found&quot;</span>)<br><br>    search = pattern.search(string)<br>    <span class="hljs-keyword">if</span> search:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Search found:&quot;</span>, search.group())<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;No search found&quot;</span>)<br><br>    matches = pattern.findall(string)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Findall matches:&quot;</span>, matches)<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    compile_example()<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>分类问题评价指标</title>
    <link href="/p/93b65aed/"/>
    <url>/p/93b65aed/</url>
    
    <content type="html"><![CDATA[<h1 id="分类问题的评价指标"><a href="#分类问题的评价指标" class="headerlink" title="分类问题的评价指标"></a>分类问题的评价指标</h1><h2 id="Accuracy"><a href="#Accuracy" class="headerlink" title="Accuracy"></a>Accuracy</h2><p>准确率是预测正确的样本占总样本的百分比。</p><p>$$ Accuracy &#x3D; \frac{TP + TN}{TP + TN + FP + FN} $$<br>其中TP、TN、FP、FN分别代表真正例、真负例、假正例、假负例。</p><p>准确率的缺点：在类别不均衡的情况下，准确率这个指标无法客观的评价算法的优劣。<br>比如有100个样本，99个是正样本，1个是反样本，这时候算法只要全部输出为正，准确率就能达到99%，此时的数值虽然很高，但是又有什么意义呢</p><h2 id="Precision"><a href="#Precision" class="headerlink" title="Precision"></a>Precision</h2><p>精确率又称查准率，它是针对预测结果而言的，它的含义是在所有被预测为正的样本中实际为正的样本的概率，意思就是在预测为正样本的结果中，我们有多少把握可以预测正确。精准率和准确率看上去有些类似，但是完全不同的两个概念。精准率代表对正样本结果中的预测准确程度，而准确率则代表整体的预测准确程度，既包括正样本，也包括负样本。</p><p>$$ Precision &#x3D; \frac{TP}{TP + FP} $$<br>表示预测为正例的样本中，真正例的比例。</p><h2 id="Recall"><a href="#Recall" class="headerlink" title="Recall"></a>Recall</h2><p>召回率（Recall）又叫查全率，它是针对原样本而言的，它的含义是在实际为正的样本中被预测为正样本的概率，其公式如下：</p><p>$$ Recall &#x3D; \frac{TP}{TP + FN} $$<br>表示实际为正例的样本中，真正例的比例。</p><h2 id="F1-Score"><a href="#F1-Score" class="headerlink" title="F1 Score"></a>F1 Score</h2><p>Precision和Recall指标有时是此消彼长的，即精准率高了，召回率就下降，在一些场景下要兼顾精准率和召回率，最常见的方法就是F1 Score<br>$$ F1 Score &#x3D; 2 \cdot \frac{Precision \cdot Recall}{Precision + Recall} $$<br>F1 Score是Precision和Recall的调和平均数，用于平衡Precision和Recall。</p><h2 id="ROC曲线和AUC值"><a href="#ROC曲线和AUC值" class="headerlink" title="ROC曲线和AUC值"></a>ROC曲线和AUC值</h2><p>ROC曲线（Receiver Operating Characteristic Curve）和AUC值（Area Under Curve）是评估分类模型性能的重要指标，特别是在二分类问题中。</p><p>ROC曲线绘制的是不同阈值下的真正例率（True Positive Rate, TPR）和假正例率（False Positive Rate, FPR）之间的关系。AUC值则是ROC曲线下面积的大小。</p><p>ROC曲线的横轴是FPR，纵轴是TPR。当阈值越大时，TPR越大，FPR越小。AUC值越大，表示模型的分类性能越好。</p><p>ROC曲线和AUC值可以用于比较不同模型的性能，也可以用于选择最优的阈值。</p><h2 id="混淆矩阵"><a href="#混淆矩阵" class="headerlink" title="混淆矩阵"></a>混淆矩阵</h2><p>混淆矩阵（Confusion Matrix）是评估分类模型性能的一种常用方法，特别是在二分类问题中。</p><h2 id="其他评价指标"><a href="#其他评价指标" class="headerlink" title="其他评价指标"></a>其他评价指标</h2><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://www.cnblogs.com/guoyaohua/p/classification-metrics.html">https://www.cnblogs.com/guoyaohua/p/classification-metrics.html</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>dgl.sampling</title>
    <link href="/p/f47e88c3/"/>
    <url>/p/f47e88c3/</url>
    
    <content type="html"><![CDATA[<h1 id="dgl-sampling"><a href="#dgl-sampling" class="headerlink" title="dgl.sampling"></a>dgl.sampling</h1><p>dgl.sampling 包含用于通过随机游走、邻居采样等从图中采样的运算符和实用程序。它们通常与 dgl.dataloading 包中的 DataLoader 一起使用。</p><h2 id="Random-walk-随机游走"><a href="#Random-walk-随机游走" class="headerlink" title="Random walk 随机游走"></a>Random walk 随机游走</h2><h3 id="random-walk"><a href="#random-walk" class="headerlink" title="random_walk"></a><a href="https://docs.dgl.ai/generated/dgl.sampling.random_walk.html">random_walk</a></h3><p>根据给定的元路径，从起始节点数组生成随机行走轨迹。<br>每个开始节点都将生成一个轨迹，该轨迹<br>    1. 从给定节点开始，将 t 设置为0。<br>    2. 从当前节点沿着边类型 metapath[t] 拾取和遍历。<br>    3. 如果找不到边，则停止。否则，递增 t 并转到步骤2。<br>要为单个节点生成多个轨迹，可以多次指定同一节点。<br>返回的跟踪都具有长度 len(metapath) + 1 ，其中第一个节点是起始节点本身。<br>如果随机游走提前停止，DGL会用-1填充轨迹，使其具有相同的长度。<br>此函数支持GPU上的图形和UVA采样。   </p><p>Usage:<code>dgl.sampling.random_walk(g, nodes, *, metapath=None, length=None, prob=None, restart_prob=None, return_eids=False)</code></p><p>Parameters: </p><ul><li><code>g</code> (DGLGraph) – 输入图。</li><li><code>nodes</code> (tensor) – 起始节点。随机游走轨迹开始的节点ID张量。张量必须与图的ID类型具有相同的dtype。张量必须与图形位于同一设备上，或者在图形固定时位于GPU上（UVA采样）。</li><li><code>metapath</code> (list[str or tuple of str], optional) – 元路径。元路径，指定为边类型的列表。与 length 互斥。如果省略，DGL假设 g 只有一个节点和边类型。在这种情况下，参数 length 指定随机游走轨迹的长度。</li><li><code>length</code> (int, optional) 随机游走的长度。与 metapath 互斥。 仅在 metapath 为“None”时使用。</li><li><code>prob</code> (str, optional) 图上的边特征张量的名称，存储与每条边相关联的（未归一化的）概率，用于选择下一个节点。特征张量必须是非负的，并且所有节点的出站边缘的概率之和必须为正（尽管它们的总和不必为1）。否则结果将是未定义的。特征张量必须与图形位于同一设备上。如果忽略，DGL假定均匀拾取相邻对象。</li><li><code>restart_prob</code> (float or Tensor, optional) 在每次转换之前终止当前跟踪的概率。如果给定了张量，则 restart_prob 应该与图形位于同一设备上，或者在图形固定（UVA采样）时位于GPU上，并且与 metapath 或 length 具有相同的长度。</li><li><code>return_eids</code> (bool, optional) 如果为True，则另外返回遍历的边ID。Default: False.</li></ul><p>Returns:</p><ul><li><code>traces</code>（Tensor）-二维节点ID张量，形状为 (num_seeds, len(metapath) + 1) 或 (num_seeds, length + 1) （如果 metapath 为None）。</li><li><code>eids</code>（Tensor，可选）-二维边ID张量，形状为 (num_seeds, len(metapath)) 或 (num_seeds, length) （如果 metapath 为None）。只有在 return_eids 为True时才返回。</li><li><code>types</code>（Tensor）-一维节点类型ID张量，形状为 (len(metapath) + 1) 或 (length + 1) 。类型ID与原始图 g 中的类型ID匹配。</li></ul><p>Examples:<br>下面的代码创建了一个同构图：&gt; g1 &#x3D; dgl.graph（（[0，1，1，2，3]，[1，2，3，0，0]））<br>Normal random walk: 正态随机游走：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>dgl.sampling.random_walk(g1, [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>], length=<span class="hljs-number">4</span>)<br>(tensor([[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>],<br>         [<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>],<br>         [<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">0</span>],<br>         [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>]]), tensor([<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]))<br></code></pre></td></tr></table></figure><p>或返回边缘ID：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>dgl.sampling.random_walk(g1, [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>], length=<span class="hljs-number">4</span>, return_eids=<span class="hljs-literal">True</span>)<br>(tensor([[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>],<br>         [<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>],<br>         [<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">0</span>],<br>         [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>]]),<br> tensor([[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">0</span>],<br>         [<span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>],<br>         [<span class="hljs-number">3</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>],<br>         [<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">0</span>]]),<br> tensor([<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]))<br></code></pre></td></tr></table></figure><p>第一张量指示每个种子节点的随机行走路径。第二个张量中的第j个元素表示每条路径中第j个节点的节点类型ID。在本例中，它返回全0。   </p><p>重新开始的随机游走：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>dgl.sampling.random_walk_with_restart(g1, [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>], length=<span class="hljs-number">4</span>, restart_prob=<span class="hljs-number">0.5</span>)<br>(tensor([[ <span class="hljs-number">0</span>, -<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>],<br>         [ <span class="hljs-number">1</span>,  <span class="hljs-number">3</span>,  <span class="hljs-number">0</span>, -<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>],<br>         [ <span class="hljs-number">2</span>, -<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>],<br>         [ <span class="hljs-number">0</span>, -<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>]]), tensor([<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]))<br></code></pre></td></tr></table></figure><p>非均匀随机游走：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>g1.edata[<span class="hljs-string">&#x27;p&#x27;</span>] = torch.FloatTensor([<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>])     <span class="hljs-comment"># disallow going from 1 to 2</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>dgl.sampling.random_walk(g1, [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>], length=<span class="hljs-number">4</span>, prob=<span class="hljs-string">&#x27;p&#x27;</span>)<br>(tensor([[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>],<br>         [<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>],<br>         [<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">0</span>],<br>         [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>]]), tensor([<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]))<br></code></pre></td></tr></table></figure><p>基于元路径的随机游走：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>g2 = dgl.heterograph(&#123;<br> ... (<span class="hljs-string">&#x27;user&#x27;</span>, <span class="hljs-string">&#x27;follow&#x27;</span>, <span class="hljs-string">&#x27;user&#x27;</span>): ([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]),<br> ... (<span class="hljs-string">&#x27;user&#x27;</span>, <span class="hljs-string">&#x27;view&#x27;</span>, <span class="hljs-string">&#x27;item&#x27;</span>): ([<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>]),<br> ... (<span class="hljs-string">&#x27;item&#x27;</span>, <span class="hljs-string">&#x27;viewed-by&#x27;</span>, <span class="hljs-string">&#x27;user&#x27;</span>): ([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>])<br><span class="hljs-meta">&gt;&gt;&gt; </span>dgl.sampling.random_walk(<br> ... g2, [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>], metapath=[<span class="hljs-string">&#x27;follow&#x27;</span>, <span class="hljs-string">&#x27;view&#x27;</span>, <span class="hljs-string">&#x27;viewed-by&#x27;</span>] * <span class="hljs-number">2</span>)<br>(tensor([[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>],<br>         [<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>],<br>         [<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],<br>         [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>]]), tensor([<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>]))<br></code></pre></td></tr></table></figure><p>基于元路径的随机游走，仅在项目上重新启动（即在遍历“视图”关系后）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">dgl.sampling.random_walk(<br>    g2, [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>], metapath=[<span class="hljs-string">&#x27;follow&#x27;</span>, <span class="hljs-string">&#x27;view&#x27;</span>, <span class="hljs-string">&#x27;viewed-by&#x27;</span>] * <span class="hljs-number">2</span>,<br>    restart_prob=torch.FloatTensor([<span class="hljs-number">0</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0</span>]))<br>(tensor([[ <span class="hljs-number">0</span>,  <span class="hljs-number">1</span>, -<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>],<br>         [ <span class="hljs-number">1</span>,  <span class="hljs-number">3</span>,  <span class="hljs-number">1</span>,  <span class="hljs-number">0</span>,  <span class="hljs-number">1</span>,  <span class="hljs-number">1</span>,  <span class="hljs-number">0</span>],<br>         [ <span class="hljs-number">2</span>,  <span class="hljs-number">0</span>,  <span class="hljs-number">1</span>,  <span class="hljs-number">1</span>,  <span class="hljs-number">3</span>,  <span class="hljs-number">2</span>,  <span class="hljs-number">2</span>],<br>         [ <span class="hljs-number">0</span>,  <span class="hljs-number">1</span>,  <span class="hljs-number">1</span>,  <span class="hljs-number">3</span>,  <span class="hljs-number">0</span>,  <span class="hljs-number">0</span>,  <span class="hljs-number">0</span>]]), tensor([<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>]))<br></code></pre></td></tr></table></figure><h3 id="node2vec-random-walk"><a href="#node2vec-random-walk" class="headerlink" title="node2vec_random_walk"></a><a href="https://docs.dgl.ai/generated/dgl.sampling.node2vec_random_walk.html">node2vec_random_walk</a></h3><p>基于node 2 vec模型，从起始节点数组生成随机游走轨迹。<br>返回的轨迹都具有长度 walk_length + 1 ，其中第一个节点是起始节点本身。<br>请注意，如果随机游走提前停止，DGL会用-1填充迹线，使其具有相同的长度。   </p><p>Usage:<code>dgl.sampling.node2vec_random_walk(g, nodes, p, q, walk_length, prob=None, return_eids=False)</code></p><p>Parameters:</p><ul><li>g (DGLGraph)  该图必须在CPU上。注意，node 2 vec只支持同构图。</li><li>nodes (Tensor) 随机游走轨迹开始的节点ID张量。张量必须在CPU上，并且必须具有与图的ID类型相同的dtype。</li><li>p (float) 在遍历中立即重新访问节点的可能性。</li><li>q (float) 在广度优先策略和深度优先策略之间进行插值的控制参数。</li><li>walk_length (int) 随机游走的长度。</li><li>prob (str, optional) 图上的边特征张量的名称，存储与每条边相关联的（未归一化的）概率，用于选择下一个节点。特征张量必须是非负的，并且所有节点的出站边缘的概率之和必须为正（尽管它们的总和不必为1）。否则结果将是未定义的。如果忽略，DGL假定均匀拾取相邻对象。</li><li>return_eids (bool, optional) 如果为True，则另外返回遍历的边ID。Default: False.</li></ul><p>Returns:</p><ul><li>traces (Tensor) 形状为 (num_seeds, walk_length + 1) 的二维节点ID张量。</li><li>eids (Tensor, optional) 一个二维边ID张量，形状为 (num_seeds, length) 。只有在 return_eids 为True时才返回。</li></ul><p>Examples:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>g1 = dgl.graph(([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]))<br><span class="hljs-meta">&gt;&gt;&gt; </span>dgl.sampling.node2vec_random_walk(g1, [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>], <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, walk_length=<span class="hljs-number">4</span>)<br>tensor([[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>],<br>        [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>],<br>        [<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">0</span>],<br>        [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>]])<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>dgl.sampling.node2vec_random_walk(g1, [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>], <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, walk_length=<span class="hljs-number">4</span>, return_eids=<span class="hljs-literal">True</span>)<br>(tensor([[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>],<br>         [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>],<br>         [<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>],<br>         [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>]]),<br> tensor([[<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">0</span>],<br>         [<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>],<br>         [<span class="hljs-number">3</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>],<br>         [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">0</span>]]))<br></code></pre></td></tr></table></figure><h3 id="pack-traces"><a href="#pack-traces" class="headerlink" title="pack_traces"></a><a href="https://docs.dgl.ai/generated/dgl.sampling.pack_traces.html">pack_traces</a></h3><h2 id="Neighbor-sampling-邻域抽样法"><a href="#Neighbor-sampling-邻域抽样法" class="headerlink" title="Neighbor sampling 邻域抽样法"></a>Neighbor sampling 邻域抽样法</h2><h2 id="Negative-sampling-负采样"><a href="#Negative-sampling-负采样" class="headerlink" title="Negative sampling 负采样"></a>Negative sampling 负采样</h2>]]></content>
    
    
    <categories>
      
      <category>dgl</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>dgl.function</title>
    <link href="/p/70a6638/"/>
    <url>/p/70a6638/</url>
    
    <content type="html"><![CDATA[<h1 id="dgl-function"><a href="#dgl-function" class="headerlink" title="dgl.function"></a>dgl.function</h1><p><code>dgl.function</code>这个子包包含DGL提供的所有内置函数,用于定义消息传递和聚合的函数。这些函数在图神经网络中用来处理消息传递机制，即从节点及其邻居中收集和聚合信息。</p><h2 id="DGL-Built-in-Function-DGL内置函数"><a href="#DGL-Built-in-Function-DGL内置函数" class="headerlink" title="DGL Built-in Function(DGL内置函数)"></a>DGL Built-in Function(DGL内置函数)</h2><table><thead><tr><th>Category</th><th>Functions</th></tr></thead><tbody><tr><td><strong>Unary message function</strong></td><td><code>copy_u</code>, <code>copy_e</code></td></tr><tr><td></td><td><code>u_add_v</code>, <code>u_sub_v</code>, <code>u_mul_v</code>, <code>u_div_v</code>, <code>u_dot_v</code></td></tr><tr><td></td><td><code>u_add_e</code>, <code>u_sub_e</code>, <code>u_mul_e</code>, <code>u_div_e</code>, <code>u_dot_e</code></td></tr><tr><td></td><td></td></tr><tr><td><strong>Binary message function</strong></td><td><code>v_add_u</code>, <code>v_sub_u</code>, <code>v_mul_u</code>, <code>v_div_u</code>, <code>v_dot_u</code></td></tr><tr><td></td><td><code>v_add_e</code>, <code>v_sub_e</code>, <code>v_mul_e</code>, <code>v_div_e</code>, <code>v_dot_e</code></td></tr><tr><td></td><td><code>e_add_u</code>, <code>e_sub_u</code>, <code>e_mul_u</code>, <code>e_div_u</code>, <code>e_dot_u</code></td></tr><tr><td></td><td><code>e_add_v</code>, <code>e_sub_v</code>, <code>e_mul_v</code>, <code>e_div_v</code>, <code>e_dot_v</code></td></tr><tr><td></td><td></td></tr><tr><td><strong>Reduce function</strong></td><td><code>max</code>, <code>min</code>, <code>sum</code>, <code>mean</code></td></tr></tbody></table><h2 id="Message-functions-消息函数"><a href="#Message-functions-消息函数" class="headerlink" title="Message functions(消息函数)"></a>Message functions(消息函数)</h2><h3 id="dgl-function-copy-u"><a href="#dgl-function-copy-u" class="headerlink" title="dgl.function.copy_u"></a><a href="https://docs.dgl.ai/generated/dgl.function.copy_u.html">dgl.function.copy_u</a></h3><p>内置消息函数，使用源节点特征计算消息。</p><p>Usage: <code>dgl.function.copy_u(u, out)</code></p><p>Parameters:</p><ul><li>u(str) 源节点特征字段</li><li>out(str) 输出的消息字段</li></ul><p>Example:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> dgl<br>message_func = dgl.function.copy_u(<span class="hljs-string">&#x27;h&#x27;</span>, <span class="hljs-string">&#x27;m&#x27;</span>)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">message_func</span>(<span class="hljs-params">edges</span>):<br>    <span class="hljs-keyword">return</span> &#123;<span class="hljs-string">&#x27;m&#x27;</span>: edges.src[<span class="hljs-string">&#x27;h&#x27;</span>]&#125;<br></code></pre></td></tr></table></figure><h3 id="dgl-function-copy-e"><a href="#dgl-function-copy-e" class="headerlink" title="dgl.function.copy_e"></a><a href="https://docs.dgl.ai/generated/dgl.function.copy_e.html">dgl.function.copy_e</a></h3><p>内置消息函数，使用边特征计算消息。</p><p>Usage: <code>dgl.function.copy_e(e, out)</code></p><p>Parameters:</p><ul><li>e(str) 边特征字段</li><li>out(str) 输出的消息字段</li></ul><p>Example:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> dgl<br>message_func = dgl.function.copy_e(<span class="hljs-string">&#x27;h&#x27;</span>, <span class="hljs-string">&#x27;m&#x27;</span>)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">message_func</span>(<span class="hljs-params">edges</span>):<br>    <span class="hljs-keyword">return</span> &#123;<span class="hljs-string">&#x27;m&#x27;</span>: edges.data[<span class="hljs-string">&#x27;h&#x27;</span>]&#125;<br></code></pre></td></tr></table></figure><h3 id="dgl-function-u-add-v"><a href="#dgl-function-u-add-v" class="headerlink" title="dgl.function.u_add_v"></a><a href="https://docs.dgl.ai/generated/dgl.function.u_add_v.html">dgl.function.u_add_v</a></h3><p>内置的消息函数，如果特征具有相同的形状，则通过在u和v的特征之间执行逐元素相加来计算边缘上的消息;<br>否则，它首先将特征广播到新的形状并执行逐元素操作。</p><p>Usage: <code>dgl.function.u_add_v(lhs_field, rhs_field, out)</code></p><p>Parameters:</p><ul><li>lhs_field(str) u的特征字段</li><li>rhs_field(str) v的特征字段</li><li>out(str) 输出的消息字段</li></ul><p>Example:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> dgl<br>message_func = dgl.function.u_add_v(<span class="hljs-string">&#x27;h&#x27;</span>, <span class="hljs-string">&#x27;h&#x27;</span>, <span class="hljs-string">&#x27;m&#x27;</span>)<br></code></pre></td></tr></table></figure><h3 id="dgl-function-u-sub-v"><a href="#dgl-function-u-sub-v" class="headerlink" title="dgl.function.u_sub_v"></a><a href="https://docs.dgl.ai/generated/dgl.function.u_sub_v.html">dgl.function.u_sub_v</a></h3><p>内置的消息函数，如果特征具有相同的形状，则通过在u和v的特征之间执行逐元素相减来计算边缘上的消息;<br>否则，它首先将特征广播到新的形状并执行逐元素操作。</p><p>Usage: <code>dgl.function.u_sub_v(lhs_field, rhs_field, out)</code></p><p>Parameters:</p><ul><li>lhs_field(str) u的特征字段</li><li>rhs_field(str) v的特征字段</li><li>out(str) 输出的消息字段</li></ul><p>Example:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> dgl<br>message_func = dgl.function.u_sub_v(<span class="hljs-string">&#x27;h&#x27;</span>, <span class="hljs-string">&#x27;h&#x27;</span>, <span class="hljs-string">&#x27;m&#x27;</span>)<br></code></pre></td></tr></table></figure><h3 id="dgl-function-u-mul-v"><a href="#dgl-function-u-mul-v" class="headerlink" title="dgl.function.u_mul_v"></a><a href="https://docs.dgl.ai/generated/dgl.function.u_mul_v.html">dgl.function.u_mul_v</a></h3><p>内置的消息函数，如果特征具有相同的形状，则通过在u和v的特征之间执行逐元素相乘来计算边缘上的消息;<br>否则，它首先将特征广播到新的形状并执行逐元素操作。</p><p>Usage: <code>dgl.function.u_mul_v(lhs_field, rhs_field, out)</code></p><p>Parameters:</p><ul><li>lhs_field(str) u的特征字段</li><li>rhs_field(str) v的特征字段</li><li>out(str) 输出的消息字段</li></ul><p>Example:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> dgl<br>message_func = dgl.function.u_mul_v(<span class="hljs-string">&#x27;h&#x27;</span>, <span class="hljs-string">&#x27;h&#x27;</span>, <span class="hljs-string">&#x27;m&#x27;</span>)<br></code></pre></td></tr></table></figure><h3 id="dgl-function-u-div-v"><a href="#dgl-function-u-div-v" class="headerlink" title="dgl.function.u_div_v"></a><a href="https://docs.dgl.ai/generated/dgl.function.u_div_v.html">dgl.function.u_div_v</a></h3><p>内置的消息函数，如果特征具有相同的形状，则通过在u和v的特征之间执行逐元素相除来计算边缘上的消息;<br>否则，它首先将特征广播到新的形状并执行逐元素操作。</p><p>Usage: <code>dgl.function.u_div_v(lhs_field, rhs_field, out)</code></p><p>Parameters:</p><ul><li>lhs_field(str) u的特征字段</li><li>rhs_field(str) v的特征字段</li><li>out(str) 输出的消息字段</li></ul><p>Example:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> dgl<br>message_func = dgl.function.u_div_v(<span class="hljs-string">&#x27;h&#x27;</span>, <span class="hljs-string">&#x27;h&#x27;</span>, <span class="hljs-string">&#x27;m&#x27;</span>)<br></code></pre></td></tr></table></figure><h2 id="Reduce-functions-规约函数"><a href="#Reduce-functions-规约函数" class="headerlink" title="Reduce functions(规约函数)"></a>Reduce functions(规约函数)</h2><h3 id="dgl-function-sum"><a href="#dgl-function-sum" class="headerlink" title="dgl.function.sum"></a><a href="https://docs.dgl.ai/generated/dgl.function.sum.html">dgl.function.sum</a></h3><p>内置reduce函数，按总和聚合消息   </p><p>Usage: <code>dgl.function.sum(msg, out)</code>  </p><p>Parameters:</p><ul><li>msg(str) 消息字段</li><li>out(str) 输出的节点特征字段</li></ul><p>Example:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> dgl<br>reduce_func = dgl.function.<span class="hljs-built_in">sum</span>(<span class="hljs-string">&#x27;m&#x27;</span>, <span class="hljs-string">&#x27;h&#x27;</span>)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">reduce_func</span>(<span class="hljs-params">nodes</span>):<br>    <span class="hljs-keyword">return</span> &#123;<span class="hljs-string">&#x27;h&#x27;</span>: torch.<span class="hljs-built_in">sum</span>(nodes.mailbox[<span class="hljs-string">&#x27;m&#x27;</span>], dim=<span class="hljs-number">1</span>)&#125;<br></code></pre></td></tr></table></figure><h3 id="dgl-function-max"><a href="#dgl-function-max" class="headerlink" title="dgl.function.max"></a><a href="https://docs.dgl.ai/generated/dgl.function.max.html">dgl.function.max</a></h3><p>内置reduce函数，以最大值聚合消息。</p><p>Usage: <code>dgl.function.max(msg, out)</code>  </p><p>Parameters:</p><ul><li>msg(str) 消息字段</li><li>out(str) 输出的节点特征字段</li></ul><p>Example:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> dgl<br>reduce_func = dgl.function.<span class="hljs-built_in">max</span>(<span class="hljs-string">&#x27;m&#x27;</span>, <span class="hljs-string">&#x27;h&#x27;</span>)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">reduce_func</span>(<span class="hljs-params">nodes</span>):<br>    <span class="hljs-keyword">return</span> &#123;<span class="hljs-string">&#x27;h&#x27;</span>: torch.<span class="hljs-built_in">max</span>(nodes.mailbox[<span class="hljs-string">&#x27;m&#x27;</span>], dim=<span class="hljs-number">1</span>)&#125;<br></code></pre></td></tr></table></figure><h3 id="dgl-function-min"><a href="#dgl-function-min" class="headerlink" title="dgl.function.min"></a><a href="https://docs.dgl.ai/generated/dgl.function.min.html">dgl.function.min</a></h3><p>  内置reduce函数，以最小值聚合消息。   </p><p>Usage: <code>dgl.function.min(msg, out)</code>  </p><p>Parameters:</p><ul><li>msg(str) 消息字段</li><li>out(str) 输出的节点特征字段</li></ul><p>Example:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> dgl<br>reduce_func = dgl.function.<span class="hljs-built_in">min</span>(<span class="hljs-string">&#x27;m&#x27;</span>, <span class="hljs-string">&#x27;h&#x27;</span>)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">reduce_func</span>(<span class="hljs-params">nodes</span>):<br>    <span class="hljs-keyword">return</span> &#123;<span class="hljs-string">&#x27;h&#x27;</span>: torch.<span class="hljs-built_in">min</span>(nodes.mailbox[<span class="hljs-string">&#x27;m&#x27;</span>], dim=<span class="hljs-number">1</span>)&#125;<br></code></pre></td></tr></table></figure><h3 id="dgl-function-mean"><a href="#dgl-function-mean" class="headerlink" title="dgl.function.mean"></a><a href="https://docs.dgl.ai/generated/dgl.function.mean.html">dgl.function.mean</a></h3><p>内置reduce函数，以平均值聚合消息。</p><p>Usage: <code>dgl.function.mean(msg, out)</code>  </p><p>Parameters:</p><ul><li>msg(str) 消息字段</li><li>out(str) 输出的节点特征字段</li></ul><p>Example:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> dgl<br>reduce_func = dgl.function.mean(<span class="hljs-string">&#x27;m&#x27;</span>, <span class="hljs-string">&#x27;h&#x27;</span>)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">reduce_func</span>(<span class="hljs-params">nodes</span>):<br>    <span class="hljs-keyword">return</span> &#123;<span class="hljs-string">&#x27;h&#x27;</span>: torch.mean(nodes.mailbox[<span class="hljs-string">&#x27;m&#x27;</span>], dim=<span class="hljs-number">1</span>)&#125;<br></code></pre></td></tr></table></figure><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="https://docs.dgl.ai/index.html">https://docs.dgl.ai/index.html</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>dgl</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>师兄教诲</title>
    <link href="/p/1ee8e810/"/>
    <url>/p/1ee8e810/</url>
    
    <content type="html"><![CDATA[<p>勤读文献，善于思考，主动担当，做好规划</p><p>多学习，多留心，搞好细节</p>]]></content>
    
    
    <categories>
      
      <category>经验与心得</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>如何修改macos命令行终端的默认提示符</title>
    <link href="/p/ab4c78df/"/>
    <url>/p/ab4c78df/</url>
    
    <content type="html"><![CDATA[<h2 id="命令行终端的提示符是什么玩意？"><a href="#命令行终端的提示符是什么玩意？" class="headerlink" title="命令行终端的提示符是什么玩意？"></a>命令行终端的提示符是什么玩意？</h2><p>在命令行终端中，前缀显示的内容通常称为“提示符”（Prompt）。更具体地，它通常是指命令行界面中的命令提示符，这是一种用户界面元素，用于指示系统已准备好接收用户输入的命令。</p><p>在 Unix 和类 Unix 系统（如 Linux 和 macOS）的 shell 程序中，这个提示符是通过特定的环境变量定义的，最常见的是 <code>PS1</code>（Prompt Statement 1）。通过自定义 <code>PS1</code> 变量，用户可以更改提示符的外观和提供的信息，例如显示当前工作目录、用户名、主机名等。这些自定义通常在用户的 shell 配置文件中设置（如 Bash 的 <code>.bashrc</code> 或 Zsh 的 <code>.zshrc</code> 文件）。</p><h2 id="如何修改macos命令行终端的默认提示符"><a href="#如何修改macos命令行终端的默认提示符" class="headerlink" title="如何修改macos命令行终端的默认提示符"></a>如何修改macos命令行终端的默认提示符</h2><p>MacBook 使用的是 macOS 操作系统，其默认的命令行界面（终端）使用的是 Bash 或 Zsh 作为其默认的 shell。在 macOS 中，命令行提示符默认设置为显示用户名和主机名，而不是当前工作目录的路径。</p><p>如果您想让 MacBook 的命令行提示符显示当前路径，像 Linux 那样，您可以通过修改 shell 的配置文件来实现。对于 Bash，这通常是 <code>.bash_profile</code> 或 <code>.bashrc</code> 文件；对于 Zsh，这是 <code>.zshrc</code> 文件。这些文件位于用户的主目录下。</p><p>例如，如果您使用的是 Bash，您可以通过以下步骤来更改提示符：</p><ol><li>打开终端。</li><li>使用文本编辑器打开 <code>.bash_profile</code> 或 <code>.bashrc</code> 文件，例如通过命令 <code>nano ~/.bash_profile</code>，或者vscode编辑器等打开<code>.bashrc</code>或<code>.bash_profile</code>或<code>.zshrc</code>文件。</li><li>在文件中添加或修改 <code>PS1</code> 变量。例如，您可以添加如下行来显示当前路径： <code>export PS1=&quot;\w \$ &quot;</code>。</li><li>保存并关闭文件。</li><li>重新加载配置文件，或关闭并重新打开终端,或者<code>source ~/.zshrc</code>.</li></ol><h3 id="将提示符设置为当前工作目录的绝对路径"><a href="#将提示符设置为当前工作目录的绝对路径" class="headerlink" title="将提示符设置为当前工作目录的绝对路径"></a>将提示符设置为当前工作目录的绝对路径</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-built_in">export</span> <span class="hljs-attribute">PS1</span>=<span class="hljs-string">&quot;%d\$ &quot;</span><br></code></pre></td></tr></table></figure><img src="/p/undefined/%E4%BF%AE%E6%94%B9%E5%90%8E%E7%9A%84%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%8F%90%E7%A4%BA%E7%AC%A6.png" class=""><h3 id="将提示符设置为当前工作目录的相对路径"><a href="#将提示符设置为当前工作目录的相对路径" class="headerlink" title="将提示符设置为当前工作目录的相对路径"></a>将提示符设置为当前工作目录的相对路径</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-built_in">export</span> <span class="hljs-attribute">PS1</span>=<span class="hljs-string">&quot;%~\$ &quot;</span><br></code></pre></td></tr></table></figure><img src="/p/undefined/%E7%9B%B8%E5%AF%B9%E8%B7%AF%E5%BE%84.png" class=""><h3 id="恢复为原来的默认提示符"><a href="#恢复为原来的默认提示符" class="headerlink" title="恢复为原来的默认提示符"></a>恢复为原来的默认提示符</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-built_in">export</span> <span class="hljs-attribute">PS1</span>=<span class="hljs-string">&quot;%n@%m %1~ %# &quot;</span><br></code></pre></td></tr></table></figure><img src="/p/undefined/%E9%BB%98%E8%AE%A4%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%8F%90%E7%A4%BA%E7%AC%A6.png" class=""><p>注意修改完文件之后需要<code>source ~/.zshrc</code>是你的修改生效。你的终端用的是哪种shell,你就需要去修改对应的文件，然后让它生效。从上面的图片中可以看到，这里我的终端中最顶部显示的是-zsh，所以我就要去修改.zshrc文件,如果你是bash，就修改.bashrc文件。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>二叉堆</title>
    <link href="/p/69c852d4/"/>
    <url>/p/69c852d4/</url>
    
    <content type="html"><![CDATA[<h3 id="二叉堆"><a href="#二叉堆" class="headerlink" title="二叉堆"></a>二叉堆</h3><p>二叉堆<br>二叉堆的逻辑结构是完全二叉树,实际上是由1为起点的一维数组表示。<br>给定一个节点的下标<code>i</code>,其父节点<code>parent(i)=1/2</code>、左子节点<code>left(i)=2*i</code>、右子节点<code>right(i)=2*i+1</code>。<br>最大堆性质：节点的键值小于等于其父节点的键值。最大堆堆根中存储着最大的值。<br>最小堆性质：节点的键值大于等于其父节点的键值。最小堆的根中存储着最小的值。</p><h4 id="1-Complete-Binary-Tree"><a href="#1-Complete-Binary-Tree" class="headerlink" title="1.Complete Binary Tree"></a>1.Complete Binary Tree</h4><p>读取以完全二叉树形式表示的二叉堆，并按照下面的格式输出二叉堆各节点信息<br>node id: key &#x3D; k, parent key &#x3D; pk, left key &#x3D; lk, right key &#x3D; rk<br>输入：第一行堆的大小H。接下来一行，按节点编号顺序输入代表二叉堆节点值的H个整数<br>输出：按节点编号顺序（1～H）输出代表二叉堆各节点的信息<br>（挑战程序设计竞赛2算法与数据结构191页例题）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">二叉堆</span><br><span class="hljs-string">二叉堆的逻辑结构是完全二叉树,实际上是由1为起点的一维数组表示。</span><br><span class="hljs-string">给定一个节点的下标`i`,其父节点`parent(i)=1/2`、左子节点`left(i)=2*i`、右子节点`right(i)=2*i+1`。</span><br><span class="hljs-string">最大堆性质：节点的键值小于等于其父节点的键值。最大堆堆根中存储着最大的值。</span><br><span class="hljs-string">最小堆性质：节点的键值大于等于其父节点的键值。最小堆的根中存储着最小的值。</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">parent</span>(<span class="hljs-params">i</span>):<br>    <span class="hljs-keyword">return</span> i//<span class="hljs-number">2</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">left</span>(<span class="hljs-params">i</span>):<br>    <span class="hljs-keyword">return</span> i*<span class="hljs-number">2</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">right</span>(<span class="hljs-params">i</span>):<br>    <span class="hljs-keyword">return</span> i*<span class="hljs-number">2</span>+<span class="hljs-number">1</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">solve</span>():<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    5                             </span><br><span class="hljs-string">    7 8 1 2 3</span><br><span class="hljs-string">    node 1: key = 7,  left key = 8,  right key = 1, </span><br><span class="hljs-string">    node 2: key = 8,  parent key = 7,  left key = 2,  right key = 3, </span><br><span class="hljs-string">    node 3: key = 1,  parent key = 7,  </span><br><span class="hljs-string">    node 4: key = 2,  parent key = 8,  </span><br><span class="hljs-string">    node 5: key = 3,  parent key = 8, </span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    H = <span class="hljs-built_in">int</span>(<span class="hljs-built_in">input</span>())<br>    array = [<span class="hljs-number">0</span>]<br>    array.extend(<span class="hljs-built_in">list</span>(<span class="hljs-built_in">map</span>(<span class="hljs-built_in">int</span>,<span class="hljs-built_in">input</span>().split())))<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,H+<span class="hljs-number">1</span>):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;node &#123;&#125;: key = &#123;&#125;, &#x27;</span>.<span class="hljs-built_in">format</span>(i,array[i]),end=<span class="hljs-string">&#x27; &#x27;</span>)<br>        <span class="hljs-keyword">if</span> parent(i) &gt;= <span class="hljs-number">1</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;parent key = &#123;&#125;, &#x27;</span>.<span class="hljs-built_in">format</span>(array[parent(i)]),end=<span class="hljs-string">&#x27; &#x27;</span>)<br>        <span class="hljs-keyword">if</span> left(i) &lt;= H:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;left key = &#123;&#125;, &#x27;</span>.<span class="hljs-built_in">format</span>(array[left(i)]),end=<span class="hljs-string">&#x27; &#x27;</span>)<br>        <span class="hljs-keyword">if</span> right(i) &lt;= H:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;right key = &#123;&#125;, &#x27;</span>.<span class="hljs-built_in">format</span>(array[right(i)]),end=<span class="hljs-string">&#x27;&#x27;</span>) <br>        <span class="hljs-built_in">print</span>()<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    solve()<br></code></pre></td></tr></table></figure><h4 id="2-Maximum-Heap"><a href="#2-Maximum-Heap" class="headerlink" title="2.Maximum Heap"></a>2.Maximum Heap</h4><p>使用给定数组生成最大堆。复杂度O(H)<br>输入：第一行堆的大小H。接下来一行，按节点编号顺序输入代表二叉堆节点值的H个整数<br>输出：按节点编号顺序（1～H）输出代表二叉堆各节点的值<br>（挑战程序设计竞赛2算法与数据结构193页例题）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">10</span><br><span class="hljs-string">4 1 3 2 16 9 10 14 8 7  </span><br><span class="hljs-string"></span><br><span class="hljs-string">16, 14, 10, 8, 7, 9, 3, 2, 4, 1</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">left</span>(<span class="hljs-params">i</span>):<br>    <span class="hljs-keyword">return</span> i*<span class="hljs-number">2</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">right</span>(<span class="hljs-params">i</span>):<br>    <span class="hljs-keyword">return</span> i*<span class="hljs-number">2</span>+<span class="hljs-number">1</span><br><br>H = <span class="hljs-number">0</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">maxHeapify</span>(<span class="hljs-params">A,i</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    用于从根节点i向叶节点方向寻找A[i]值的恰当位置,从而使以i为根节点的子树成为最大堆。这里我们假设堆的大小为H</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    l = left(i)<br>    r = right(i)<br>    largest = <span class="hljs-literal">None</span><br>    <span class="hljs-keyword">if</span> l &lt;= H <span class="hljs-keyword">and</span> A[l] &gt; A[i]:<br>        largest = l<br>    <span class="hljs-keyword">else</span>:<br>        largest = i<br><br>    <span class="hljs-keyword">if</span> r &lt;= H <span class="hljs-keyword">and</span> A[r] &gt; A[largest]:<br>        largest = r<br>    <br>    <span class="hljs-keyword">if</span> largest != i :<br>        A[i],A[largest] = A[largest],A[i]<br>        maxHeapify(A,largest) <span class="hljs-comment"># 递归调用</span><br>    <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">buildMaxHeap</span>(<span class="hljs-params">A</span>):<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(H//<span class="hljs-number">2</span>,<span class="hljs-number">0</span>,-<span class="hljs-number">1</span>):<br>        <span class="hljs-comment"># print(&#x27;i=&#x27;,i)</span><br>        maxHeapify(A,i)<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    H = <span class="hljs-built_in">int</span>(<span class="hljs-built_in">input</span>())<br>    A = [<span class="hljs-number">0</span>]<br>    A.extend(<span class="hljs-built_in">list</span>(<span class="hljs-built_in">map</span>(<span class="hljs-built_in">int</span>,<span class="hljs-built_in">input</span>().split())))<br>    buildMaxHeap(A)<br>    <span class="hljs-built_in">print</span>(A)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>数据结构和算法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>设置SSH秘钥免密码登陆服务器</title>
    <link href="/p/965bfc0a/"/>
    <url>/p/965bfc0a/</url>
    
    <content type="html"><![CDATA[<p>当你需要设置SSH密钥以进行远程访问时，以下是从生成SSH密钥对到将公钥添加到远程服务器的完整步骤，以及每一步的解释：</p><h3 id="1-生成SSH密钥对"><a href="#1-生成SSH密钥对" class="headerlink" title="1. 生成SSH密钥对"></a>1. 生成SSH密钥对</h3><ol><li>打开终端（Linux或Mac）或使用Git Bash（Windows）。</li><li>在终端中运行以下命令来生成SSH密钥对： <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ssh-keygen -t rsa -b 4096 -C <span class="hljs-string">&quot;your_email@example.com&quot;</span><br></code></pre></td></tr></table></figure><ul><li><code>ssh-keygen</code>: 生成SSH密钥对的命令。</li><li><code>-t rsa</code>: 指定生成RSA类型的密钥。</li><li><code>-b 4096</code>: 指定密钥长度为4096位，这是一种更安全的长度。</li><li><code>-C &quot;your_email@example.com&quot;</code>: 提供一个注释，一般是你的电子邮件地址。</li></ul></li><li>按照提示，选择密钥文件的保存路径和设置密码（可选）。<br> 这里一直enter就可以。</li></ol><h3 id="2-将公钥添加到远程服务器"><a href="#2-将公钥添加到远程服务器" class="headerlink" title="2. 将公钥添加到远程服务器"></a>2. 将公钥添加到远程服务器</h3><ol><li>使用以下命令将公钥复制到远程服务器（确保将 <code>your_email@example.com</code> 替换为你的电子邮件地址）： <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">ssh-copy-id username@remote_host<br></code></pre></td></tr></table></figure><ul><li><code>ssh-copy-id</code>: 一个用于将本地公钥复制到远程主机的命令。</li><li><code>username</code>: 远程服务器上你要登录的用户名。</li><li><code>remote_host</code>: 目标服务器的IP地址或域名。<br> 这里第一次使用ssh-copy-id username@remote_host，需要输入username@remote_host的密码。</li></ul></li><li>如果 <code>ssh-copy-id</code> 命令不可用，你可以手动复制公钥内容并登录到远程服务器，然后将其添加到 <code>~/.ssh/authorized_keys</code> 文件中： <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">mkdir</span> -p ~/.ssh &amp;&amp; <span class="hljs-built_in">chmod</span> 700 ~/.ssh &amp;&amp; <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;PASTE_YOUR_PUBLIC_KEY_HERE&quot;</span> &gt;&gt; ~/.ssh/authorized_keys &amp;&amp; <span class="hljs-built_in">chmod</span> 600 ~/.ssh/authorized_keys<br></code></pre></td></tr></table></figure><ul><li><code>PASTE_YOUR_PUBLIC_KEY_HERE</code>: 将你的公钥内容粘贴到这里。</li></ul></li></ol><h3 id="3-SSH连接"><a href="#3-SSH连接" class="headerlink" title="3. SSH连接"></a>3. SSH连接</h3><ol><li>使用以下命令通过SSH连接到远程服务器：<pre><code class="bash">ssh username@remote_host</code></pre><ul><li><code>username</code>: 远程服务器上你的用户名。</li><li><code>remote_host</code>: 远程服务器的IP地址或域名。</li></ul></li><li>如果设置了私钥密码，系统会要求输入私钥密码。如果一切设置正确，你将能够无需密码直接连接到远程服务器。</li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>SSH</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>如何在Ubuntu上安装Pycharm</title>
    <link href="/p/8c9f7322/"/>
    <url>/p/8c9f7322/</url>
    
    <content type="html"><![CDATA[<h2 id="在Ubuntu上安装Pycharm"><a href="#在Ubuntu上安装Pycharm" class="headerlink" title="在Ubuntu上安装Pycharm"></a>在Ubuntu上安装Pycharm</h2><h3 id="1-在官网下载安装包"><a href="#1-在官网下载安装包" class="headerlink" title="1.在官网下载安装包"></a>1.在官网下载安装包</h3><p>下载安装包(.tar.gz)文件，然后解压到你需要去保存到位置。<br><a href="https://www.jetbrains.com/pycharm/download/?section=linux">Pycharm的官网的网址</a></p><h3 id="2-解压安装"><a href="#2-解压安装" class="headerlink" title="2.解压安装"></a>2.解压安装</h3><p>tar -zxvf pycharm-professional-2023.2.2.tar.gz -C &#x2F;home&#x2F;jiqing&#x2F;application(安装的位置….)<br><img src="https://raw.githubusercontent.com/JiqingJiang/erode_Picgo_repo/main/img/%E5%A6%82%E4%BD%95%E5%9C%A8Ubuntu%E4%B8%8A%E5%AE%89%E8%A3%85Pycharm1.png"><br>打开解压好的文件pycharm-professional-2023.2.2然后在bin目录下，在终端运行 <code>./pycharm.sh</code>，pycharm正常运行打开。<br><img src="https://raw.githubusercontent.com/JiqingJiang/erode_Picgo_repo/main/img/20240820001244.png"></p><h3 id="3-设置环境变量启动"><a href="#3-设置环境变量启动" class="headerlink" title="3.设置环境变量启动"></a>3.设置环境变量启动</h3><p>我们在用户根目录（也就是家目录～下），显示隐藏的文件，然后打开.profile文件并在底部添加一行代码</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">export PATH=~/application/pycharm-2023.2.2/bin:$PATH<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/JiqingJiang/erode_Picgo_repo/main/img/20240820001410.png"><br><img src="https://raw.githubusercontent.com/JiqingJiang/erode_Picgo_repo/main/img/20240820001445.png"></p><p>打开终端，使用 在任意目录中输入pycharm.sh来启动pycharm<br>这里我从desktop在命令行中用 <code>pycharm.sh</code>启动pycharm<br>source ~&#x2F;.profile使环境变量生效<br><img src="https://raw.githubusercontent.com/JiqingJiang/erode_Picgo_repo/main/img/20240820001522.png"></p><h3 id="4-添加至系统应用程序"><a href="#4-添加至系统应用程序" class="headerlink" title="4.添加至系统应用程序"></a>4.添加至系统应用程序</h3><p>在.local&#x2F;share&#x2F;application目录中创建名称为pycharm.desktop的文件，然后在文件内键入内容：</p><figure class="highlight abnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs abnf">[Desktop Entry]   <br><span class="hljs-attribute">Type</span><span class="hljs-operator">=</span>Application <br><span class="hljs-attribute">Name</span><span class="hljs-operator">=</span>pycharm-<span class="hljs-number">2023.2</span>.<span class="hljs-number">2</span><br><span class="hljs-attribute">Comment</span><span class="hljs-operator">=</span>Intelligent Python IDE <br><span class="hljs-attribute">Exec</span><span class="hljs-operator">=/</span>home/jiqing/application/pycharm-<span class="hljs-number">2023.2</span>.<span class="hljs-number">3</span>/bin/pycharm.sh %f<br><span class="hljs-attribute">Icon</span><span class="hljs-operator">=/</span>home/jiqing/application/pycharm-<span class="hljs-number">2023.2</span>.<span class="hljs-number">3</span>/bin/pycharm.png<br><span class="hljs-attribute">Categories</span><span class="hljs-operator">=</span>Development<span class="hljs-comment">;IDE;</span><br><span class="hljs-attribute">Terminal</span><span class="hljs-operator">=</span>false<br>Startup Notify<span class="hljs-operator">=</span>true<br><span class="hljs-attribute">StartupWMClass</span><span class="hljs-operator">=</span>pycharm-<span class="hljs-number">2023.2</span>.<span class="hljs-number">2</span><br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/JiqingJiang/erode_Picgo_repo/main/img/20240820001609.png"><br><img src="https://raw.githubusercontent.com/JiqingJiang/erode_Picgo_repo/main/img/20240820001701.png"></p><p>至此，你可以在ubuntu的启动台找到Pycharm的图标，然后点击启动。<br><img src="https://raw.githubusercontent.com/JiqingJiang/erode_Picgo_repo/main/img/20240820001744.png"><br>其他的参考链接：<a href="http://t.csdnimg.cn/4lsM0">http://t.csdnimg.cn/4lsM0</a></p>]]></content>
    
    
    <categories>
      
      <category>安装软件</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>前路漫漫亦灿灿</title>
    <link href="/p/f31adf19/"/>
    <url>/p/f31adf19/</url>
    
    <content type="html"><![CDATA[<p><strong>永远都不要提前焦虑，也不要预知烦恼，更不要被当下的情绪所消耗。</strong><br><strong>你只需好好享受当下就好，车到山前必有路，关关难过关关过。</strong><br><strong>也不要揪着过去不放，不要过度思考，不依不饶就是画地为牢。</strong><br><strong>你要允许一切发生，更要调整好情绪，毕竟我们无法左右的事情太多了，握不住的东西和留不住的人也太多了。</strong><br><strong>但你要知道，发生的一切固有它的道理，尽心尽力后顺其自然就好了。</strong><br><strong>所以就不要把时间浪费在尚未发生且没有必要的人和事上了，更不要在焦虑和烦恼中度过了。</strong><br><strong>当你开始做时间的主人，那些焦虑和烦恼自然都烟消云散了。</strong><br><strong>慢慢即漫漫，漫漫亦灿灿</strong></p>]]></content>
    
    
    <categories>
      
      <category>心情日记</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>宽恕自己</title>
    <link href="/p/7b61a2cc/"/>
    <url>/p/7b61a2cc/</url>
    
    <content type="html"><![CDATA[<p><strong>钝感力+屏蔽力&#x3D;人生无敌</strong>              </p><p><strong>关我屁事+关你屁事&#x3D;岁月静好</strong></p>]]></content>
    
    
    <categories>
      
      <category>心情日记</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>诗人激情</title>
    <link href="/p/4f97a555/"/>
    <url>/p/4f97a555/</url>
    
    <content type="html"><![CDATA[<h3 id="2022-09-28-计算机组成原理课程有感"><a href="#2022-09-28-计算机组成原理课程有感" class="headerlink" title="2022.09.28 计算机组成原理课程有感"></a>2022.09.28 计算机组成原理课程有感</h3><p>看似三门课，周周是期末。<br>痛并快乐着，摆烂东山便，再也不能起。<br>一花一世界，一叶一菩提。<br>一周一单元，每周都难熬。<br>周一一考试，秒变小学生。<br>换汤不换药，依旧不会做。<br>依旧做不完，刚想来复习，又是一轮回。<br>心态不敢崩，佛不渡菜狗。<br>过了这学期，会迎来天明。  </p><h3 id="2023-08-29-计算机组成原理助教"><a href="#2023-08-29-计算机组成原理助教" class="headerlink" title="2023.08.29 计算机组成原理助教"></a>2023.08.29 计算机组成原理助教</h3><p>又是一学期，又是那门课。<br>回想一年前，笑看下一届。<br>本就是菜狗，竟要成人师。  </p><p>哈哈哈哈哈，缘分奇妙啊，下一级能不能学会我不知道，反正这一遍过去我倒是会了。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>光芒</title>
    <link href="/p/6680ee70/"/>
    <url>/p/6680ee70/</url>
    
    <content type="html"><![CDATA[<p><img src="https://raw.githubusercontent.com/JiqingJiang/erode_Picgo_repo/main/2023/%E4%B8%80%E6%9D%9F%E5%85%89.jpg"></p><video id="video" controls="true" controlslist="nodownload" preload="true" allowfullscreen="true" position= "absolute" width="100%" poster="封面的URL">    <source id="mp4" src="https://raw.githubusercontent.com/JiqingJiang/erode_Picgo_repo/main/2023/douyin.mp4" type="video/mp4" ></video>]]></content>
    
    
    <categories>
      
      <category>心情日记</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>感悟与收获</title>
    <link href="/p/d9415bdf/"/>
    <url>/p/d9415bdf/</url>
    
    <content type="html"><![CDATA[<ol><li>心理素质</li><li>见一个人，多一条路</li><li>这个世界是靠你兜里的钱来排名的</li><li>人类之所以有进步，是因为下一代不听上一代的话</li><li>静能生慧</li><li>从赚钱变值钱</li><li>永远不要算计未来可能帮你的人</li><li>你们正处于你人生最容易懈怠的时候</li><li>言多必失</li><li>不要教育你的孩子如何致富，要教育你的孩子如何幸福快乐</li><li>人生无憾，只要你愿意为爱而战</li><li>面对未来，请用好奇代替恐惧</li><li>朝着一个既定的方向努力，就算没有天赋，在时间的积累下，也能稍稍有点成就</li><li>择一业谋食养命，等一运扭转乾坤</li><li>不惧风雨的同路人</li><li><strong>What doesn’t kill you will make you stronger !</strong></li><li>与自己同行的人越来越少，感到孤单是必然的。<strong>选择了就别后悔</strong>。停止焦虑，尽自己所能，因为你所能做的也是只有努力。很残酷但很真实的是，最后能帮助自己的只有你自己。</li><li>失败是个好事，失败教会了我们如何去成功。一个人要想成事就必须拿的起放得下。</li><li>如果能像坐牢一样去完成一件事，那么你必然有很大的突破。</li><li>在校园，一切都有参考答案；在社会，永远没有参考答案。</li><li>无论干什么都有机会，对于普通人来说，或许会有选择，工作，行业，有赚钱容易的，有赚钱难的，对于资本家，顶尖人物，眼里看到的是各种机会，无论干什么，投入的精力久，都会成为这方面的专家。</li><li>冰冻三尺，非一日之寒</li><li>小修在深山，大修在世间</li><li>人生其实没有走错路的地方，你只是角度不一样，你没必要也没有理由瞧不起人家在某条路上走的很远，恰好相反，我就会很好奇，是什么会支撑你能够走这么远。</li><li>千万不要想着我做不出来然后就拖着，慢慢摸索早晚会有进展。    </li><li>轻舟已过万重山   </li><li>天欲福人，必先以祸敬之</li><li>你所担心的事情96%不会发生</li></ol>]]></content>
    
    
    <categories>
      
      <category>所思所想</category>
      
    </categories>
    
    
    <tags>
      
      <tag>便签</tag>
      
      <tag>随心记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Matplotlib Pyplot学习笔记</title>
    <link href="/p/bfa4a5ca/"/>
    <url>/p/bfa4a5ca/</url>
    
    <content type="html"><![CDATA[<h2 id="Matplotlib-Pyplot"><a href="#Matplotlib-Pyplot" class="headerlink" title="Matplotlib Pyplot"></a><a href="https://www.runoob.com/matplotlib/matplotlib-pyplot.html">Matplotlib Pyplot</a></h2><p>Pyplot 是 Matplotlib 的子库，提供了和 MATLAB 类似的绘图 API。</p><p>Pyplot 是常用的绘图模块，能很方便让用户绘制 2D 图表。</p><p>Pyplot 包含一系列绘图函数的相关函数，每个函数会对当前的图像进行一些修改，例如：给图像加上标记，生新的图像，在图像中产生新的绘图区域等等。</p><p>使用的时候，我们可以使用 import 导入 pyplot 库，并设置一个别名 <strong>plt</strong>：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br></code></pre></td></tr></table></figure><p><strong>plot()</strong> 用于画图它可以绘制点和线，语法格式如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 画单条线</span><br>plot([x], y, [fmt], *, data=<span class="hljs-literal">None</span>, **kwargs)<br><span class="hljs-comment"># 画多条线</span><br>plot([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)<br></code></pre></td></tr></table></figure><p>参数说明：</p><ul><li><strong>x, y：</strong>点或线的节点，x 为 x 轴数据，y 为 y 轴数据，数据可以列表或数组。</li><li><strong>fmt：</strong>可选，定义基本格式（如颜色、标记和线条样式）。</li><li>*<strong>*kwargs：</strong>可选，用在二维平面图上，设置指定属性，如标签，线的宽度等。</li></ul><p><strong>颜色字符：</strong>‘b’ 蓝色，’m’ 洋红色，’g’ 绿色，’y’ 黄色，’r’ 红色，’k’ 黑色，’w’ 白色，’c’ 青绿色，’#008000’ RGB 颜色符串。多条曲线不指定颜色时，会自动选择不同颜色。</p><p><strong>线型参数：</strong>‘‐’ 实线，’‐‐’ 破折线，’‐.’ 点划线，’:’ 虚线。</p><p><strong>标记字符：</strong>‘.’ 点标记，’,’ 像素标记(极小点)，’o’ 实心圈标记，’v’ 倒三角标记，’^’ 上三角标记，’&gt;’ 右三角标记，’&lt;’ 左三角标记…等等。</p><p><img src="https://raw.githubusercontent.com/JiqingJiang/erode_Picgo_repo/main/2023/image-20230226184738175.png"></p><p><img src="https://raw.githubusercontent.com/JiqingJiang/erode_Picgo_repo/main/2023/image-20230226185408622.png"></p><h2 id="Matplotlib-绘图标记"><a href="#Matplotlib-绘图标记" class="headerlink" title="Matplotlib 绘图标记"></a>Matplotlib 绘图标记</h2><p>绘图过程如果我们想要给坐标自定义一些不一样的标记，就可以使用 <strong>plot()</strong> 方法的 <strong>marker</strong> 参数来定义。</p><p><strong>marker</strong> 可以定义的符号如下：</p><table><thead><tr><th align="left">标记</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left">“.”</td><td align="left">点</td></tr><tr><td align="left">“,”</td><td align="left">像素点</td></tr><tr><td align="left">“o”</td><td align="left">实心圆</td></tr><tr><td align="left">…</td><td align="left">…</td></tr><tr><td align="left"><strong>fmt 参数</strong></td><td align="left"></td></tr></tbody></table><p>fmt 参数定义了基本格式，如标记、线条样式和颜色。</p><figure class="highlight inform7"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs inform7">fmt = &#x27;<span class="hljs-comment">[marker]</span><span class="hljs-comment">[line]</span><span class="hljs-comment">[color]</span>&#x27;<br></code></pre></td></tr></table></figure><p>实例:例如 <strong>o:r</strong>，<strong>o</strong> 表示实心圆标记，**:** 表示虚线，<strong>r</strong> 表示颜色为红色。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br>ypoints = np.array([<span class="hljs-number">6</span>, <span class="hljs-number">2</span>, <span class="hljs-number">13</span>, <span class="hljs-number">10</span>])<br><br>plt.plot(ypoints, <span class="hljs-string">&#x27;o:r&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/JiqingJiang/erode_Picgo_repo/main/2023/image-20230226200322478.png"></p><p><strong>线类型：</strong></p><table><thead><tr><th align="left">线类型标记</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left">‘-‘</td><td align="left">实线</td></tr><tr><td align="left">‘:’</td><td align="left">虚线</td></tr><tr><td align="left">‘–’</td><td align="left">破折线</td></tr><tr><td align="left">‘-.’</td><td align="left">点划线</td></tr></tbody></table><p><strong>颜色类型：</strong></p><table><thead><tr><th align="left">颜色标记</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left">‘r’</td><td align="left">红色</td></tr><tr><td align="left">‘g’</td><td align="left">绿色</td></tr><tr><td align="left">‘b’</td><td align="left">蓝色</td></tr><tr><td align="left">‘c’</td><td align="left">青色</td></tr><tr><td align="left">‘m’</td><td align="left">品红</td></tr><tr><td align="left">‘y’</td><td align="left">黄色</td></tr><tr><td align="left">‘k’</td><td align="left">黑色</td></tr><tr><td align="left">‘w’</td><td align="left">白色</td></tr></tbody></table><p><strong>标记大小与颜色</strong></p><p>我们可以自定义标记的大小与颜色，使用的参数分别是：</p><ul><li>markersize，简写为 <strong>ms</strong>：定义标记的大小。</li><li>markerfacecolor，简写为 <strong>mfc</strong>：定义标记内部的颜色。</li><li>markeredgecolor，简写为 <strong>mec</strong>：定义标记边框的颜色。</li></ul><p><img src="https://raw.githubusercontent.com/JiqingJiang/erode_Picgo_repo/main/2023/image-20230226200717092.png"></p><h2 id="Matplotlib-绘图线"><a href="#Matplotlib-绘图线" class="headerlink" title="Matplotlib 绘图线"></a>Matplotlib 绘图线</h2><p>绘图过程如果我们自定义线的样式，包括线的类型、颜色和大小等。</p><p><strong>线的类型</strong></p><p>线的类型可以使用 <strong>linestyle</strong> 参数来定义，简写为 <strong>ls</strong>。</p><table><thead><tr><th align="left">类型</th><th align="left">简写</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">‘solid’ (默认)</td><td align="left">‘-‘</td><td align="left">实线</td></tr><tr><td align="left">‘dotted’</td><td align="left">‘:’</td><td align="left">点虚线</td></tr><tr><td align="left">‘dashed’</td><td align="left">‘–’</td><td align="left">破折线</td></tr><tr><td align="left">‘dashdot’</td><td align="left">‘-.’</td><td align="left">点划线</td></tr><tr><td align="left">‘None’</td><td align="left">‘’ 或 ‘ ‘</td><td align="left">不画线</td></tr></tbody></table><p><strong>线的颜色</strong></p><p>线的颜色可以使用 <strong>color</strong> 参数来定义，简写为 <strong>c</strong>。</p><table><thead><tr><th align="left">颜色标记</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left">‘r’</td><td align="left">红色</td></tr><tr><td align="left">‘g’</td><td align="left">绿色</td></tr><tr><td align="left">‘b’</td><td align="left">蓝色</td></tr><tr><td align="left">‘c’</td><td align="left">青色</td></tr><tr><td align="left">‘m’</td><td align="left">品红</td></tr><tr><td align="left">‘y’</td><td align="left">黄色</td></tr><tr><td align="left">‘k’</td><td align="left">黑色</td></tr><tr><td align="left">‘w’</td><td align="left">白色</td></tr></tbody></table><p>当然也可以自定义颜色类型，例如：<strong>SeaGreen、#8FBC8F</strong> 等，完整样式可以参考 <a href="https://www.runoob.com/html/html-colorvalues.html">HTML 颜色值</a>。</p><p><strong>线的宽度</strong></p><p>线的宽度可以使用 <strong>linewidth</strong> 参数来定义，简写为 <strong>lw</strong>，值可以是浮点数，如：<strong>1</strong>、<strong>2.0</strong>、<strong>5.67</strong> 等。</p><p><strong>多条线</strong></p><p>plot() 方法中可以包含多对 x,y 值来绘制多条线。</p><p><img src="https://raw.githubusercontent.com/JiqingJiang/erode_Picgo_repo/main/2023/image-20230226201622460.png"></p><h2 id="Matplotlib-轴标签和标题"><a href="#Matplotlib-轴标签和标题" class="headerlink" title="Matplotlib 轴标签和标题"></a>Matplotlib 轴标签和标题</h2><p>我们可以使用 <strong>xlabel()</strong> 和 <strong>ylabel()</strong> 方法来设置 x 轴和 y 轴的标签。</p><p><strong>标题</strong></p><p>我们可以使用 <strong>title()</strong> 方法来设置标题。</p><p><strong>图形中文显示</strong></p><p>Matplotlib 默认情况不支持中文，我们可以使用以下简单的方法来解决。</p><p>这里我们使用思源黑体，思源黑体是 Adobe 与 Google 推出的一款开源字体。</p><p>官网：<a href="https://source.typekit.com/source-han-serif/cn/">https://source.typekit.com/source-han-serif/cn/</a></p><p>GitHub 地址：<a href="https://github.com/adobe-fonts/source-han-sans/tree/release/OTF/SimplifiedChinese">https://github.com/adobe-fonts/source-han-sans/tree/release/OTF/SimplifiedChinese</a></p><p><strong>标题与标签的定位</strong></p><p><strong>title()</strong> 方法提供了 <strong>loc</strong> 参数来设置标题显示的位置，可以设置为: **’left’, ‘right’, 和 ‘center’， 默认值为 ‘center’**。</p><p><strong>xlabel()</strong> 方法提供了 <strong>loc</strong> 参数来设置 x 轴显示的位置，可以设置为: **’left’, ‘right’, 和 ‘center’， 默认值为 ‘center’**。</p><p><strong>ylabel()</strong> 方法提供了 <strong>loc</strong> 参数来设置 y 轴显示的位置，可以设置为: **’bottom’, ‘top’, 和 ‘center’， 默认值为 ‘center’**。</p><p><img src="https://raw.githubusercontent.com/JiqingJiang/erode_Picgo_repo/main/2023/image-20230226203227138.png"></p><h2 id="Matplotlib-网格线"><a href="#Matplotlib-网格线" class="headerlink" title="Matplotlib 网格线"></a>Matplotlib 网格线</h2><p>我们可以使用 pyplot 中的 grid() 方法来设置图表中的网格线。</p><p>grid() 方法语法格式如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">matplotlib.pyplot.grid(b=<span class="hljs-literal">None</span>, which=<span class="hljs-string">&#x27;major&#x27;</span>, axis=<span class="hljs-string">&#x27;both&#x27;</span>, )<br></code></pre></td></tr></table></figure><p><strong>参数说明：</strong></p><ul><li><strong>b</strong>：可选，默认为 None，可以设置布尔值，true 为显示网格线，false 为不显示，如果设置 **kwargs 参数，则值为 true。</li><li><strong>which</strong>：可选，可选值有 ‘major’、’minor’ 和 ‘both’，默认为 ‘major’，表示应用更改的网格线。</li><li><strong>axis</strong>：可选，设置显示哪个方向的网格线，可以是取 ‘both’（默认），’x’ 或 ‘y’，分别表示两个方向，x 轴方向或 y 轴方向。</li><li>*<strong>*kwargs</strong>：可选，设置网格样式，可以是 color&#x3D;’r’, linestyle&#x3D;’-‘ 和 linewidth&#x3D;2，分别表示网格线的颜色，样式和宽度。</li></ul><p><img src="https://raw.githubusercontent.com/JiqingJiang/erode_Picgo_repo/main/2023/image-20230226203616576.png"></p><p>以下实例添加一个简单的网格线，并设置网格线的样式，格式如下：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-function"><span class="hljs-title">grid</span><span class="hljs-params">(color = <span class="hljs-string">&#x27;color&#x27;</span>, linestyle = <span class="hljs-string">&#x27;linestyle&#x27;</span>, linewidth = number)</span></span><br></code></pre></td></tr></table></figure><p><strong>参数说明：</strong></p><p><strong>color：</strong>‘b’ 蓝色，’m’ 洋红色，’g’ 绿色，’y’ 黄色，’r’ 红色，’k’ 黑色，’w’ 白色，’c’ 青绿色，’#008000’ RGB 颜色符串。</p><p><strong>linestyle：</strong>‘‐’ 实线，’‐‐’ 破折线，’‐.’ 点划线，’:’ 虚线。</p><p><strong>linewidth</strong>：设置线的宽度，可以设置一个数字。</p><p><img src="https://raw.githubusercontent.com/JiqingJiang/erode_Picgo_repo/main/2023/image-20230226204117105.png"></p><h2 id="Matplotlib-绘制多图"><a href="#Matplotlib-绘制多图" class="headerlink" title="Matplotlib 绘制多图"></a>Matplotlib 绘制多图</h2><p>我们可以使用 pyplot 中的 <strong>subplot()</strong> 和 <strong>subplots()</strong> 方法来绘制多个子图。</p><p><strong>subplot()</strong> 方法在绘图时需要指定位置，<strong>subplots()</strong> 方法可以一次生成多个，在调用时只需要调用生成对象的 ax 即可。</p><p><strong>subplot</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">subplot(nrows, ncols, index, **kwargs)<br>subplot(pos, **kwargs)<br>subplot(**kwargs)<br>subplot(ax)<br></code></pre></td></tr></table></figure><p>以上函数将整个绘图区域分成 nrows 行和 ncols 列，然后从左到右，从上到下的顺序对每个子区域进行编号 <strong>1…N</strong> ，左上的子区域的编号为 1、右下的区域编号为 N，编号可以通过参数 <strong>index</strong> 来设置。</p><p>设置 numRows ＝ 1，numCols ＝ 2，就是将图表绘制成 1x2 的图片区域, 对应的坐标为：</p><figure class="highlight clojure"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs clojure">(<span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span>)<span class="hljs-punctuation">,</span> (<span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure><p><strong>plotNum ＝ 1</strong>, 表示的坐标为(1, 1), 即第一行第一列的子图。</p><p><strong>plotNum ＝ 2</strong>, 表示的坐标为(1, 2), 即第一行第二列的子图。</p><p><img src="https://raw.githubusercontent.com/JiqingJiang/erode_Picgo_repo/main/2023/image-20230226205116439.png"></p><p>设置 numRows ＝ 2，numCols ＝ 2，就是将图表绘制成 2x2 的图片区域, 对应的坐标为：</p><figure class="highlight clojure"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs clojure">(<span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span>)<span class="hljs-punctuation">,</span> (<span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">2</span>)<br>(<span class="hljs-number">2</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span>)<span class="hljs-punctuation">,</span> (<span class="hljs-number">2</span><span class="hljs-punctuation">,</span> <span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure><p><strong>plotNum ＝ 1</strong>, 表示的坐标为(1, 1), 即第一行第一列的子图。</p><p><strong>plotNum ＝ 2</strong>, 表示的坐标为(1, 2), 即第一行第二列的子图。</p><p><strong>plotNum ＝ 3</strong>, 表示的坐标为(2, 1), 即第二行第一列的子图。</p><p><strong>plotNum ＝ 4</strong>, 表示的坐标为(2, 2), 即第二行第二列的子图。</p><p><img src="https://raw.githubusercontent.com/JiqingJiang/erode_Picgo_repo/main/2023/image-20230226205350635.png"></p><p><strong>subplots()</strong></p><p>subplots() 方法语法格式如下：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">matplotlib.pyplot.subplots(<span class="hljs-attribute">nrows</span>=1, <span class="hljs-attribute">ncols</span>=1, *, <span class="hljs-attribute">sharex</span>=<span class="hljs-literal">False</span>, <span class="hljs-attribute">sharey</span>=<span class="hljs-literal">False</span>, <span class="hljs-attribute">squeeze</span>=<span class="hljs-literal">True</span>, <span class="hljs-attribute">subplot_kw</span>=None, <span class="hljs-attribute">gridspec_kw</span>=None, *<span class="hljs-number">*f</span>ig_kw)<br></code></pre></td></tr></table></figure><p><strong>参数说明：</strong></p><ul><li><strong>nrows</strong>：默认为 1，设置图表的行数。</li><li><strong>ncols</strong>：默认为 1，设置图表的列数。</li><li><strong>sharex、sharey</strong>：设置 x、y 轴是否共享属性，默认为 false，可设置为 ‘none’、’all’、’row’ 或 ‘col’。 False 或 none 每个子图的 x 轴或 y 轴都是独立的，True 或 ‘all’：所有子图共享 x 轴或 y 轴，’row’ 设置每个子图行共享一个 x 轴或 y 轴，’col’：设置每个子图列共享一个 x 轴或 y 轴。</li><li><strong>squeeze</strong>：布尔值，默认为 True，表示额外的维度从返回的 Axes(轴)对象中挤出，对于 N<em>1 或 1</em>N 个子图，返回一个 1 维数组，对于 N*M，N&gt;1 和 M&gt;1 返回一个 2 维数组。如果设置为 False，则不进行挤压操作，返回一个元素为 Axes 实例的2维数组，即使它最终是1x1。</li><li><strong>subplot_kw</strong>：可选，字典类型。把字典的关键字传递给 add_subplot() 来创建每个子图。</li><li><strong>gridspec_kw</strong>：可选，字典类型。把字典的关键字传递给 GridSpec 构造函数创建子图放在网格里(grid)。</li><li>*<strong>*fig_kw</strong>：把详细的关键字参数传给 figure() 函数。</li></ul><p><img src="https://raw.githubusercontent.com/JiqingJiang/erode_Picgo_repo/main/2023/image-20230226210152767.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># 创建一些测试数据 -- 图1</span><br>x = np.linspace(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>*np.pi, <span class="hljs-number">400</span>)<br>y = np.sin(x**<span class="hljs-number">2</span>)<br><br><span class="hljs-comment"># 创建一个画像和子图 -- 图2</span><br>fig, ax = plt.subplots()<br>ax.plot(x, y)<br>ax.set_title(<span class="hljs-string">&#x27;Simple plot&#x27;</span>)<br><br><span class="hljs-comment"># 创建两个子图 -- 图3</span><br>f, (ax1, ax2) = plt.subplots(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, sharey=<span class="hljs-literal">True</span>)<br>ax1.plot(x, y)<br>ax1.set_title(<span class="hljs-string">&#x27;Sharing Y axis&#x27;</span>)<br>ax2.scatter(x, y)<br><br><span class="hljs-comment"># 创建四个子图 -- 图4</span><br>fig, axs = plt.subplots(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, subplot_kw=<span class="hljs-built_in">dict</span>(projection=<span class="hljs-string">&quot;polar&quot;</span>))<br>axs[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>].plot(x, y)<br>axs[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>].scatter(x, y)<br><br><span class="hljs-comment"># 共享 x 轴</span><br>plt.subplots(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, sharex=<span class="hljs-string">&#x27;col&#x27;</span>)<br><br><span class="hljs-comment"># 共享 y 轴</span><br>plt.subplots(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, sharey=<span class="hljs-string">&#x27;row&#x27;</span>)<br><br><span class="hljs-comment"># 共享 x 轴和 y 轴</span><br>plt.subplots(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, sharex=<span class="hljs-string">&#x27;all&#x27;</span>, sharey=<span class="hljs-string">&#x27;all&#x27;</span>)<br><br><span class="hljs-comment"># 这个也是共享 x 轴和 y 轴</span><br>plt.subplots(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, sharex=<span class="hljs-literal">True</span>, sharey=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># 创建标识为 10 的图，已经存在的则删除</span><br>fig, ax = plt.subplots(num=<span class="hljs-number">10</span>, clear=<span class="hljs-literal">True</span>)<br><br>plt.show()<br></code></pre></td></tr></table></figure><h2 id="Matplotlib-散点图"><a href="#Matplotlib-散点图" class="headerlink" title="Matplotlib 散点图"></a>Matplotlib 散点图</h2><p>我们可以使用 pyplot 中的 scatter() 方法来绘制散点图。</p><p>scatter() 方法语法格式如下：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">matplotlib.pyplot.scatter(x, y, <span class="hljs-attribute">s</span>=None, <span class="hljs-attribute">c</span>=None, <span class="hljs-attribute">marker</span>=None, <span class="hljs-attribute">cmap</span>=None, <span class="hljs-attribute">norm</span>=None, <span class="hljs-attribute">vmin</span>=None, <span class="hljs-attribute">vmax</span>=None, <span class="hljs-attribute">alpha</span>=None, <span class="hljs-attribute">linewidths</span>=None, *, <span class="hljs-attribute">edgecolors</span>=None, <span class="hljs-attribute">plotnonfinite</span>=<span class="hljs-literal">False</span>, <span class="hljs-attribute">data</span>=None, **kwargs)<br></code></pre></td></tr></table></figure><p><strong>参数说明：</strong></p><p><strong>x，y</strong>：长度相同的数组，也就是我们即将绘制散点图的数据点，输入数据。</p><p><strong>s</strong>：点的大小，默认 20，也可以是个数组，数组每个参数为对应点的大小。</p><p><strong>c</strong>：点的颜色，默认蓝色 ‘b’，也可以是个 RGB 或 RGBA 二维行数组。</p><p><strong>marker</strong>：点的样式，默认小圆圈 ‘o’。</p><p><strong>cmap</strong>：Colormap，默认 None，标量或者是一个 colormap 的名字，只有 c 是一个浮点数数组的时才使用。如果没有申明就是 image.cmap。</p><p><strong>norm</strong>：Normalize，默认 None，数据亮度在 0-1 之间，只有 c 是一个浮点数的数组的时才使用。</p><p><strong>vmin，vmax：</strong>：亮度设置，在 norm 参数存在时会忽略。</p><p><strong>alpha：</strong>：透明度设置，0-1 之间，默认 None，即不透明。</p><p><strong>linewidths：</strong>：标记点的长度。</p><p><strong>edgecolors：</strong>：颜色或颜色序列，默认为 ‘face’，可选值有 ‘face’, ‘none’, None。</p><p><strong>plotnonfinite：</strong>：布尔值，设置是否使用非限定的 c ( inf, -inf 或 nan) 绘制点。</p><p>*<strong>*kwargs：</strong>：其他参数。</p><p><img src="https://raw.githubusercontent.com/JiqingJiang/erode_Picgo_repo/main/2023/image-20230226215018315.png"></p><p><strong>颜色条 Colormap</strong></p><p>Matplotlib 模块提供了很多可用的颜色条。</p><p>颜色条就像一个颜色列表，其中每种颜色都有一个范围从 0 到 100 的值。</p><p>下面是一个颜色条的例子：</p><p><img src="https://raw.githubusercontent.com/JiqingJiang/erode_Picgo_repo/main/2023/img_colorbar.png"></p><p>设置颜色条需要使用 cmap 参数，默认值为 ‘viridis’，之后颜色值设置为 0 到 100 的数组。</p><p>如果要显示颜色条，需要使用 <strong>plt.colorbar()</strong> 方法</p><p>颜色条参数值可以是以下值：</p><table><thead><tr><th align="left">颜色名称</th><th align="left"></th><th align="left">保留关键字</th></tr></thead><tbody><tr><td align="left">Accent</td><td align="left"></td><td align="left">Accent_r</td></tr><tr><td align="left">Blues</td><td align="left"></td><td align="left">Blues_r</td></tr><tr><td align="left">BrBG</td><td align="left"></td><td align="left">BrBG_r</td></tr><tr><td align="left">BuGn</td><td align="left"></td><td align="left">BuGn_r</td></tr><tr><td align="left">BuPu</td><td align="left"></td><td align="left">BuPu_r</td></tr><tr><td align="left">CMRmap</td><td align="left"></td><td align="left">CMRmap_r</td></tr><tr><td align="left">Dark2</td><td align="left"></td><td align="left">Dark2_r</td></tr><tr><td align="left">GnBu</td><td align="left"></td><td align="left">GnBu_r</td></tr><tr><td align="left">Greens</td><td align="left"></td><td align="left">Greens_r</td></tr><tr><td align="left">Greys</td><td align="left"></td><td align="left">Greys_r</td></tr><tr><td align="left">OrRd</td><td align="left"></td><td align="left">OrRd_r</td></tr><tr><td align="left">Oranges</td><td align="left"></td><td align="left">Oranges_r</td></tr><tr><td align="left">PRGn</td><td align="left"></td><td align="left">PRGn_r</td></tr><tr><td align="left">Paired</td><td align="left"></td><td align="left">Paired_r</td></tr><tr><td align="left">Pastel1</td><td align="left"></td><td align="left">Pastel1_r</td></tr><tr><td align="left">Pastel2</td><td align="left"></td><td align="left">Pastel2_r</td></tr><tr><td align="left">PiYG</td><td align="left"></td><td align="left">PiYG_r</td></tr><tr><td align="left">PuBu</td><td align="left"></td><td align="left">PuBu_r</td></tr><tr><td align="left">PuBuGn</td><td align="left"></td><td align="left">PuBuGn_r</td></tr><tr><td align="left">PuOr</td><td align="left"></td><td align="left">PuOr_r</td></tr><tr><td align="left">PuRd</td><td align="left"></td><td align="left">PuRd_r</td></tr><tr><td align="left">Purples</td><td align="left"></td><td align="left">Purples_r</td></tr><tr><td align="left">RdBu</td><td align="left"></td><td align="left">RdBu_r</td></tr><tr><td align="left">RdGy</td><td align="left"></td><td align="left">RdGy_r</td></tr><tr><td align="left">RdPu</td><td align="left"></td><td align="left">RdPu_r</td></tr><tr><td align="left">RdYlBu</td><td align="left"></td><td align="left">RdYlBu_r</td></tr><tr><td align="left">RdYlGn</td><td align="left"></td><td align="left">RdYlGn_r</td></tr><tr><td align="left">Reds</td><td align="left"></td><td align="left">Reds_r</td></tr><tr><td align="left">Set1</td><td align="left"></td><td align="left">Set1_r</td></tr><tr><td align="left">Set2</td><td align="left"></td><td align="left">Set2_r</td></tr><tr><td align="left">Set3</td><td align="left"></td><td align="left">Set3_r</td></tr><tr><td align="left">Spectral</td><td align="left"></td><td align="left">Spectral_r</td></tr><tr><td align="left">Wistia</td><td align="left"></td><td align="left">Wistia_r</td></tr><tr><td align="left">YlGn</td><td align="left"></td><td align="left">YlGn_r</td></tr><tr><td align="left">YlGnBu</td><td align="left"></td><td align="left">YlGnBu_r</td></tr><tr><td align="left">YlOrBr</td><td align="left"></td><td align="left">YlOrBr_r</td></tr><tr><td align="left">YlOrRd</td><td align="left"></td><td align="left">YlOrRd_r</td></tr><tr><td align="left">afmhot</td><td align="left"></td><td align="left">afmhot_r</td></tr><tr><td align="left">autumn</td><td align="left"></td><td align="left">autumn_r</td></tr><tr><td align="left">binary</td><td align="left"></td><td align="left">binary_r</td></tr><tr><td align="left">bone</td><td align="left"></td><td align="left">bone_r</td></tr><tr><td align="left">brg</td><td align="left"></td><td align="left">brg_r</td></tr><tr><td align="left">bwr</td><td align="left"></td><td align="left">bwr_r</td></tr><tr><td align="left">cividis</td><td align="left"></td><td align="left">cividis_r</td></tr><tr><td align="left">cool</td><td align="left"></td><td align="left">cool_r</td></tr><tr><td align="left">coolwarm</td><td align="left"></td><td align="left">coolwarm_r</td></tr><tr><td align="left">copper</td><td align="left"></td><td align="left">copper_r</td></tr><tr><td align="left">cubehelix</td><td align="left"></td><td align="left">cubehelix_r</td></tr><tr><td align="left">flag</td><td align="left"></td><td align="left">flag_r</td></tr><tr><td align="left">gist_earth</td><td align="left"></td><td align="left">gist_earth_r</td></tr><tr><td align="left">gist_gray</td><td align="left"></td><td align="left">gist_gray_r</td></tr><tr><td align="left">gist_heat</td><td align="left"></td><td align="left">gist_heat_r</td></tr><tr><td align="left">gist_ncar</td><td align="left"></td><td align="left">gist_ncar_r</td></tr><tr><td align="left">gist_rainbow</td><td align="left"></td><td align="left">gist_rainbow_r</td></tr><tr><td align="left">gist_stern</td><td align="left"></td><td align="left">gist_stern_r</td></tr><tr><td align="left">gist_yarg</td><td align="left"></td><td align="left">gist_yarg_r</td></tr><tr><td align="left">gnuplot</td><td align="left"></td><td align="left">gnuplot_r</td></tr><tr><td align="left">gnuplot2</td><td align="left"></td><td align="left">gnuplot2_r</td></tr><tr><td align="left">gray</td><td align="left"></td><td align="left">gray_r</td></tr><tr><td align="left">hot</td><td align="left"></td><td align="left">hot_r</td></tr><tr><td align="left">hsv</td><td align="left"></td><td align="left">hsv_r</td></tr><tr><td align="left">inferno</td><td align="left"></td><td align="left">inferno_r</td></tr><tr><td align="left">jet</td><td align="left"></td><td align="left">jet_r</td></tr><tr><td align="left">magma</td><td align="left"></td><td align="left">magma_r</td></tr><tr><td align="left">nipy_spectral</td><td align="left"></td><td align="left">nipy_spectral_r</td></tr><tr><td align="left">ocean</td><td align="left"></td><td align="left">ocean_r</td></tr><tr><td align="left">pink</td><td align="left"></td><td align="left">pink_r</td></tr><tr><td align="left">plasma</td><td align="left"></td><td align="left">plasma_r</td></tr><tr><td align="left">prism</td><td align="left"></td><td align="left">prism_r</td></tr><tr><td align="left">rainbow</td><td align="left"></td><td align="left">rainbow_r</td></tr><tr><td align="left">seismic</td><td align="left"></td><td align="left">seismic_r</td></tr><tr><td align="left">spring</td><td align="left"></td><td align="left">spring_r</td></tr><tr><td align="left">summer</td><td align="left"></td><td align="left">summer_r</td></tr><tr><td align="left">tab10</td><td align="left"></td><td align="left">tab10_r</td></tr><tr><td align="left">tab20</td><td align="left"></td><td align="left">tab20_r</td></tr><tr><td align="left">tab20b</td><td align="left"></td><td align="left">tab20b_r</td></tr><tr><td align="left">tab20c</td><td align="left"></td><td align="left">tab20c_r</td></tr><tr><td align="left">terrain</td><td align="left"></td><td align="left">terrain_r</td></tr><tr><td align="left">twilight</td><td align="left"></td><td align="left">twilight_r</td></tr><tr><td align="left">twilight_shifted</td><td align="left"></td><td align="left">twilight_shifted_r</td></tr><tr><td align="left">viridis</td><td align="left"></td><td align="left">viridis_r</td></tr><tr><td align="left">winter</td><td align="left"></td><td align="left">winter_r</td></tr></tbody></table><h2 id="Matplotlib-柱形图"><a href="#Matplotlib-柱形图" class="headerlink" title="Matplotlib 柱形图"></a>Matplotlib 柱形图</h2><p>我们可以使用 pyplot 中的 bar() 方法来绘制柱形图。</p><p>bar() 方法语法格式如下：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">matplotlib.pyplot.bar(x, height, <span class="hljs-attribute">width</span>=0.8, <span class="hljs-attribute">bottom</span>=None, *, <span class="hljs-attribute">align</span>=<span class="hljs-string">&#x27;center&#x27;</span>, <span class="hljs-attribute">data</span>=None, **kwargs)<br></code></pre></td></tr></table></figure><p><strong>参数说明：</strong></p><p><strong>x</strong>：浮点型数组，柱形图的 x 轴数据。</p><p><strong>height</strong>：浮点型数组，柱形图的高度。</p><p><strong>width</strong>：浮点型数组，柱形图的宽度。</p><p><strong>bottom</strong>：浮点型数组，底座的 y 坐标，默认 0。</p><p><strong>align</strong>：柱形图与 x 坐标的对齐方式，’center’ 以 x 位置为中心，这是默认值。 ‘edge’：将柱形图的左边缘与 x 位置对齐。要对齐右边缘的条形，可以传递负数的宽度值及 align&#x3D;’edge’。</p><p>*<strong>*kwargs：</strong>：其他参数。</p><p>垂直方向的柱形图可以使用 <strong>barh()</strong> 方法来设置，水平用bar()</p><p>设置柱形图宽度，<strong>bar()</strong> 方法使用 <strong>width</strong> 设置，<strong>barh()</strong> 方法使用 <strong>height</strong> 设置 height</p><p><img src="https://raw.githubusercontent.com/JiqingJiang/erode_Picgo_repo/main/2023/image-20230226214537178.png"></p><h2 id="Matplotlib-饼图"><a href="#Matplotlib-饼图" class="headerlink" title="Matplotlib 饼图"></a>Matplotlib 饼图</h2><p>我们可以使用 pyplot 中的 pie() 方法来绘制饼图。</p><p>pie() 方法语法格式如下：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">matplotlib.pyplot.pie(x, <span class="hljs-attribute">explode</span>=None, <span class="hljs-attribute">labels</span>=None, <span class="hljs-attribute">colors</span>=None, <span class="hljs-attribute">autopct</span>=None, <span class="hljs-attribute">pctdistance</span>=0.6, <span class="hljs-attribute">shadow</span>=<span class="hljs-literal">False</span>, <span class="hljs-attribute">labeldistance</span>=1.1, <span class="hljs-attribute">startangle</span>=0, <span class="hljs-attribute">radius</span>=1, <span class="hljs-attribute">counterclock</span>=<span class="hljs-literal">True</span>, <span class="hljs-attribute">wedgeprops</span>=None, <span class="hljs-attribute">textprops</span>=None, <span class="hljs-attribute">center</span>=0, 0, <span class="hljs-attribute">frame</span>=<span class="hljs-literal">False</span>, <span class="hljs-attribute">rotatelabels</span>=<span class="hljs-literal">False</span>, *, <span class="hljs-attribute">normalize</span>=None, <span class="hljs-attribute">data</span>=None)[source]<br></code></pre></td></tr></table></figure><p><strong>参数说明：</strong></p><p><strong>x</strong>：浮点型数组，表示每个扇形的面积。</p><p><strong>explode</strong>：数组，表示各个扇形之间的间隔，默认值为0。</p><p><strong>labels</strong>：列表，各个扇形的标签，默认值为 None。</p><p><strong>colors</strong>：数组，表示各个扇形的颜色，默认值为 None。</p><p><strong>autopct</strong>：设置饼图内各个扇形百分比显示格式，**%d%%** 整数百分比，**%0.1f** 一位小数， <strong>%0.1f%%</strong> 一位小数百分比， <strong>%0.2f%%</strong> 两位小数百分比。</p><p><strong>labeldistance</strong>：标签标记的绘制位置，相对于半径的比例，默认值为 1.1，如 <strong>&lt;1</strong>则绘制在饼图内侧。</p><p><strong>pctdistance：</strong>：类似于 labeldistance，指定 autopct 的位置刻度，默认值为 0.6。</p><p><strong>shadow：</strong>：布尔值 True 或 False，设置饼图的阴影，默认为 False，不设置阴影。</p><p><strong>radius：</strong>：设置饼图的半径，默认为 1。</p><p><strong>startangle：</strong>：起始绘制饼图的角度，默认为从 x 轴正方向逆时针画起，如设定 &#x3D;90 则从 y 轴正方向画起。</p><p><strong>counterclock</strong>：布尔值，设置指针方向，默认为 True，即逆时针，False 为顺时针。</p><p><strong>wedgeprops</strong> ：字典类型，默认值 None。参数字典传递给 wedge 对象用来画一个饼图。例如：wedgeprops&#x3D;{‘linewidth’:5} 设置 wedge 线宽为5。</p><p><strong>textprops</strong> ：字典类型，默认值为：None。传递给 text 对象的字典参数，用于设置标签（labels）和比例文字的格式。</p><p><strong>center</strong> ：浮点类型的列表，默认值：(0,0)。用于设置图标中心位置。</p><p><strong>frame</strong> ：布尔类型，默认值：False。如果是 True，绘制带有表的轴框架。</p><p><strong>rotatelabels</strong> ：布尔类型，默认为 False。如果为 True，旋转每个 label 到指定的角度。</p><p><img src="https://raw.githubusercontent.com/JiqingJiang/erode_Picgo_repo/main/2023/image-20230226213708282.png"></p>]]></content>
    
    
    <categories>
      
      <category>python学习笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Matplotlib</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>networkx学习笔记</title>
    <link href="/p/a75be723/"/>
    <url>/p/a75be723/</url>
    
    <content type="html"><![CDATA[<h2 id="networkX"><a href="#networkX" class="headerlink" title="networkX"></a><a href="https://networkx.org/documentation/stable/tutorial.html#">networkX</a></h2><h3 id="创建一个图"><a href="#创建一个图" class="headerlink" title="创建一个图"></a>创建一个图</h3><p>首先创建一个没有节点没有边的空图</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> networkx <span class="hljs-keyword">as</span> nx<br>G = nx.Graph()<br></code></pre></td></tr></table></figure><p>根据图的定义，一个图包含一个节点集合和一个边集合。在NetworkX中，节点可以是任何哈希对象，比如一个字符串，一幅图像，一个XML对象，甚至是另一个图或者任意定制的节点对象。</p><blockquote><p>注：Python中的None对象不能作为节点的类型。</p></blockquote><h3 id="节点"><a href="#节点" class="headerlink" title="节点"></a>节点</h3><p>图可以以多种形式扩张。NetworkX包括许多<a href="https://link.zhihu.com/?target=https://networkx.org/documentation/stable/reference/generators.html">图生成函数</a>和<a href="https://link.zhihu.com/?target=https://networkx.org/documentation/stable/reference/readwrite/index.html">工具</a>，用于读取和写入多种格式的图。</p><p>作为简单开始，可以每次添加一个节点：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">G.add_node(<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><p>或者从可迭代容器（如列表）中添加多个节点:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">G.add_node([<span class="hljs-number">2</span>,<span class="hljs-number">3</span>])<br></code></pre></td></tr></table></figure><p>你也可以同时添加包含节点属性的节点:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">G.add_node([<br>    (<span class="hljs-number">4</span>,&#123;<span class="hljs-string">&quot;color&quot;</span>:<span class="hljs-string">&quot;red&quot;</span>&#125;),<br>    (<span class="hljs-number">5</span>,&#123;<span class="hljs-string">&quot;color&quot;</span>:<span class="hljs-string">&quot;green&quot;</span>&#125;)<br>])<br></code></pre></td></tr></table></figure><p>一个图中的节点可以合并到另一个图中:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">H = nx.path_graph(<span class="hljs-number">10</span>)<br>G.add_nodes_from(H)<br></code></pre></td></tr></table></figure><p>现在图<code>G</code>中包含了原来图<code>H</code>中的节点。你也可以将整个图<code>H</code>作为图<code>G</code>中的一个节点</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">G.add_node(H)<br></code></pre></td></tr></table></figure><p>现在图<code>G</code>将图<code>H</code>整体作为一个节点。这种灵活性非常强大，因为它允许图形组成图形、文件组成图形、函数组成图形等等。值得考虑的是如何构造应用程序，使节点成为有用的实体。当然，如果您愿意，您总是可以在中使用唯一标识符，并使用单独的字典根据标识符对节点信息进行键控。</p><h3 id="边"><a href="#边" class="headerlink" title="边"></a>边</h3><p>图也可以以加边的方式增长：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">G.add_edge(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>e = (<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)<br>G.add_edge(*e)  <span class="hljs-comment"># unpack edge tuple*</span><br></code></pre></td></tr></table></figure><p>通过边的列表增长:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">G.add_edges_from([(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>), (<span class="hljs-number">1</span>, <span class="hljs-number">3</span>)])<br></code></pre></td></tr></table></figure><p>或者通过添加任何边的ebunch</p><p>ebunch是边的元组的任何可迭代容器。边的元组可以是2元组节点，也可以是3元组，在第2个节点后面加上边的属性字典，例如<code>(2,3,&#123;&#39;weight&#39;:3.1415&#125;)</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">G.add_edges_from(H.edges)<br></code></pre></td></tr></table></figure><p>添加现有节点或边时没有任何冲突。例如，删除所有节点和边后，</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">G.clear()<br></code></pre></td></tr></table></figure><p>在添加新的节点&#x2F;边时，NetworkX悄悄地忽略任何已经存在的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">G.add_edges_from([(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>), (<span class="hljs-number">1</span>, <span class="hljs-number">3</span>)])<br>G.add_node(<span class="hljs-number">1</span>)<br>G.add_edge(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>G.add_node(<span class="hljs-string">&quot;spam&quot;</span>)        <span class="hljs-comment"># adds node &quot;spam&quot;</span><br>G.add_nodes_from(<span class="hljs-string">&quot;spam&quot;</span>)  <span class="hljs-comment"># adds 4 nodes: &#x27;s&#x27;, &#x27;p&#x27;, &#x27;a&#x27;, &#x27;m&#x27;</span><br>G.add_edge(<span class="hljs-number">3</span>, <span class="hljs-string">&#x27;m&#x27;</span>)<br></code></pre></td></tr></table></figure><p>此时，图由 8 个节点和 3 条边组成，如下所示</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>G.number_of_nodes()<br><span class="hljs-number">8</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>G.number_of_edges()<br><span class="hljs-number">3</span><br></code></pre></td></tr></table></figure><blockquote><p>注：邻接报告的顺序（例如，<a href="https://networkx.org/documentation/stable/reference/classes/generated/networkx.Graph.adj.html#networkx.Graph.adj"><code>G.adj</code></a>，G<a href="https://networkx.org/documentation/stable/reference/classes/generated/networkx.DiGraph.successors.html#networkx.DiGraph.successors"><code>.successors</code></a>，<a href="https://networkx.org/documentation/stable/reference/classes/generated/networkx.DiGraph.predecessors.html#networkx.DiGraph.predecessors"><code>G.predecessors</code></a>）是 边缘添加。但是，G.edge 的顺序是邻接的顺序 其中包括节点的顺序和每个节点 节点的邻接关系。请参阅以下示例：</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">DG = nx.DiGraph()<br>DG.add_edge(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>)   <span class="hljs-comment"># adds the nodes in order 2, 1</span><br>DG.add_edge(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>)<br>DG.add_edge(<span class="hljs-number">2</span>, <span class="hljs-number">4</span>)<br>DG.add_edge(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br><span class="hljs-keyword">assert</span> <span class="hljs-built_in">list</span>(DG.successors(<span class="hljs-number">2</span>)) == [<span class="hljs-number">1</span>, <span class="hljs-number">4</span>]<br><span class="hljs-keyword">assert</span> <span class="hljs-built_in">list</span>(DG.edges) == [(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>), (<span class="hljs-number">2</span>, <span class="hljs-number">4</span>), (<span class="hljs-number">1</span>, <span class="hljs-number">3</span>), (<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)]<br></code></pre></td></tr></table></figure><h3 id="检查图的元素"><a href="#检查图的元素" class="headerlink" title="检查图的元素"></a>检查图的元素</h3><p>我们可以检查节点和边。四个基本图属性有助于报告：<code>G.nodes</code>、<code>G.edges</code>、<code>G.adj</code>和<code>G.degree</code> 。这些图中节点、边、相邻（邻接）和节点的度的集合视图。它们为图形结构提供了一个不断更新的只读视图。它们也是类似于<code>dict</code>，因为您可以查找节点和边数据属性通过使用方法<code>.items()</code>、<code>.data()</code>迭代数据属性。 如果您需要一个特定的容器类型而不是视图，可以指定一个。 这里我们使用列表，尽管集合、字典、元组和其他容器在其他情况下可能会更好。<code>G.nodes</code> <code>G.edges</code> <code>G.adj</code> <code>G.degree</code> <code>.items()</code> <code>.data()</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(G.nodes)<br>[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-string">&#x27;spam&#x27;</span>, <span class="hljs-string">&#x27;s&#x27;</span>, <span class="hljs-string">&#x27;p&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;m&#x27;</span>]<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(G.edges)<br>[(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>), (<span class="hljs-number">1</span>, <span class="hljs-number">3</span>), (<span class="hljs-number">3</span>, <span class="hljs-string">&#x27;m&#x27;</span>)]<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(G.adj[<span class="hljs-number">1</span>])  <span class="hljs-comment"># or list(G.neighbors(1))</span><br>[<span class="hljs-number">2</span>, <span class="hljs-number">3</span>]<br><span class="hljs-meta">&gt;&gt;&gt; </span>G.degree[<span class="hljs-number">1</span>]  <span class="hljs-comment"># the number of edges incident to 1</span><br><span class="hljs-number">2</span><br></code></pre></td></tr></table></figure><h3 id="从图形中删除元素"><a href="#从图形中删除元素" class="headerlink" title="从图形中删除元素"></a>从图形中删除元素</h3><p>可以采用与添加类似的方式从图形中删除节点和边。 使用方法 <code>Graph.remove_node()</code>、<code>Graph.remove_nodes_from()</code>、<code>Graph.remove_edge()</code> 和 <code>Graph.remove_edges_from()</code>例如</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>G.remove_node(<span class="hljs-number">2</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span>G.remove_nodes_from(<span class="hljs-string">&quot;spam&quot;</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(G.nodes)<br>[<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-string">&#x27;spam&#x27;</span>]<br><span class="hljs-meta">&gt;&gt;&gt; </span>G.remove_edge(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>)<br></code></pre></td></tr></table></figure><h3 id="使用图形构造函数"><a href="#使用图形构造函数" class="headerlink" title="使用图形构造函数"></a>使用图形构造函数</h3><p>图形对象不只是用增量方式构建 - 指定图形结构的数据可以直接传递给各种图形类构造函数。 通过实例化其中一个图创建图结构时，您 可以指定多种格式的数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>G.add_edge(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span>H = nx.DiGraph(G)  <span class="hljs-comment"># create a DiGraph using the connections from G</span><br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(H.edges())<br>[(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>), (<span class="hljs-number">2</span>, <span class="hljs-number">1</span>)]<br><span class="hljs-meta">&gt;&gt;&gt; </span>edgelist = [(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>), (<span class="hljs-number">1</span>, <span class="hljs-number">2</span>), (<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)]<br><span class="hljs-meta">&gt;&gt;&gt; </span>H = nx.Graph(edgelist)  <span class="hljs-comment"># create a graph from an edge list</span><br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(H.edges())<br>[(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>), (<span class="hljs-number">1</span>, <span class="hljs-number">2</span>), (<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)]<br><span class="hljs-meta">&gt;&gt;&gt; </span>adjacency_dict = &#123;<span class="hljs-number">0</span>: (<span class="hljs-number">1</span>, <span class="hljs-number">2</span>), <span class="hljs-number">1</span>: (<span class="hljs-number">0</span>, <span class="hljs-number">2</span>), <span class="hljs-number">2</span>: (<span class="hljs-number">0</span>, <span class="hljs-number">1</span>)&#125;<br><span class="hljs-meta">&gt;&gt;&gt; </span>H = nx.Graph(adjacency_dict)  <span class="hljs-comment"># create a Graph dict mapping nodes to nbrs</span><br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(H.edges())<br>[(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>), (<span class="hljs-number">0</span>, <span class="hljs-number">2</span>), (<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)]<br></code></pre></td></tr></table></figure><h3 id="用作节点和边的内容"><a href="#用作节点和边的内容" class="headerlink" title="用作节点和边的内容"></a>用作节点和边的内容</h3><p>您可能会注意到节点和边未指定为 NetworkX 对象。这使您可以自由地使用有意义的项作为节点和边。最常见的选择是数字或字符串，但节点可以是任何可哈希对象（除了<code>None</code>），并且可以使用的<code>G.add_edge(n1, n2, object=x)</code> 将边与任何对象<code>x</code>关联。</p><p>例如，可以是来自RCSB蛋白质 数据库的蛋白质对象，而<code>x</code>可以参考详细说明的出版物的<code>XML</code>记录,详细说明了它们相互作用的实验观察结果。</p><p>我们发现这种力量非常有用， 除非熟悉 Python，否则被滥用可能会导致令人惊讶的行为。 如有疑问，请考虑使用 <a href="https://networkx.org/documentation/stable/reference/generated/networkx.relabel.convert_node_labels_to_integers.html#networkx.relabel.convert_node_labels_to_integers"><code>convert_node_labels_to_integers()</code></a> 获取带有整数标签的更传统的图。</p><h3 id="访问边和邻接点"><a href="#访问边和邻接点" class="headerlink" title="访问边和邻接点"></a>访问边和邻接点</h3><p>除了<a href="https://networkx.org/documentation/stable/reference/classes/generated/networkx.Graph.edges.html#networkx.Graph.edges"><code>视图 Graph.edge</code> 和</a> <a href="https://networkx.org/documentation/stable/reference/classes/generated/networkx.Graph.adj.html#networkx.Graph.adj"><code>Graph.adj</code></a> 之外， 可以使用下标表示法访问边和相邻点。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>G = nx.Graph([(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, &#123;<span class="hljs-string">&quot;color&quot;</span>: <span class="hljs-string">&quot;yellow&quot;</span>&#125;)])<br><span class="hljs-meta">&gt;&gt;&gt; </span>G[<span class="hljs-number">1</span>]  <span class="hljs-comment"># same as G.adj[1]</span><br>AtlasView(&#123;<span class="hljs-number">2</span>: &#123;<span class="hljs-string">&#x27;color&#x27;</span>: <span class="hljs-string">&#x27;yellow&#x27;</span>&#125;&#125;)<br><span class="hljs-meta">&gt;&gt;&gt; </span>G[<span class="hljs-number">1</span>][<span class="hljs-number">2</span>]<br>&#123;<span class="hljs-string">&#x27;color&#x27;</span>: <span class="hljs-string">&#x27;yellow&#x27;</span>&#125;<br><span class="hljs-meta">&gt;&gt;&gt; </span>G.edges[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>]<br>&#123;<span class="hljs-string">&#x27;color&#x27;</span>: <span class="hljs-string">&#x27;yellow&#x27;</span>&#125;<br></code></pre></td></tr></table></figure><p>您可以使用下标表示法获取&#x2F;设置边的属性 如果边缘已存在。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>G.add_edge(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span>G[<span class="hljs-number">1</span>][<span class="hljs-number">3</span>][<span class="hljs-string">&#x27;color&#x27;</span>] = <span class="hljs-string">&quot;blue&quot;</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>G.edges[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>][<span class="hljs-string">&#x27;color&#x27;</span>] = <span class="hljs-string">&quot;red&quot;</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>G.edges[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>]<br>&#123;<span class="hljs-string">&#x27;color&#x27;</span>: <span class="hljs-string">&#x27;red&#x27;</span>&#125;<br></code></pre></td></tr></table></figure><p>使用<code>G.adjacency()</code>或<code>G.adj.items()</code>实现对所有（节点、邻接）对的快速检查。 请注意，对于无向图，邻接迭代会看到每条边两次。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>FG = nx.Graph()<br><span class="hljs-meta">&gt;&gt;&gt; </span>FG.add_weighted_edges_from([(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0.125</span>), (<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">0.75</span>), (<span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">1.2</span>), (<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">0.375</span>)])<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> n, nbrs <span class="hljs-keyword">in</span> FG.adj.items():<br><span class="hljs-meta">... </span>   <span class="hljs-keyword">for</span> nbr, eattr <span class="hljs-keyword">in</span> nbrs.items():<br><span class="hljs-meta">... </span>       wt = eattr[<span class="hljs-string">&#x27;weight&#x27;</span>]<br><span class="hljs-meta">... </span>       <span class="hljs-keyword">if</span> wt &lt; <span class="hljs-number">0.5</span>: <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;(<span class="hljs-subst">&#123;n&#125;</span>, <span class="hljs-subst">&#123;nbr&#125;</span>, <span class="hljs-subst">&#123;wt:<span class="hljs-number">.3</span>&#125;</span>)&quot;</span>)<br>(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0.125</span>)<br>(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0.125</span>)<br>(<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">0.375</span>)<br>(<span class="hljs-number">4</span>, <span class="hljs-number">3</span>, <span class="hljs-number">0.375</span>)<br></code></pre></td></tr></table></figure><p>通过 edge 属性可以方便地访问所有边。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> (u, v, wt) <span class="hljs-keyword">in</span> FG.edges.data(<span class="hljs-string">&#x27;weight&#x27;</span>):<br><span class="hljs-meta">... </span>    <span class="hljs-keyword">if</span> wt &lt; <span class="hljs-number">0.5</span>:<br><span class="hljs-meta">... </span>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;(<span class="hljs-subst">&#123;u&#125;</span>, <span class="hljs-subst">&#123;v&#125;</span>, <span class="hljs-subst">&#123;wt:<span class="hljs-number">.3</span>&#125;</span>)&quot;</span>)<br>(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0.125</span>)<br>(<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">0.375</span>)<br></code></pre></td></tr></table></figure><h3 id="向图形、节点和边添加属性"><a href="#向图形、节点和边添加属性" class="headerlink" title="向图形、节点和边添加属性"></a>向图形、节点和边添加属性</h3><p>如权重、标签、颜色或任何你喜欢的 Python 对象， 可以附加到图形、节点或边上。</p><p>每个图、节点和边都可以在关联的属性字典中保存键值对（键必须是可哈希的）。默认情况下，这些是空的， 但可以使用<code>add_edge</code> 、<code>add_node</code>或直接添加<code>G.graph</code>、<code>G.nodes</code>、<code>G.edges</code>的属性字典来添加或更改属性。</p><h4 id="图属性"><a href="#图属性" class="headerlink" title="图属性"></a>图属性</h4><p>创建新图时分配图属性</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>G = nx.Graph(day=<span class="hljs-string">&quot;Friday&quot;</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span>G.graph<br>&#123;<span class="hljs-string">&#x27;day&#x27;</span>: <span class="hljs-string">&#x27;Friday&#x27;</span>&#125;<br></code></pre></td></tr></table></figure><p>或者您可以稍后修改属性</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>G.graph[<span class="hljs-string">&#x27;day&#x27;</span>] = <span class="hljs-string">&quot;Monday&quot;</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>G.graph<br>&#123;<span class="hljs-string">&#x27;day&#x27;</span>: <span class="hljs-string">&#x27;Monday&#x27;</span>&#125;<br></code></pre></td></tr></table></figure><h4 id="节点属性"><a href="#节点属性" class="headerlink" title="节点属性"></a>节点属性</h4><p>使用<code>add_node()</code>、<code>add_nodes_from()</code>或<code>G.nodes</code>添加节点属性</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>G.add_node(<span class="hljs-number">1</span>, time=<span class="hljs-string">&#x27;5pm&#x27;</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span>G.add_nodes_from([<span class="hljs-number">3</span>], time=<span class="hljs-string">&#x27;2pm&#x27;</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span>G.nodes[<span class="hljs-number">1</span>]<br>&#123;<span class="hljs-string">&#x27;time&#x27;</span>: <span class="hljs-string">&#x27;5pm&#x27;</span>&#125;<br><span class="hljs-meta">&gt;&gt;&gt; </span>G.nodes[<span class="hljs-number">1</span>][<span class="hljs-string">&#x27;room&#x27;</span>] = <span class="hljs-number">714</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>G.nodes.data()<br>NodeDataView(&#123;<span class="hljs-number">1</span>: &#123;<span class="hljs-string">&#x27;time&#x27;</span>: <span class="hljs-string">&#x27;5pm&#x27;</span>, <span class="hljs-string">&#x27;room&#x27;</span>: <span class="hljs-number">714</span>&#125;, <span class="hljs-number">3</span>: &#123;<span class="hljs-string">&#x27;time&#x27;</span>: <span class="hljs-string">&#x27;2pm&#x27;</span>&#125;&#125;)<br></code></pre></td></tr></table></figure><blockquote><p>请注意，将节点添加到<code>G.nodes</code>不会将其添加到图形中，<code>G.add_node()</code>用于添加新节点。边也是如此。</p></blockquote><h4 id="边属性"><a href="#边属性" class="headerlink" title="边属性"></a>边属性</h4><p>使用<code>add_edge()</code> 、<code>add_edges_from()</code>或下标表示法添加或更改边属性。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs stylus">&gt;&gt;&gt; G<span class="hljs-selector-class">.add_edge</span>(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, weight=<span class="hljs-number">4.7</span> )<br>&gt;&gt;&gt; G<span class="hljs-selector-class">.add_edges_from</span>(<span class="hljs-selector-attr">[(3, 4), (4, 5)]</span>, <span class="hljs-attribute">color</span>=<span class="hljs-string">&#x27;red&#x27;</span>)<br>&gt;&gt;&gt; G<span class="hljs-selector-class">.add_edges_from</span>(<span class="hljs-selector-attr">[(1, 2, &#123;<span class="hljs-string">&#x27;color&#x27;</span>: <span class="hljs-string">&#x27;blue&#x27;</span>&#125;), (2, 3, &#123;<span class="hljs-string">&#x27;weight&#x27;</span>: 8&#125;)]</span>)<br>&gt;&gt;&gt; G<span class="hljs-selector-attr">[1]</span><span class="hljs-selector-attr">[2]</span><span class="hljs-selector-attr">[<span class="hljs-string">&#x27;weight&#x27;</span>]</span> = <span class="hljs-number">4.7</span><br>&gt;&gt;&gt; G<span class="hljs-selector-class">.edges</span><span class="hljs-selector-attr">[3, 4]</span><span class="hljs-selector-attr">[<span class="hljs-string">&#x27;weight&#x27;</span>]</span> = <span class="hljs-number">4.2</span><br></code></pre></td></tr></table></figure><p>特殊属性<code>weight</code>应为数字，因为它被需要加权边的算法使用。</p><h3 id="有向图"><a href="#有向图" class="headerlink" title="有向图"></a>有向图</h3><p><a href="https://networkx.org/documentation/stable/reference/classes/digraph.html#networkx.DiGraph"><code>DiGraph</code></a> 类提供了特定于有向边的其他方法和属性，例如<code>DiGraph.out_edges</code>、<code>DiGraph.in_degree</code>、<code>DiGraph.predecessors</code>、<code>DiGraph.successors</code>等。为了使算法能够轻松地处理这两个类，有向版本的<code>neighbors</code>等同于<code>successors</code>，而<code>degree</code>是 <code>in_degree</code>和<code>out_degree</code>，尽管有时可能会感到不一致。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span>DG = nx.DiGraph()<br><span class="hljs-meta">&gt;&gt;&gt; </span>DG.add_weighted_edges_from([(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0.5</span>), (<span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0.75</span>)])<br><span class="hljs-meta">&gt;&gt;&gt; </span>DG.out_degree(<span class="hljs-number">1</span>, weight=<span class="hljs-string">&#x27;weight&#x27;</span>)<br><span class="hljs-number">0.5</span><br><span class="hljs-meta">&gt;&gt;&gt; </span>DG.degree(<span class="hljs-number">1</span>, weight=<span class="hljs-string">&#x27;weight&#x27;</span>)<br><span class="hljs-number">1.25</span><br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(DG.successors(<span class="hljs-number">1</span>))<br>[<span class="hljs-number">2</span>]<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(DG.neighbors(<span class="hljs-number">1</span>))<br>[<span class="hljs-number">2</span>]<br></code></pre></td></tr></table></figure><p>有些算法仅适用于有向图，而另一些算法对有向图没有很好的定义 。将有向图和无向图混在一起是危险的。如果你想将有向图作为无向图的某些测量，您可能应该使用<code>Graph.to_undirected（）</code>将有向图作为无向图，或者使用</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">H = nx.Graph(G)  <span class="hljs-comment"># create an undirected graph H from a directed graph G</span><br></code></pre></td></tr></table></figure><p>获得无向图。</p><h3 id="多图"><a href="#多图" class="headerlink" title="多图"></a>多图</h3><p>NetworkX 提供了允许任意一对节点之间存在多条边的图类。 MultiGraph 和 MultiDiGraph 类允许您添加相同的边两次，可能使用不同的边数据。 这对于某些应用程序来说可能很强大，但许多算法在此类图上没有很好地定义。 在结果定义明确的地方，例如 <code>MultiGraph.degree() </code>我们提供了函数。 否则，您应该以使测量明确定义的方式转换为标准图表。</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs text">&gt;&gt;&gt; MG = nx.MultiGraph()<br>&gt;&gt;&gt; MG.add_weighted_edges_from([(1, 2, 0.5), (1, 2, 0.75), (2, 3, 0.5)])<br>&gt;&gt;&gt; dict(MG.degree(weight=&#x27;weight&#x27;))<br>&#123;1: 1.25, 2: 1.75, 3: 0.5&#125;<br>&gt;&gt;&gt; GG = nx.Graph()<br>&gt;&gt;&gt; for n, nbrs in MG.adjacency():<br>...    for nbr, edict in nbrs.items():<br>...        minvalue = min([d[&#x27;weight&#x27;] for d in edict.values()])<br>...        GG.add_edge(n, nbr, weight = minvalue)<br><br>&gt;&gt;&gt; nx.shortest_path(GG, 1, 3)<br>[1, 2, 3]<br></code></pre></td></tr></table></figure><h3 id="图生成器和图操作"><a href="#图生成器和图操作" class="headerlink" title="图生成器和图操作"></a>图生成器和图操作</h3><p>除了逐节点或逐边构造图之外，它们还可以通过以下方式生成</p><h4 id="1-应用经典的图操作，例如："><a href="#1-应用经典的图操作，例如：" class="headerlink" title="1. 应用经典的图操作，例如："></a>1. 应用经典的图操作，例如：</h4><table><thead><tr><th>方法</th><th>介绍</th></tr></thead><tbody><tr><td>subgraph(G, nbunch)</td><td>Returns the subgraph induced on nodes in nbunch.</td></tr><tr><td>union(G, H[, rename, name])</td><td>Return the union of graphs G and H.</td></tr><tr><td>disjoint_union(G, H)</td><td>Return the disjoint union of graphs G and H.</td></tr><tr><td>cartesian_product(G, H)</td><td>Returns the Cartesian product of G and H.</td></tr><tr><td>compose(G, H)</td><td>Returns a new graph of G composed with H.</td></tr><tr><td>complement(G)</td><td>Returns the graph complement of G.</td></tr><tr><td>create_empty_copy(G[, with_data])</td><td>Returns a copy of the graph G with all of the edges removed.</td></tr><tr><td>to_undirected(graph)</td><td>Returns an undirected view of the graph graph.</td></tr><tr><td>to_directed(graph)</td><td>Returns a directed view of the graph graph.</td></tr></tbody></table><h4 id="2-对经典小图进行调用"><a href="#2-对经典小图进行调用" class="headerlink" title="2. 对经典小图进行调用"></a>2. 对经典小图进行调用</h4><table><thead><tr><th>方法</th><th>介绍</th></tr></thead><tbody><tr><td>petersen_graph([create_using])</td><td>Returns the Petersen graph.</td></tr><tr><td>tutte_graph([create_using])</td><td>Returns the Tutte graph.</td></tr><tr><td>sedgewick_maze_graph([create_using])</td><td>Return a small maze with a cycle.</td></tr><tr><td>tetrahedral_graph([create_using])</td><td>Returns the 3-regular Platonic Tetrahedral graph.</td></tr></tbody></table><h4 id="3-为经典图使用（constructive）生成器"><a href="#3-为经典图使用（constructive）生成器" class="headerlink" title="3. 为经典图使用（constructive）生成器"></a>3. 为经典图使用（constructive）生成器</h4><table><thead><tr><th>方法</th><th>介绍</th></tr></thead><tbody><tr><td>complete_graph(n[, create_using])</td><td>Return the complete graph K_n with n nodes.</td></tr><tr><td>complete_bipartite_graph(n1, n2[, create_using])</td><td>Returns the complete bipartite graph K_{n_1,n_2}.</td></tr><tr><td>barbell_graph(m1, m2[, create_using])</td><td>Returns the Barbell Graph: two complete graphs connected by a path.</td></tr><tr><td>lollipop_graph(m, n[, create_using])</td><td>Returns the Lollipop Graph; K_m connected to P_n.</td></tr></tbody></table><p>像这样</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs text">K_5 = nx.complete_graph(5)<br>K_3_5 = nx.complete_bipartite_graph(3, 5)<br>barbell = nx.barbell_graph(10, 10)<br>lollipop = nx.lollipop_graph(10, 20)<br></code></pre></td></tr></table></figure><h4 id="4-使用随机图生成器，"><a href="#4-使用随机图生成器，" class="headerlink" title="4. 使用随机图生成器，"></a>4. 使用随机图生成器，</h4><table><thead><tr><th>方法</th><th>介绍</th></tr></thead><tbody><tr><td>erdos_renyi_graph(n, p[, seed, directed])</td><td>Returns a Gn,p random graph, also known as an Erdős-Rényi graph or a binomial graph.</td></tr><tr><td>watts_strogatz_graph(n, k, p[, seed])</td><td>Returns a Watts–Strogatz small-world graph.</td></tr><tr><td>barabasi_albert_graph(n, m[, seed, …])</td><td>Returns a random graph using Barabási–Albert preferential attachment</td></tr><tr><td>random_lobster(n, p1, p2[, seed])</td><td>Returns a random lobster graph.</td></tr></tbody></table><p>像这样</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs text">er = nx.erdos_renyi_graph(100, 0.15)<br>ws = nx.watts_strogatz_graph(30, 3, 0.1)<br>ba = nx.barabasi_albert_graph(100, 5)<br>red = nx.random_lobster(100, 0.9, 0.9)<br></code></pre></td></tr></table></figure><h4 id="5-使用常用图形格式读取存储在文件中的图形"><a href="#5-使用常用图形格式读取存储在文件中的图形" class="headerlink" title="5. 使用常用图形格式读取存储在文件中的图形"></a>5. 使用常用图形格式读取存储在文件中的图形</h4><p>NetworkX 支持许多流行的格式，例如边缘列表、邻接列表、GML、GraphML、pickle、LEDA 等。</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs text">nx.write_gml(red, &quot;path.to.file&quot;)<br>mygraph = nx.read_gml(&quot;path.to.file&quot;)<br></code></pre></td></tr></table></figure><p>有关图形格式的详细信息，请参<a href="https://link.zhihu.com/?target=https://networkx.org/documentation/stable/reference/readwrite/index.html">阅读和写图形</a>；有关图形生成器函数，请参阅<a href="https://link.zhihu.com/?target=https://networkx.org/documentation/stable/reference/generators.html">图形生成器</a></p><h3 id="分析图"><a href="#分析图" class="headerlink" title="分析图"></a>分析图</h3><p>可以使用各种图论函数来分析图<code>G</code> 的结构，例如：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs text">&gt;&gt;&gt; G = nx.Graph()<br>&gt;&gt;&gt; G.add_edges_from([(1, 2), (1, 3)])<br>&gt;&gt;&gt; G.add_node(&quot;spam&quot;)       # adds node &quot;spam&quot;<br>&gt;&gt;&gt; list(nx.connected_components(G))<br>[&#123;1, 2, 3&#125;, &#123;&#x27;spam&#x27;&#125;]<br>&gt;&gt;&gt; sorted(d for n, d in G.degree())<br>[0, 1, 1, 2]<br>&gt;&gt;&gt; nx.clustering(G)<br>&#123;1: 0, 2: 0, 3: 0, &#x27;spam&#x27;: 0&#125;<br></code></pre></td></tr></table></figure><p>一些具有大输出的函数迭代 (node, value) 2 元组。 如果您愿意，这些很容易存储在 dict 结构中。</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs text">&gt;&gt;&gt; sp = dict(nx.all_pairs_shortest_path(G))<br>&gt;&gt;&gt; sp[3]<br>&#123;3: [3], 1: [3, 1], 2: [3, 1, 2]&#125;<br></code></pre></td></tr></table></figure><p>有关支持的图形算法的详细信息，请参阅<a href="https://link.zhihu.com/?target=https://networkx.org/documentation/stable/reference/algorithms/index.html">算法</a>。</p><h3 id="绘制图"><a href="#绘制图" class="headerlink" title="绘制图"></a>绘制图</h3><p>NetworkX 主要不是一个图形绘图包，而是包含使用 Matplotlib 的基本绘图以及使用开源 Graphviz 软件包的接口。 这些是<code> networkx.drawing</code> 模块的一部分，如果可能，将被导入。</p><p>首先导入 Matplotlib 的绘图接口（pylab 也可以）</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs text">import matplotlib.pyplot as plt<br></code></pre></td></tr></table></figure><p>要测试 <code>nx_pylab</code> 是否成功导入，请使用以下方法之一绘制图<code>G</code></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs text">G = nx.petersen_graph()<br>subax1 = plt.subplot(121)<br>nx.draw(G, with_labels=True, font_weight=&#x27;bold&#x27;)<br>subax2 = plt.subplot(122)<br>nx.draw_shell(G, nlist=[range(5, 10), range(5)], with_labels=True, font_weight=&#x27;bold&#x27;)<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/JiqingJiang/erode_Picgo_repo/main/img/NetworkX%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01.webp"></p><p>交互式操作时上述图像会自动展示。 请注意如果您没有在交互模式下使用 matplotlib，您可能需要下面命令展示图形</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs text">plt.show()  <br><br>options = &#123;<br>    &#x27;node_color&#x27;: &#x27;black&#x27;,<br>    &#x27;node_size&#x27;: 100,<br>    &#x27;width&#x27;: 3,<br>&#125;<br>subax1 = plt.subplot(221)<br>nx.draw_random(G, **options)<br>subax2 = plt.subplot(222)<br>nx.draw_circular(G, **options)<br>subax3 = plt.subplot(223)<br>nx.draw_spectral(G, **options)<br>subax4 = plt.subplot(224)<br>nx.draw_shell(G, nlist=[range(5,10), range(5)], **options)<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/JiqingJiang/erode_Picgo_repo/main/img/NetworkX%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02.webp"></p><p>您可以通过 <code>draw_networkx()</code> 找到其他选项，并通过布局模块找到布局。 您可以通过 <code>draw_shell()</code> 使用多个 shell。</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs text">G = nx.dodecahedral_graph()<br>shells = [[2, 3, 4, 5, 6], [8, 1, 0, 19, 18, 17, 16, 15, 14, 7], [9, 10, 11, 12, 13]]<br>nx.draw_shell(G, nlist=shells, **options)<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/JiqingJiang/erode_Picgo_repo/main/img/NetworkX%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03.webp"></p><p>要将图形保存到文件中，请使用，例如</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs text">nx.draw(G)<br>plt.savefig(&quot;path.png&quot;)<br></code></pre></td></tr></table></figure><p>此函数写入本地目录中的文件 <code>path.png</code>。 如果 Graphviz 和 PyGraphviz 或 pydot 在您的系统上可用，您还可以使用 <code>networkx.drawing.nx_agraph.graphviz_layout</code> 或 <code>networkx.drawing.nx_pydot.graphviz_layout</code> 来获取节点位置，或者以点格式编写图形以进行进一步处理。</p>]]></content>
    
    
    <categories>
      
      <category>python学习笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>networkx</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
